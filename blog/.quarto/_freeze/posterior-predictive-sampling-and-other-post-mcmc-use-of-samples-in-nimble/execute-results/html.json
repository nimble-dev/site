{
  "hash": "a9a1606b4ba81cef7acb5259cb6935da",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Posterior predictive sampling and other post-MCMC use of samples in NIMBLE\"\ndescription: \"\"\nauthor: \"Chris Paciorek and Sally Paganin\"\ndate: \"2021-08-10\"\ncategories: ['announcement','tutorial']\n---\n\n\nOnce one has samples from an MCMC, one often wants to do some post hoc manipulation of the samples. An important example is posterior predictive sampling, which is needed for posterior predictive checking.\n\nWith posterior predictive sampling, we need to simulate new data values, once for each posterior sample. These samples can then be compared with the actual data as a model check.\n\nIn this example, we’ll follow the posterior predictive checking done in the Gelman et al. Bayesian Data Analysis book, using Newcomb’s speed of light measurements (Section 6.3).\n\n## Posterior predictive sampling using a loop in R\n\nSimon Newcomb made 66 measurements of the speed of light, which one might model using a normal distribution. One question discussed in Gelman et al. is whether the lowest measurements, which look like outliers, could have reasonably come from a normal distribution.\n\n### Setup\n\nWe set up the nimble model.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nimble, warn.conflicts = FALSE)\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncode <- nimbleCode({\n    ## noninformative priors\n    mu ~ dflat()\n    sigma ~ dhalfflat()\n    ## likelihood\n    for(i in 1:n) {\n        y[i] ~ dnorm(mu, sd = sigma)\n    }\n})\n\ndata <- list(y = MASS::newcomb)\ninits <- list(mu = 0, sigma = 5)\nconstants <- list(n = length(data$y))\n\nmodel <- nimbleModel(code = code, data = data, constants = constants, inits = inits)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDefining model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nBuilding model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting data and initial values\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nChecking model sizes and dimensions\n```\n\n\n:::\n:::\n\n\nNext we’ll create some vectors of node names that will be useful for our manipulations.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\n## Ensure we have the nodes needed to simulate new datasets\ndataNodes <- model$getNodeNames(dataOnly = TRUE)\nparentNodes <- model$getParents(dataNodes, stochOnly = TRUE)  # `getParents` is new in nimble 0.11.0\n## Ensure we have both data nodes and deterministic intermediates (e.g., lifted nodes)\nsimNodes <- model$getDependencies(parentNodes, self = FALSE)\n```\n:::\n\n\nNow run the MCMC.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\ncmodel  <- compileNimble(model)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n\n```{.r .cell-code}\nmcmc    <- buildMCMC(model, monitors = parentNodes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n===== Monitors =====\nthin = 1: mu, sigma\n===== Samplers =====\nconjugate sampler (2)\n  - mu\n  - sigma\n```\n\n\n:::\n\n```{.r .cell-code}\ncmcmc   <- compileNimble(mcmc, project = model)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n\n```{.r .cell-code}\nsamples <- runMCMC(cmcmc, niter = 1000, nburnin = 500)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrunning chain 1...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n```\n\n\n:::\n:::\n\n    \n\n### Posterior predictive sampling by direct variable assignment\n\nWe’ll loop over the samples and use the compiled model (uncompiled would be ok too, but slower) to simulate new datasets.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nnSamp <- nrow(samples)\nn <- length(data$y)\nppSamples <- matrix(0, nSamp, n)\n\nset.seed(1)\nfor(i in 1:nSamp){\n  cmodel[[\"mu\"]] <- samples[i, \"mu\"]             ## or cmodel$mu <- samples[i, \"mu\"]\n  cmodel[[\"sigma\"]] <- samples[i, \"sigma\"]\n  cmodel$simulate(simNodes, includeData = TRUE)\n  ppSamples[i, ] <- cmodel[[\"y\"]]\n}\n```\n:::\n\n\n### Posterior predictive sampling using `values`\n\nThat’s fine, but we needed to manually insert values for the different variables. For a more general solution, we can use nimble’s `values` function as follows.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nppSamples <- matrix(0, nrow = nSamp, ncol =\n          length(model$expandNodeNames(dataNodes, returnScalarComponents = TRUE)))\npostNames <- colnames(samples)\n\nset.seed(1)\nsystem.time({\nfor(i in seq_len(nSamp)) {\n    values(cmodel, postNames) <- samples[i, ]  # assign 'flattened' values\n    cmodel$simulate(simNodes, includeData = TRUE)\n    ppSamples[i, ] <- values(cmodel, dataNodes)\n}\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  4.518   0.010   4.540 \n```\n\n\n:::\n:::\n\n    \n\nSide note: For large models, it might be faster to use the variable names as the second argument to `values()` rather than the names of all the elements of the variables. If one chooses to do this, it’s important to check that the ordering of variables in the ‘flattened’ values in `samples` is the same as the ordering of variables in the second argument to `values` so that the first line of the for loop assigns the values from `samples` correctly into the model.\n\n### Doing the posterior predictive check\n\nAt this point, we can implement the check we want using our chosen discrepancy measure. Here a simple check uses the minimum observation.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nobsMin <- min(data$y)\nppMin <- apply(ppSamples, 1, min)\n\n# ## Check with plot in Gelman et al. (3rd edition), Figure 6.3\nhist(ppMin, xlim = c(-50, 20),\n    main = \"Discrepancy = min(y)\",\n    xlab = \"min(y_rep)\")\nabline(v = obsMin, col = 'red')\n```\n\n::: {.cell-output-display}\n![](posterior-predictive-sampling-and-other-post-mcmc-use-of-samples-in-nimble_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Fast posterior predictive sampling using a nimbleFunction\n\nThe approach above could be slow, even with a compiled model, because the loop is carried out in R. We could instead do all the work in a compiled nimbleFunction.\n\n### Writing the nimbleFunction\n\nLet’s set up a nimbleFunction. In the setup code, we’ll manipulate the nodes and variables, similarly to the code above. In the run code, we’ll loop through the samples and simulate, also similarly.\n\nRemember that all querying of the model structure needs to happen in the setup code. We also need to pass the MCMC object to the nimble function, so that we can determine at setup time the names of the variables we are copying from the posterior samples into the model.\n\nThe run code takes the actual samples as the input argument, so the nimbleFunction will work regardless of how long the MCMC was run for.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nppSamplerNF <- nimbleFunction(\n          setup = function(model, mcmc) {\n              dataNodes <- model$getNodeNames(dataOnly = TRUE)\n              parentNodes <- model$getParents(dataNodes, stochOnly = TRUE)\n              cat(\"Stochastic parents of data are:\", paste(parentNodes, collapse = ','), \".\\n\")\n              simNodes <- model$getDependencies(parentNodes, self = FALSE)\n              vars <- mcmc$mvSamples$getVarNames()  # need ordering of variables in mvSamples / samples matrix\n              cat(\"Using posterior samples of:\", paste(vars, collapse = ','), \".\\n\")\n              n <- length(model$expandNodeNames(dataNodes, returnScalarComponents = TRUE))\n          },\n          run = function(samples = double(2)) {\n              nSamp <- dim(samples)[1]\n              ppSamples <- matrix(nrow = nSamp, ncol = n)\n              for(i in 1:nSamp) {\n                    values(model, vars) <<- samples[i, ]\n                    model$simulate(simNodes, includeData = TRUE)\n                    ppSamples[i, ] <- values(model, dataNodes)\n              }\n              returnType(double(2))\n              return(ppSamples)\n          })\n```\n:::\n\n\n### Using the nimbleFunction\n\nWe’ll create the instance of the nimbleFunction for this model and MCMC.  \nThen we run the compiled nimbleFunction.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\n## Create the sampler for this model and this MCMC.\nppSampler <- ppSamplerNF(model, mcmc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStochastic parents of data are: mu,sigma .\nUsing posterior samples of: mu,sigma .\n```\n\n\n:::\n\n```{.r .cell-code}\ncppSampler <- compileNimble(ppSampler, project = model)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n\n```{.r .cell-code}\n## Check ordering of variables is same in 'vars' and in 'samples'.\ncolnames(samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"mu\"    \"sigma\"\n```\n\n\n:::\n\n```{.r .cell-code}\nidentical(colnames(samples), model$expandNodeNames(mcmc$mvSamples$getVarNames()))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(1)\nsystem.time(ppSamples_via_nf <- cppSampler$run(samples))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.002   0.000   0.003 \n```\n\n\n:::\n\n```{.r .cell-code}\nidentical(ppSamples, ppSamples_via_nf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n    \n\nSo we get exactly the same results (note the use of `set.seed` to ensure this) but much faster.\n\nHere the speed doesn’t really matter but for more samples and larger models it often will, even after accounting for the time spent to compile the nimbleFunction.",
    "supporting": [
      "posterior-predictive-sampling-and-other-post-mcmc-use-of-samples-in-nimble_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}