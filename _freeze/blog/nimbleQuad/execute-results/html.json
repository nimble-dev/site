{
  "hash": "5fae50b3ea84c57bda342731c6cb5f36",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Using nimbleQuad for maximum likelihood estimation and deterministic posterior approximation\"\nauthor: \"Paul van Dam-Bates and Christopher Paciorek\"\ndate: \"2026-01-29\"\ncategories: ['announcement','R','tutorial']\nbibliography: nimbleQuad_references.bib\nexecute:\n  freeze: true\n---\n\n\n## Introduction\n\n\nThe new `nimbleQuad` package provides quadrature-based inference methods that use Laplace and Adaptive Gauss-Hermite Quadrature (AGHQ) approximation. It has two main components:\n\n1. Laplace and AGHQ approximations for marginalizing over latent nodes (e.g., random effects) to perform approximate maximum likelihood estimation.\n2. Deterministic nested quadrature methods for posterior approximation using Laplace approximation for the inner marginalization of the latent nodes and general quadrature methods for outer marginalization of the parameter nodes. These methods borrow ideas from the popular INLA approach [@rue2009approximate] and from the extended Gaussian latent model approach provided in the R `aghq` package [@stringer2023fast].\n\nThe first component was present in the core `nimble` package until version 1.4.0. The second component is a new feature.\n\nBy providing these methods in the NIMBLE system, via `nimbleQuad`, users can use the methods on models constructed using the NIMBLE model language (which extends the BUGS model language) and  can also customize and extend the algorithms.\n\nIn this document we will introduce some of the workflow and show the flexibility available when using `buildNestedApprox` for posterior approximations.\n\n\n::: {.cell}\n\n:::\n\n\nAdditional details about nimbleQuad can be found in [the NIMBLE User Manual](https://r-nimble.org/manual/cha-laplace.html) and [the nimbleQuad vignette](https://r-nimble.org/vignettes/nimbleQuad.html).\n\n### What is NIMBLE?\n\n[NIMBLE](https://r-nimble.org) is a system for writing hierarchical statistical models and algorithms.  It is distributed as the R package, [nimble](https://CRAN.R-project.org/package=nimble). NIMBLE includes:\n\n1. A dialect of the BUGS model language that is extensible.  NIMBLE uses almost the same model code as WinBUGS, OpenBUGS, and JAGS.  Being \"extensible\" means that it is possible to write new functions and distributions and use them in your models.\n\n2. An algorithm library including Markov chain Monte Carlo (MCMC) and other methods.\n\n3. A compiler that generates C++ for each model and algorithm, compiles the C++, and lets you use it from R.  You don't need to know anything about C++ to use nimble.\n\n### Illustrative Example\n\nFor this vignette we will use the salamander example from the R package `glmmTMB`. Example usage of this model can be seen by running `?glmmTMB`. The data are repeated counts, $y$, of salamanders at multiple sites, which are zero-inflated. Some of the sites are mined, and some are not ($x$). Based on `glmmTMB`, we can analyze these data as\n\n$$\n  y_i|z_i \\sim \\text{Poisson}(z_i\\lambda_i) \n$$\n\n$$\n  z_i \\sim \\text{Bernoulli}(1-p_i)\n$$\n\n$$\n  \\text{log}(\\lambda_i) = \\beta x_i + u_{site_i}\n$$\n\n$$\n\\text{logit}(p_i) = \\alpha_0 x_i\n$$\n\n$$\nu_{j} \\sim \\text{Normal}(0, \\sigma^2)\n$$\n\nThe maximum likelihood estimator can be found with `glmmTMB`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmmTMB)\ndata(Salamanders)\nfit_tmb <- glmmTMB(count ~ spp * mined + (1|site), zi=~1, family=poisson, data=Salamanders)\n```\n:::\n\n\nWe can also fit this model in `INLA` to get the approximate posterior using the \"zeroinflatedpoisson1\" model family.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(INLA)\nfit_inla <- inla( count ~ spp * mined + f(site, model=\"iid\"), \n  quantiles = c(.025,.25,.5,.75,.975), family= \"zeroinflatedpoisson1\", data=Salamanders,\n  control.inla = list(int.strategy = \"ccd\"))\n```\n:::\n\n\n## Building a GLMM in NIMBLE\n\nTo write this model in NIMBLE we will first write a zero-inflated Poisson distribution that marginalizes over the indirectly observed discrete value $z$. When $y=0$, $z$ is a discrete latent variable. In order to use the methods in `nimbleQuad`, all the latent variables in this model must be continuous.\n\n`nimbleQuad` relies on automatic differentiation (AD) to take derivatives. To include AD in a `nimbleFunction` we must set `buildDerivs = 'run'`.\n\nA nimbleFunction, `rZIP`, to simulate from the distribution is also provided for convenience, although this is not strictly needed for this model to fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nimbleQuad)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndZIP <- nimbleFunction(\n run = function(x = double(), lambda = double(),\n                zeroProb = double(), log = logical(0, default = 0)) {\n   returnType(double())\n   ## For use with AD, we cannot use an `if` statement to handle the mixture.\n   prob <- zeroProb * dbinom(x, size = 1, prob = 0) + (1 - zeroProb) * dpois(x, lambda)\n   if (log) return(log(prob))\n     return(prob)\n   },\n   buildDerivs = 'run'   # Needed when used with AD-based algorithms.\n)\n\nrZIP <- nimbleFunction(\n run = function(n = integer(), lambda = double(), zeroProb = double()) {\n   returnType(double())\n   isStructuralZero <- rbinom(1, prob = zeroProb, size = 1)\n   if (isStructuralZero) return(0)\n   return(rpois(1, lambda))\n})\n```\n:::\n\n\nWe will use prior distributions to match the default priors in the `INLA` model that was fit above to allow direct comparison of results. See `inla.priors.used(fit_inla)` for a description of the default priors. INLA places a logit-normal prior on $p$. For convenience, we will write our own logit-normal prior. In this case, we explicitly register the distribution to tell NIMBLE the range of possible values, needed for use in NIMBLE's parameter transformation system.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndlogitnormal = nimbleFunction(\n  run = function(x = double(), mean = double(), sd = double(), \n      log = logical(0, default = 0)){\n    ans <- dnorm(logit(x), mean = mean, sd = sd, log = TRUE) - log(x) - log(1-x)\n    returnType(double())\n    if(log) return(ans)\n    else return(exp(ans))\n  },\n  buildDerivs = 'run'\n)\n\nregisterDistributions(list(\n  dlogitnormal = list(BUGSdist = \"dlogitnormal(mean, sd)\", range = c(0,1))\n))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n  [Warning] Random generation function for dlogitnormal is not available. NIMBLE is generating a placeholder function, rlogitnormal, that will invoke an error if an algorithm needs to simulate from this distribution. Some algorithms (such as random-walk Metropolis MCMC sampling) will work without the ability to simulate from the distribution.  If simulation is needed, provide a nimbleFunction (with no setup code) to do it.\n```\n\n\n:::\n:::\n\n\nIf not trying to use the same model as INLA, one might choose different priors, and one would not necessarily need to define the logit-normal distribution.\n\nFollowing INLA, we use a completely flat prior on the intercept `dflat`, a gamma prior on precision `tau_re` and a logit-normal on `p`. We can now write the model code, and then provide constants and data to build the model. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncode <- nimbleCode({\n  beta[1] ~ dflat()\n  for( i in 2:np ) \n    beta[i] ~ dnorm(0, tau = 0.001) \n  tau_re ~ dgamma(shape = 1, rate = 5e-5)\n  p ~ dlogitnormal(mean = -1, sd = 1/sqrt(0.2))\n  \n  for(i in 1:nsites) \n    re[i] ~ dnorm(0, tau = tau_re)\n\n  for( i in 1:nobs ){\n    log(lam[i]) <- sum(beta[1:np]*X[i,1:np]) + re[site[i]]\n    count[i] ~ dZIP(lambda = lam[i], zeroProb = p)\n  }\n})\n\nnimconst <- list()\nnimconst$X <- as.matrix(model.matrix( ~ spp * mined, data = Salamanders ))\nnimconst$np <- ncol(nimconst$X)\nnimconst$nobs <- nrow(nimconst$X)\nnimconst$site <- as.numeric(factor(Salamanders$site))\nnimconst$nsites <- max(nimconst$site)\n\nnimdata <- list()\nnimdata$count <- Salamanders$count\n\ninits <- list(p = .29,\n  tau_re = 3,\n  re = rnorm(nimconst$nsites,0,0.1),\n  beta = rnorm(nimconst$np))\n\nm <- nimbleModel(code, data = nimdata, constants = nimconst, inits = inits, buildDerivs = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDefining model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n  [Note] Registering 'dZIP' as a distribution based on its use in BUGS code. If you make changes to the nimbleFunctions for the distribution, you must call 'deregisterDistributions' before using the distribution in BUGS code for those changes to take effect.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nBuilding model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting data and initial values\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nChecking model sizes and dimensions\n```\n\n\n:::\n\n```{.r .cell-code}\ncm <- compileNimble(m)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n:::\n\n\nNote that since nimbleQuad methods use NIMBLE's automatic differentiation (AD) system, we set `buildDerivs = TRUE` when building the nimble model.\n\n## Using the Laplace Approximation\n\nWe can find the maximum likelihood estimate by using `buildLaplace` (or `buildAGHQ` if we want more quadrature nodes for marginalizing over the random effects, as discussed below).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlaplace <- buildLaplace(model = m)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nBuilding 23 individual Laplace approximations (one dot for each): .......................\n```\n\n\n:::\n\n```{.r .cell-code}\nclaplace <- compileNimble(laplace)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n\n```{.r .cell-code}\nnimble_fit <- runLaplace(claplace)\n```\n:::\n\n\nNIMBLE reports that it is able to use 23 separate (one-dimensional) Laplace approximations (because the random effects, plus their data dependencies, are conditionally independent). \n\nLet's compare the results with `glmmTMB`. The results are very similar.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnimble_params <- nimble_fit$summary$params\nnimble_fixef <- nimble_params[grep(\"beta\", rownames(nimble_params)),]\n\nplot(nimble_fixef[, \"estimate\"], fixef(fit_tmb)$cond, \n  xlab = \"NIMBLE Estimate\", ylab = \"glmmTMB Estimate\")\nabline(a = 0, b = 1, lty = 2)\n```\n\n::: {.cell-output-display}\n![](nimbleQuad_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# logit(p):\nfit_tmb$fit$par[\"betazi\"]     # TMB\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   betazi \n-0.933805 \n```\n\n\n:::\n\n```{.r .cell-code}\nlogit(nimble_params[\"p\",1])   # NIMBLE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.9338518\n```\n\n\n:::\n\n```{.r .cell-code}\n# tau_re\nexp(-2*fit_tmb$fit$par[\"theta\"])  # TMB\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   theta \n3.646564 \n```\n\n\n:::\n\n```{.r .cell-code}\nnimble_params[\"tau_re\",1]         # NIMBLE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.646879\n```\n\n\n:::\n:::\n\n\nIt can be helpful to check the Laplace approximation at the MLE by increasing the number of quadrature nodes using AGHQ. If the likelihood is very different when increasing the number of quadrature nodes, that indicates that Laplace was not a good approximation to the marginal likelihood. In this particular case, since we have have conditionally independent random effects (and therefore a set of 1-dimensional AGHQ approximations), increasing the number of quadrature nodes will be relatively quick, but in other cases it can be very slow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclaplace$calcLogLik(nimble_fit$summary$params[, 'estimate'], trans = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -881.9147\n```\n\n\n:::\n\n```{.r .cell-code}\nclaplace$updateSettings(nQuad = 5)\nclaplace$calcLogLik(nimble_fit$summary$params[, 'estimate'], trans = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -881.8752\n```\n\n\n:::\n\n```{.r .cell-code}\nclaplace$updateSettings(nQuad = 11)\nclaplace$calcLogLik(nimble_fit$summary$params[, 'estimate'], trans = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -881.8749\n```\n\n\n:::\n:::\n\n\n## Posterior Approximation\n\nPosterior approximation is done by `buildNestedApprox` (to build the approximation) and `runNestedApprox` (to calculate the approximation). Note that the setup for `buildNestedApprox`is similar to `buildLaplace` except that we refer to the combination of random effects and fixed effects as `latentNodes` and the hyperparameters as `paramNodes`, which we will refer to more generally as the \"parameters\". Including fixed effects with the random effects reduces the dimension of the outer quadrature, which reduces computation time. However, in some cases one may want to include the fixed effects in the `paramNodes`, as the choice of where to include the fixed effects can affect statistical performance. If the user does not specify `latentNodes` and `paramNodes`, NIMBLE will automatically determine them based on the graphical structure of the model, and the user can check that a reasonable determination has been made.\n\n\n::: {.cell}\n\n```{.r .cell-code}\napprox <- buildNestedApprox(model = m, \n                            paramNodes = c('p', 'tau_re'), \n                            latentNodes = c('beta', 're'))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nBuilding nested posterior approximation for the following node sets:\n  - parameter nodes: p, tau_re\n  - latent nodes: beta (14 elements), re (23 elements)\n  with AGHQ grid for the parameters and Laplace approximation for the latent nodes.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nBuilding Laplace approximation.\n```\n\n\n:::\n\n```{.r .cell-code}\ncapprox <- compileNimble(approx, project = m)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n:::\n\n\nWe then use `runNestedApprox` to run the basic approximation and apply various methods to the output object to carry out further inference. The steps include:\n\n1. Find posterior mode of the parameters using Laplace (or AGHQ) to marginalize over the latent nodes.\n2. Use an *integration-free* method analogous to that of INLA to quickly approximate the marginal posterior distribution of each parameter [@martins2013bayesian].\n3. Calculate the posterior density of the parameters across the parameter quadrature grid (the default grid uses the CCD (central composite design) approach when the number of parameters is greater than two). In addition, a quadrature-based marginal log likelihood is produced based on this computation.\n4. Sample from the joint posterior of the `latentNodes` using a mixture of multivariate normal distributions with mean, covariance, and weights obtained at each grid point in the parameter quadrature grid.\n\nUsing the integration-free method is the fastest way to produce marginal posterior density estimates of the parameters. Following the methodology of INLA, this is done by using an asymmetric multivariate normal distribution to approximate the posterior density of the parameters.\n\nIf more accuracy is required, quadrature can be used such as AGHQ to marginalize over the other parameters (similar to the R package `aghq`). Here the user can choose between AGHQ, CCD, sparse AGHQ, or a user-provided quadrature rule, as discussed further below). Marginal density values are computed at a fixed set of points and then a spline is used for computations with the univariate density.\n\nFor inference on the latent nodes, at each point in the parameter grid, Laplace is used to marginalize the latent nodes to calculate the posterior density for the point, which requires an inner optimization. This can be computationally expensive (particularly with a large number of latent nodes), scaling with the number of grid points. CCD is the default (as is done in INLA) because it scales well to higher dimensions. Alternatively, an AGHQ grid can be used. This will be computationally more expensive because there are more grid points but potentially more accurate. The user can also provide their own grid. By default, we do not do compute the parameter grid when calling `runNestedApprox` unless requested, because the integration-free estimates do not require it. However, it is necessary for inference on the latent nodes and is done automatically when the latent nodes are sampled (via `sampleLatents`).\n\n\nHere is the initial (integration-free) inference for the parameters. We compare results to samples from a long HMC (MCMC) run.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult <- runNestedApprox(approx = capprox)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFinding posterior mode for parameter(s).\n```\n\n\n:::\n\n```{.r .cell-code}\nresult\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel (hyper)parameters: \n            mean         sd      2.5%       25%       50%      75%     97.5%\np      0.2829532 0.03213737 0.2223163 0.2607501 0.2822413 0.304266 0.3480409\ntau_re 4.2832265 2.21980867 1.4690074 2.7377845 3.7993353 5.280151 9.9046828\n\nMarginal log-likelihood (asymmetric Gaussian approximation): -957.392(*)\n  (*) Invalid for improper priors and may not be useful for non-informative priors.\n```\n\n\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1,2))\n\nresult$plotMarginal(\"tau_re\", xlim = c(0,17), lwd=2)\nlines(hmc_tau_re$x, inla.dmarginal(hmc_tau_re$x, fit_inla$marginals.hyperpar[[2]]), col = 'red')\nlines(hmc_tau_re, col = 'green')\nlegend(\"topright\", legend = c(\"NIMBLE\\nnested approx\", \"INLA\", \"HMC\"),\n  col = c('black','red','green'), lty = rep(1,3), cex = 0.75, bty = 'n')\n\nresult$plotMarginal(\"p\", lwd=2)\nlines(hmc_p$x, inla.dmarginal(hmc_p$x, fit_inla$marginals.hyperpar[[1]]), col = 'red')\nlines(hmc_p, col = 'green')\n```\n\n::: {.cell-output-display}\n![](nimbleQuad_files/figure-html/unnamed-chunk-12-1.png){width=768}\n:::\n:::\n\n\nThe nested approximation and INLA results match MCMC well, though for some reason the densities plotted by INLA don't quite integrate to one and therefore lies above the other two lines.\n\nHere is the sampling-based inference for the latent nodes. The density estimates match HMC reasonably well, but not quite as well as INLA for the fixed effects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# remove these lines later\n# hmc_beta1 <- density(samples_hmc[ , 'beta[1]'])\n# hmc_beta2 <- density(samples_hmc[ , 'beta[2]'])\n# hmc_re1 <- density(samples_hmc[ , 're[1]'])\n\nresult$sampleLatents(10000)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCalculating inner AGHQ/Laplace approximation at 9 parameter (outer)\n  grid points (one dot per point): .........\n```\n\n\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1,3))\nplot(density(result$samples[, \"beta[1]\"]), main = \"\", xlab = \"beta[1]\")\nlines(hmc_beta1$x, inla.dmarginal(hmc_beta1$x, fit_inla$marginals.fixed[[1]]), col = 'red')\nlines(hmc_beta1, col = 'green')\nplot(density(result$samples[, \"beta[2]\"]), main = \"\", xlab = \"beta[2]\")\nlines(hmc_beta2$x, inla.dmarginal(hmc_beta2$x, fit_inla$marginals.fixed[[2]]), col = 'red')\nlines(hmc_beta2, col = 'green')\nplot(density(result$samples[, \"re[1]\"]), main = \"\", xlab = \"re[1]\")\nlines(hmc_re1$x, inla.dmarginal(hmc_re1$x, fit_inla$marginals.random[[1]][[1]]), col = 'red')\nlines(hmc_re1, col = 'green')\n```\n\n::: {.cell-output-display}\n![](nimbleQuad_files/figure-html/unnamed-chunk-13-1.png){width=768}\n:::\n:::\n\n\n### Improving the Parameter Marginals\n\nAlthough the integration-free method is fast and performs reasonably well, it may be that the user wishes to get a more exact marginal. This is done individually for each parameter of interest, marginalizing over the remaining parameters using an outer quadrature rule (AGHQ, CCD, AGHQSPARSE, USER), with AGHQ as the default. We choose the number of quadrature points for marginalization (`nQuad`) as well as the number of points along a grid to evaluate the marginal (`nMarginalGrid`). Increasing either can improve performance but at the expense of additional computation and generally with diminishing improvement.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult  # Original (integration-free) parameter estimates\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel (hyper)parameters: \n            mean         sd      2.5%       25%       50%      75%     97.5%\np      0.2829532 0.03213737 0.2223163 0.2607501 0.2822413 0.304266 0.3480409\ntau_re 4.2832265 2.21980867 1.4690074 2.7377845 3.7993353 5.280151 9.9046828\n\nMarginal log-likelihood (asymmetric Gaussian approximation): -957.392(*)\nMarginal log-likelihood (grid-based): -957.3845(*)\n  (*) Invalid for improper priors and may not be useful for non-informative priors.\n```\n\n\n:::\n\n```{.r .cell-code}\nresult$improveParamMarginals(nodes = c('tau_re', 'p'),\n  nMarginalGrid = 9, nQuad = 3, quadRule = \"AGHQ\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nApproximating 2 individual parameter marginal densities via AGHQ:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - calculating inner AGHQ/Laplace approximation at (9) marginal points\n    with 3 quadrature grid points (one dot per grid point): (1)...(2)...(3)...(4)...(5)...(6)...(7)...(8)...(9)...\n  - calculating inner AGHQ/Laplace approximation at (9) marginal points\n    with 3 quadrature grid points (one dot per grid point): (1)...(2)...(3)...(4)...(5)...(6)...(7)...(8)...(9)...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel (hyper)parameters: \n            mean         sd     2.5%       25%       50%       75%     97.5%\np      0.2829161 0.03233042 0.220414 0.2608876 0.2826332 0.3046184  0.347118\ntau_re 4.3240215 2.34440791 1.450420 2.7365618 3.8040399 5.3028558 10.249151\n\nMarginal log-likelihood (asymmetric Gaussian approximation): -957.392(*)\nMarginal log-likelihood (grid-based): -957.3845(*)\n  (*) Invalid for improper priors and may not be useful for non-informative priors.\n```\n\n\n:::\n:::\n\n\nComparing to the quantiles from the integration-free approach, the differences are generally minor, but there is a noticeable difference in the right tail for `tau_re`.\n\n### Customizing the Quadrature Grid\n\nFor the salamanders example, we have two parameters, for which `nimbleQuad` defaults to using AGHQ as the parameter grid. The grid points are skewed based on the skew from the asymmetric multivariate normal approximation, and then scaled using spectral decomposition (or Cholesky decomposition).\n\nFor more than two parameters, `nimbleQuad` defaults to using a CCD grid, also used by INLA.\n\nThe user may also provide their own parameter grid. This must be written as a `nimbleFunction` with some specific components and is described in [the nimbleQuad vignette](https://r-nimble.org/vignettes/nimbleQuad.html#customizing-the-quadrature-grid).\n\n\n### Sampling from the posterior over parameters\n\nThe user may also choose to sample from the asymmetric multivariate normal approximation to make inference about the parameters. This can be particularly useful when making inference on parameters with dimension changes (e.g., proportions with a Dirichlet prior or elements of a covariance matrix, such as one with a Wishart or LKJ prior) or generally making inference about functionals of multiple parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples_params <- result$sampleParams(10000)\nresult$plotMarginal(\"tau_re\", xlim = c(0,17), lwd=2)\nlines(density(samples_params[,2]), col = 'black', lty = 2)\nlines(hmc_tau_re, col = 'green')\nlegend(\"topright\", legend = c(\"nested approx\", \"nested approx, samples\", \"HMC\"),\n  col = c('black','black','green'), lty = c(1,2,1), bty = 'n')\n```\n\n::: {.cell-output-display}\n![](nimbleQuad_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n### Model selection and the marginal likelihood\n\nWith proper priors, one can calculate $p(y)=\\int p(y|\\theta)p(\\theta)d\\theta$, the marginal likelihood, which is useful for model selection. This is reported with the nested approximation results, initially using an asymmetric multivariate normal approximation to the joint posterior of the parameters. An an improved estimate based on quadrature over the parameter grid (by default using AGHQ) can be obtained using the `calcMarginalLogLikImproved` method of the nested approximation object.\n\nHowever, much caution is warranted. The quantity is ill-defined when the prior is improper. Furthermore, even with proper priors, if the prior is diffuse, the integral will generally average over values of the parameters that give very poor fits to the data, which can produce misleading results for model selection. \n",
    "supporting": [
      "nimbleQuad_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}