{
  "hash": "2bd9223bcae0ec61460aa43b2967726d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian Nonparametric Models in NIMBLE, Part 1: Density Estimation\"\ndate: \"2018-12-04\"\nauthor: \"NIMBLE Development Team\"\nformat:\n  html:\n    toc: true\n    toc-depth: 2\ncategories: [\"announcement\"]\nexecute:\n  freeze: auto\n---\n\nBayesian Nonparametric Models in NIMBLE, Part 1: Density Estimation  \n  \n\n#  Bayesian nonparametrics in NIMBLE: Density estimation \n\n##  Overview \n\nNIMBLE is a hierarchical modeling package that uses nearly the same language for model specification as the popular MCMC packages WinBUGS, OpenBUGS and JAGS, while making the modeling language extensible — you can add distributions and functions — and also allowing customization of the algorithms used to estimate the parameters of the model.\n\nRecently, we added support for Markov chain Monte Carlo (MCMC) inference for Bayesian nonparametric (BNP) mixture models to NIMBLE. In particular, starting with version 0.6-11, NIMBLE provides functionality for fitting models involving Dirichlet process priors using either the Chinese Restaurant Process (CRP) or a truncated stick-breaking (SB) representation of the Dirichlet process prior.\n\nIn this post we illustrate NIMBLE’s BNP capabilities by showing how to use nonparametric mixture models with different kernels for density estimation. In a later post, we will take a parametric generalized linear mixed model and show how to switch to a nonparametric representation of the random effects that avoids the assumption of normally-distributed random effects.\n\nFor more detailed information on NIMBLE and Bayesian nonparametrics in NIMBLE, see the [NIMBLE User Manual](https://r-nimble.org/documentation).\n\n##  Basic density estimation using Dirichlet Process Mixture models \n\nNIMBLE provides the machinery for nonparametric density estimation by means of Dirichlet process mixture (DPM) models (Ferguson, 1974; Lo, 1984; Escobar, 1994; Escobar and West, 1995). For an independent and identically distributed sample $y_1, \\ldots, y_n$, the model takes the form\n\n$$y_i \\mid \\theta_i \\sim p(y_i \\mid \\theta_i), \\quad \\theta_i \\mid G \\sim G, \\quad G \\mid  \\alpha, H \\sim \\mbox{DP}(\\alpha, H), \\quad i=1,\\ldots, n .$$\n\nThe NIMBLE implementation of this model is flexible and allows for mixtures of arbitrary kernels, $p(y_i \\mid \\theta)$, which can be either conjugate or non-conjugate to the (also arbitrary) base measure $H$. In the case of conjugate kernel / base measure pairs, NIMBLE is able to detect the presence of the conjugacy and use it to improve the performance of the sampler.\n\nTo illustrate these capabilities, we consider the estimation of the probability density function of the waiting time between eruptions of the Old Faithful volcano data set available in R. \n    \n\n::: {.cell}\n\n```{.r .cell-code}\ndata(faithful)\n```\n:::\n\n\nThe observations $y_1, \\ldots, y_n$ correspond to the second column of the dataframe, and $n = 272$.\n\n##  Fitting a location-scale mixture of Gaussian distributions using the CRP representation \n\n###  Model specification\n\nWe first consider a location-scale Dirichlet process mixture of normal distributionss fitted to the transformed data $y_i^{*} = \\log (y_i)$:\n\n$$y^{*}_i \\mid \\mu_i, \\sigma^2_i \\sim \\mbox{N}(\\mu_i, \\sigma^2_i), \\quad (\\mu_i, \\sigma^2_i) \\mid G \\sim G, \\quad G \\mid \\alpha, H \\sim \\mbox{DP}(\\alpha, H), \\quad i=1,\\ldots, n,$$\n\nwhere $H$ corresponds to a normal-inverse-gamma distribution. This model can be interpreted as providing a Bayesian version of kernel density estimation for $y^{*}_i$ using Gaussian kernels and _adaptive bandwidths_. On the original scale of the data, this translates into an adaptive log-Gaussian kernel density estimate.\n\nIntroducing auxiliary variables $\\xi_1, \\ldots, \\xi_n$ that indicate which component of the mixture generates each observation, and integrating over the random measure $G$, we obtain the CRP representation of the model (Blackwell and MacQueen, 1973):\n\n$$y_i^{*} \\mid \\{ \\tilde{\\mu}_k \\}, \\{ \\tilde{\\sigma}_k^{2} \\} \\sim \\mbox{N}\\left( \\tilde{\\mu}_{\\xi_i}, \\tilde{\\sigma}^2_{\\xi_i} \\right), \\quad\\quad \\xi \\mid \\alpha \\sim \\mbox{CRP}(\\alpha), \\quad\\quad (\\tilde{\\mu}_k, \\tilde{\\sigma}_k^2) \\mid H \\sim H, \\quad\\quad i=1,\\ldots, n ,$$\n\nwhere\n\n$$p(\\xi \\mid \\alpha) = \\frac{\\Gamma(\\alpha)}{\\Gamma(\\alpha + n)} \\alpha^{K(\\xi)} \\prod_k  \\Gamma\\left(m_k(\\xi)\\right),$$\n\n$K(\\xi) \\le n$ is the number of unique values in the vector $\\xi$, and $m_k(\\xi)$ is the number of times the $k$-th unique value appears in $\\xi$. This specification makes it clear that each observation belongs to any of _at most_ $n$ normally distributed clusters, and that the CRP distribution corresponds to the prior distribution on the partition structure. \n\nNIMBLE’s specification of this model is given by\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nimble)\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncode <- nimbleCode({\n  for(i in 1:n) {\n    y[i] ~ dnorm(mu[i], var = s2[i])\n    mu[i] <- muTilde[xi[i]]\n    s2[i] <- s2Tilde[xi[i]]\n  }\n  xi[1:n] ~ dCRP(alpha, size = n)\n  for(i in 1:n) {\n    muTilde[i] ~ dnorm(0, var = s2Tilde[i])\n    s2Tilde[i] ~ dinvgamma(2, 1)\n  }\n  alpha ~ dgamma(1, 1)\n})\n```\n:::\n\n\nNote that in the model code the length of the parameter vectors `muTilde` and `s2Tilde` has been set to $n$. We do this because the current implementation of NIMBLE requires that the length of vector of parameters be set in advance and does not allow for their number to change between iterations. Hence, if we are to ensure that the algorithm always performs as intended we need to work with the worst case scenario, i.e., the case where there are as many components as observations. While this ensures that the algorithm always works as intended, it is also somewhat inefficient, both in terms of memory requirements (when $n$ is large a large number of unoccupied components need to be maintained) and in terms of computational burden (a large number of parameters that are not required for posterior inference need to be updated at every iteration). When we use a mixture of gamma distributions below, we will show a computational shortcut that improves the efficiency.\n\nNote also that the value of $\\alpha$ controls the number of components we expect a priori, with larger values of $\\alpha$ corresponding to a larger number of components occupied by the data. Hence, by assigning a prior to $\\alpha$ we add flexibility to the model specification. The particular choice of a Gamma prior allows NIMBLE to use a data-augmentation scheme to efficiently sample from the corresponding full conditional distribution. Alternative prior specifications for $\\alpha$ are possible, in which case the default sampler for this parameter is an adaptive random-walk Metropolis-Hastings algorithm.\n\n###  Running the MCMC algorithm\n\nThe following code sets up the data and constants, initializes the parameters, defines the model object, and builds and runs the MCMC algorithm. Because the specification is in terms of a Chinese restaurant process, the default sampler selected by NIMBLE is a collapsed Gibbs sampler (Neal, 2000).\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n# Model Data\nlFaithful <- log(faithful$waiting)\nstandlFaithful <- (lFaithful - mean(lFaithful)) / sd(lFaithful)\ndata <- list(y = standlFaithful)\n# Model Constants\nconsts <- list(n = length(standlFaithful))\n# Parameter initialization\ninits <- list(xi = sample(1:10, size=consts$n, replace=TRUE),\n              muTilde = rnorm(consts$n, 0, sd = sqrt(10)),\n              s2Tilde = rinvgamma(consts$n, 2, 1),\n              alpha = 1)\n# Model creation and compilation\nrModel <- nimbleModel(code, data = data, inits = inits, constants = consts)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDefining model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nBuilding model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting data and initial values\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nChecking model sizes and dimensions\n```\n\n\n:::\n\n```{.r .cell-code}\ncModel <- compileNimble(rModel)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n\n```{.r .cell-code}\n# MCMC configuration, creation, and compilation\nconf <- configureMCMC(rModel, monitors = c(\"xi\", \"muTilde\", \"s2Tilde\", \"alpha\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n===== Monitors =====\nthin = 1: alpha, muTilde, s2Tilde, xi\n===== Samplers =====\nCRP_concentration sampler (1)\n  - alpha\nCRP_cluster_wrapper sampler (544)\n  - s2Tilde[]  (272 elements)\n  - muTilde[]  (272 elements)\nCRP sampler (1)\n  - xi[1:272] \n```\n\n\n:::\n\n```{.r .cell-code}\nmcmc <- buildMCMC(conf)\ncmcmc <- compileNimble(mcmc, project = rModel)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n\n```{.r .cell-code}\nsamples <- runMCMC(cmcmc, niter = 7000, nburnin = 2000, setSeed = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrunning chain 1...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n```\n\n\n:::\n:::\n\n\nWe can extract the samples from the posterior distributions of the parameters and create trace plots, histograms, and any other summary of interest. For example, for the concentration parameter $\\alpha$ we have:\n    \n\n::: {.cell}\n\n```{.r .cell-code}\n# Trace plot for the concentration parameter\nts.plot(samples[ , \"alpha\"], xlab = \"iteration\", ylab = expression(alpha))\n```\n\n::: {.cell-output-display}\n![](bayesian-nonparametric-models-in-nimble-part-1-density-estimation_files/figure-html/plot1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Posterior histogram\nhist(samples[ , \"alpha\"], xlab = expression(alpha), main = \"\", ylab = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](bayesian-nonparametric-models-in-nimble-part-1-density-estimation_files/figure-html/plot1-2.png){width=672}\n:::\n\n```{.r .cell-code}\nquantile(samples[ , \"alpha\"], c(0.5, 0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      50%      2.5%     97.5% \n0.4230550 0.0580579 1.5608958 \n```\n\n\n:::\n:::\n\n\nUnder this model, the posterior predictive distribution for a new observation $\\tilde{y}$, $p(\\tilde{y} \\mid y_1, \\ldots, y_n)$, is the optimal density estimator (under squared error loss). Samples for this estimator can be easily computed from the samples generated by our MCMC:\n    \n\n::: {.cell}\n\n```{.r .cell-code}\n# posterior samples of the concentration parameter\nalphaSamples <- samples[ , \"alpha\"]\n# posterior samples of the cluster means\nmuTildeSamples <- samples[ , grep('muTilde', colnames(samples))]\n# posterior samples of the cluster variances\ns2TildeSamples <- samples[ , grep('s2Tilde', colnames(samples))]\n# posterior samples of the cluster memberships\nxiSamples <- samples [ , grep('xi', colnames(samples))]\n\nstandlGrid <- seq(-2.5, 2.5, len = 200) # standardized grid on log scale\n\ndensitySamplesStandl <- matrix(0, ncol = length(standlGrid), nrow = nrow(samples))\nfor(i in 1:nrow(samples)){\n  k <- unique(xiSamples[i, ])\n  kNew <- max(k) + 1\n  mk <- c()\n  li <- 1\n  for(l in 1:length(k)) {\n    mk[li] <- sum(xiSamples[i, ] == k[li])\n    li <- li + 1\n  }\n  alpha <- alphaSamples[i]\n\n  muK <-  muTildeSamples[i, k]\n  s2K <-  s2TildeSamples[i, k]\n  muKnew <-  muTildeSamples[i, kNew]\n  s2Knew <-  s2TildeSamples[i, kNew]\n\n  densitySamplesStandl[i, ] <- sapply(standlGrid,\n                function(x)(sum(mk * dnorm(x, muK, sqrt(s2K))) +\n                alpha * dnorm(x, muKnew, sqrt(s2Knew)) )/(alpha+consts$n))\n}\n\nhist(data$y, freq = FALSE, xlim = c(-2.5, 2.5), ylim = c(0,0.75), main = \"\",\n     xlab = \"Waiting times on standardized log scale\")\n## pointwise estimate of the density for standardized log grid\nlines(standlGrid, apply(densitySamplesStandl, 2, mean), lwd = 2, col = 'black')\nlines(standlGrid, apply(densitySamplesStandl, 2, quantile, 0.025), lty = 2, col = 'black')\nlines(standlGrid, apply(densitySamplesStandl, 2, quantile, 0.975), lty = 2, col = 'black')\n```\n\n::: {.cell-output-display}\n![](bayesian-nonparametric-models-in-nimble-part-1-density-estimation_files/figure-html/plot2-1.png){width=672}\n:::\n:::\n\n\nRecall, however, that this is the density estimate for the logarithm of the waiting time. To obtain the density on the original scale we need to apply the appropriate transformation to the kernel.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nlgrid <- standlGrid*sd(lFaithful) + mean(lFaithful) # grid on log scale\ndensitySamplesl <- densitySamplesStandl / sd(lFaithful) # density samples for grid on log scale\n\nhist(faithful$waiting, freq = FALSE, xlim = c(40, 100), ylim=c(0, 0.05),\n     main = \"\", xlab = \"Waiting times\")\nlines(exp(lgrid), apply(densitySamplesl, 2, mean)/exp(lgrid), lwd = 2, col = 'black')\nlines(exp(lgrid), apply(densitySamplesl, 2, quantile, 0.025)/exp(lgrid), lty = 2,\n      col = 'black')\nlines(exp(lgrid), apply(densitySamplesl, 2, quantile, 0.975)/exp(lgrid), lty = 2,\n      col = 'black')\n```\n\n::: {.cell-output-display}\n![](bayesian-nonparametric-models-in-nimble-part-1-density-estimation_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nIn either case, there is clear evidence that the data has two components for the waiting times.\n\n###  Generating samples from the mixing distribution\n\nWhile samples from the posterior distribution of linear functionals of the mixing distribution $G$ (such as the predictive distribution above) can be computed directly from the realizations of the collapsed sampler, inference for non-linear functionals of $G$ requires that we first generate samples from the mixing distribution. In NIMBLE we can get posterior samples from the random measure $G$, using the `getSamplesDPmeasure` function. Note that, in order to get posterior samples from $G$, we need to monitor all the random variables involved in its computations, i.e., the membership variable, `xi`, the cluster parameters, `muTilde` and `s2Tilde`, and the concentration parameter, `alpha`.\n\nThe following code generates posterior samples from the random measure $G$. The `cMCMC` object includes the model and posterior samples from the parameters. The `getSamplesDPmeasure` returns posterior samples from the random measure as a list (of length equal to the number of samples) of matrices, where each matrix represents the mixture for that iteration, with $p+1$ columns, where $p$ is the dimension of the vector of parameters with distribution $G$ (in this example $p=2$).\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nsamplesG <- getSamplesDPmeasure(cmcmc)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n```\n\n\n:::\n:::\n\n\nThe following code computes posterior samples of $P(\\tilde{y} > 70)$ using the posterior samples from the random measure $G$. Note that these samples are computed based on the transformed model and a value larger than 70 corresponds to a value larger than 0.03557236 on the above defined grid.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweightIndex <- grep('weight', colnames(samplesG[[1]]))\nmuTildeIndex <- grep('muTilde', colnames(samplesG[[1]]))\ns2TildeIndex <- grep('s2Tilde', colnames(samplesG[[1]]))\n\nprobY70 <- rep(0, nrow(samples))  # posterior samples of P(y.tilde > 70)\nfor(i in seq_len(nrow(samples))) {\n  probY70[i] <- sum(samplesG[[i]][, weightIndex] *\n                    pnorm(0.03557236, mean = samplesG[[i]][, muTildeIndex],\n                      sd = sqrt(samplesG[[i]][, s2TildeIndex]), lower.tail = FALSE))\n}\n\nhist(probY70,  xlab = \"Probability\", ylab = \"P(yTilde > 70 | data)\" , main = \"\" )\n```\n\n::: {.cell-output-display}\n![](bayesian-nonparametric-models-in-nimble-part-1-density-estimation_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n##  Fitting a mixture of gamma distributions using the CRP representation\n\nNIMBLE is not restricted to using Gaussian kernels in DPM models. In the case of the Old Faithful data, an alternative to the mixture of Gaussian kernels on the logarithmic scale that we presented in the previous section is a (scale-and-shape) mixture of Gamma distributions on the _original_ scale of the data. \n\n###  Model specification\n\nIn this case, the model takes the form\n\n$$y_i \\mid \\{ \\tilde{\\beta}_k \\}, \\{ \\tilde{\\lambda}_k \\} \\sim \\mbox{Gamma}\\left( \\tilde{\\beta}_{\\xi_i}, \\tilde{\\lambda}_{\\xi_i} \\right), \\quad\\quad \\xi \\mid \\alpha \\sim \\mbox{CRP}(\\alpha), \\quad\\quad (\\tilde{\\beta}_k, \\tilde{\\lambda}_k) \\mid H \\sim H ,$$\n\nwhere $H$ corresponds to the product of two independent Gamma distributions. The following code provides the NIMBLE specification for the model:\n    \n\n::: {.cell}\n\n```{.r .cell-code}\ncode <- nimbleCode({\n  for(i in 1:n) {\n    y[i] ~ dgamma(shape = beta[i], scale = lambda[i])\n    beta[i] <- betaTilde[xi[i]]\n    lambda[i] <- lambdaTilde[xi[i]]\n  }\n  xi[1:n] ~ dCRP(alpha, size = n)\n  for(i in 1:50) { # only 50 cluster parameters\n    betaTilde[i] ~ dgamma(shape = 71, scale = 2)\n    lambdaTilde[i] ~ dgamma(shape = 2, scale = 2)\n  }\n  alpha ~ dgamma(1, 1)\n})\n```\n:::\n\n\nNote that in this case the vectors `betaTilde` and `lambdaTilde` have length $50 \\ll n = 272$. This is done to reduce the computational and storage burdens associated with the sampling algorithm. You could think about this approach as truncating the process, except that it can be thought of as an *exact* truncation. Indeed, under the CRP representation, using parameter vector(s) with a length that is shorter than the number of observations in the sample will lead to a proper algorithm as long as the number of components instatiated by the sampler is strictly lower than the length of the parameter vector(s) for every iteration of the sampler.\n\n###  Running the MCMC algorithm\n\nThe following code sets up the model data and constants, initializes the parameters, defines the model object, and builds and runs the MCMC algorithm for the mixture of Gamma distributions. Note that, when building the MCMC, a warning message about the number of cluster parameters is generated. This is because the lengths of `betaTilde` and `lambdaTilde` are smaller than $n$. Also, note that no error message is generated during execution, which indicates that the number of clusters required never exceeded the maximum of 50.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- list(y = faithful$waiting)\nset.seed(1)\ninits <- list(xi = sample(1:10, size=consts$n, replace=TRUE),\n              betaTilde = rgamma(50, shape = 71, scale = 2),\n              lambdaTilde = rgamma(50, shape = 2, scale = 2),\n              alpha = 1)\nrModel <- nimbleModel(code, data = data, inits = inits, constants = consts)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDefining model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nBuilding model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting data and initial values\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nChecking model sizes and dimensions\n```\n\n\n:::\n\n```{.r .cell-code}\ncModel <- compileNimble(rModel)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n\n```{.r .cell-code}\nconf <- configureMCMC(rModel, monitors = c(\"xi\", \"betaTilde\", \"lambdaTilde\", \"alpha\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n===== Monitors =====\nthin = 1: alpha, betaTilde, lambdaTilde, xi\n===== Samplers =====\nCRP_concentration sampler (1)\n  - alpha\nCRP_cluster_wrapper sampler (100)\n  - betaTilde[]  (50 elements)\n  - lambdaTilde[]  (50 elements)\nCRP sampler (1)\n  - xi[1:272] \n```\n\n\n:::\n\n```{.r .cell-code}\nmcmc <- buildMCMC(conf)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n  [Warning] sampler_CRP: The number of clusters based on the cluster parameters\n            is less than the number of potential clusters. The MCMC is not\n            strictly valid if it ever proposes more components than cluster\n            parameters exist; NIMBLE will warn you if this occurs.\n```\n\n\n:::\n\n```{.r .cell-code}\ncmcmc <- compileNimble(mcmc, project = rModel)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n\n```{.r .cell-code}\nsamples <- runMCMC(cmcmc, niter = 7000, nburnin = 2000, setSeed = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrunning chain 1...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n```\n\n\n:::\n:::\n\n\nIn this case we use the posterior samples of the parameters to construct a trace plot and estimate the posterior distribution of  $\\alpha$:\n    \n\n::: {.cell}\n\n```{.r .cell-code}\n# Trace plot of the posterior samples of the concentration parameter\nts.plot(samples[ , 'alpha'], xlab = \"iteration\", ylab = expression(alpha))\n```\n\n::: {.cell-output-display}\n![](bayesian-nonparametric-models-in-nimble-part-1-density-estimation_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Histogram of the posterior samples for the concentration parameter \nhist(samples[ , 'alpha'], xlab = expression(alpha), ylab = \"Frequency\", main = \"\")\n```\n\n::: {.cell-output-display}\n![](bayesian-nonparametric-models-in-nimble-part-1-density-estimation_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n:::\n\n\n###  Generating samples from the mixing distribution\n\nAs before, we obtain samples from the posterior distribution of  $G$ using the `getSamplesDPmeasure` function. \n    \n\n::: {.cell}\n\n```{.r .cell-code}\nsamplesG <- getSamplesDPmeasure(cmcmc)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n```\n\n\n:::\n\n```{.r .cell-code}\n## sampleDPmeasure: Approximating the random measure by a finite stick-breaking representation with and error smaller than 1e-10, leads to a truncation level of 28.\n```\n:::\n\n\nWe use these samples to create an estimate of the density of the data along with a pointwise 95% credible band:\n    \n\n::: {.cell}\n\n```{.r .cell-code}\ngrid <- seq(40, 100, len = 200)\n\nweightIndex <- grep('weight', colnames(samplesG[[1]]))\nbetaTildeIndex <- grep('betaTilde', colnames(samplesG[[1]]))\nlambdaTildeIndex <- grep('lambdaTilde', colnames(samplesG[[1]]))\n\ndensitySamples <- matrix(0, ncol = length(grid), nrow = nrow(samples))\nfor(iter in seq_len(nrow(samples))) {\n  densitySamples[iter, ] <- sapply(grid, function(x)\n    sum(samplesG[[iter]][ , weightIndex] * dgamma(x, shape = samplesG[[iter]][ , betaTildeIndex],\n                scale = samplesG[[iter]][ , lambdaTildeIndex])))\n}\n\nhist(faithful$waiting, freq = FALSE, xlim = c(40,100), ylim = c(0, .05), main = \"\",\n   ylab = \"\", xlab = \"Waiting times\")\nlines(grid, apply(densitySamples, 2, mean), lwd = 2, col = 'black')\nlines(grid, apply(densitySamples, 2, quantile, 0.025), lwd = 2, lty = 2, col = 'black')\nlines(grid, apply(densitySamples, 2, quantile, 0.975), lwd = 2, lty = 2, col = 'black')\n```\n\n::: {.cell-output-display}\n![](bayesian-nonparametric-models-in-nimble-part-1-density-estimation_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nAgain, we see that the density of the data is bimodal, and looks very similar to the one we obtained before.\n\n##  Fitting a DP mixture of Gammas using a stick-breaking representation\n\n###  Model specification\n\nAn alternative representation of the Dirichlet process mixture uses the stick-breaking representation of the random distribution $G$ (Sethuraman, 1994). NIMBLE allows us to specify an approximation that involves a truncation of the Dirichlet process to a finite number of atoms, $L$. The resulting model therefore reduces to a finite mixture with $L$ components and a very particular prior on the weights of the mixture components.\n\nIntroducing auxiliary variables, $z_1, \\ldots, z_n$, that indicate which component generated each observation, the corresponding model for the mixture of Gamma densities discussed in the previous section takes the form\n\n$$y_i \\mid \\{ {\\beta}_k^{\\star} \\}, \\{ {\\lambda}_k^{\\star} \\}, z_i \\sim \\mbox{Gamma}\\left( {\\beta}_{z_i}^{\\star}, {\\lambda}_{z_i}^{\\star} \\right), \\quad\\quad \\boldsymbol{z} \\mid \\boldsymbol{w} \\sim \\mbox{Discrete}(\\boldsymbol{w}), \\quad\\quad ({\\beta}_k^{\\star}, {\\lambda}_k^{\\star}) \\mid H \\sim H ,$$\n\nwhere $H$ is again the product of two independent Gamma distributions,\n\n$$w_1=v_1, \\quad\\quad w_l=v_l\\prod_{m=1}^{l-1}(1-v_m), \\quad l=2, \\ldots, L-1,\\quad\\quad w_L=\\prod_{m=1}^{L-1}(1-v_m)$$\n\nwith $v_l \\mid \\alpha\\sim \\mbox{Beta}(1, \\alpha), l=1, \\ldots, L-1$. The following code provides the NIMBLE specification for the model:\n    \n\n::: {.cell}\n\n```{.r .cell-code}\ncode <- nimbleCode(\n  {\n    for(i in 1:n) {\n      y[i] ~ dgamma(shape = beta[i], scale = lambda[i])\n      beta[i] <- betaStar[z[i]]\n      lambda[i] <- lambdaStar[z[i]]\n      z[i] ~ dcat(w[1:Trunc])\n    }\n    for(i in 1:(Trunc-1)) { # stick-breaking variables\n      v[i] ~ dbeta(1, alpha)\n    }\n    w[1:Trunc] <- stick_breaking(v[1:(Trunc-1)]) # stick-breaking weights\n    for(i in 1:Trunc) {\n      betaStar[i] ~ dgamma(shape = 71, scale = 2)\n      lambdaStar[i] ~ dgamma(shape = 2, scale = 2)\n    }\n    alpha ~ dgamma(1, 1)\n  }\n)\n```\n:::\n\n\nNote that the truncation level $L$ of $G$ has been set to a value `Trunc`, which is to be defined in the `constants` argument of the `nimbleModel` function.\n\n###  Running the MCMC algorithm\n\nThe following code sets up the model data and constants, initializes the parameters, defines the model object, and builds and runs the MCMC algorithm for the mixture of Gamma distributions. When a stick-breaking representation is used, a blocked Gibbs sampler is assigned (Ishwaran, 2001; Ishwaran and James, 2002).\n    \n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- list(y = faithful$waiting)\nset.seed(1)\nconsts <- list(n = length(faithful$waiting), Trunc = 50)\ninits <- list(betaStar = rgamma(consts$Trunc, shape = 71, scale = 2),\n              lambdaStar = rgamma(consts$Trunc, shape = 2, scale = 2),\n              v = rbeta(consts$Trunc-1, 1, 1),\n              z = sample(1:10, size = consts$n, replace = TRUE),\n              alpha = 1)\n\nrModel <- nimbleModel(code, data = data, inits = inits, constants = consts)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDefining model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nBuilding model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting data and initial values\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nChecking model sizes and dimensions\n```\n\n\n:::\n\n```{.r .cell-code}\ncModel <- compileNimble(rModel)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n\n```{.r .cell-code}\nconf <- configureMCMC(rModel, monitors = c(\"w\", \"betaStar\", \"lambdaStar\", 'z', 'alpha'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n===== Monitors =====\nthin = 1: alpha, betaStar, lambdaStar, w, z\n===== Samplers =====\nRW sampler (101)\n  - betaStar[]  (50 elements)\n  - lambdaStar[]  (50 elements)\n  - alpha\nconjugate sampler (49)\n  - v[]  (49 elements)\ncategorical sampler (272)\n  - z[]  (272 elements)\n```\n\n\n:::\n\n```{.r .cell-code}\nmcmc <- buildMCMC(conf)\ncmcmc <- compileNimble(mcmc, project = rModel)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n\n```{.r .cell-code}\nsamples <- runMCMC(cmcmc, niter = 24000, nburnin = 4000, setSeed = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrunning chain 1...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n```\n\n\n:::\n:::\n\n\nUsing the stick-breaking approximation automatically provides an approximation, $G_L$, of the random distribution $G$. The following code computes posterior samples of $G_L$ using posterior samples from the `samples` object, and from them, a density estimate for the data.\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nbetaStarSamples <- samples[ , grep('betaStar', colnames(samples))]\nlambdaStarSamples <- samples[ , grep('lambdaStar', colnames(samples))]\nweightSamples <- samples[ , grep('w', colnames(samples))]\n\ngrid <- seq(40, 100, len = 200)\n\ndensitySamples <- matrix(0, ncol = length(grid), nrow = nrow(samples))\nfor(i in 1:nrow(samples)) {\n  densitySamples[i, ] <- sapply(grid, function(x)\n    sum(weightSamples[i, ] * dgamma(x, shape = betaStarSamples[i, ],\n                                    scale = lambdaStarSamples[i, ])))\n}\n\nhist(faithful$waiting, freq = FALSE,  xlab = \"Waiting times\", ylim=c(0,0.05),\n     main = '')\nlines(grid, apply(densitySamples, 2, mean), lwd = 2, col = 'black')\nlines(grid, apply(densitySamples, 2, quantile, 0.025), lwd = 2, lty = 2, col = 'black')\nlines(grid, apply(densitySamples, 2, quantile, 0.975), lwd = 2, lty = 2, col = 'black')\n```\n\n::: {.cell-output-display}\n![](bayesian-nonparametric-models-in-nimble-part-1-density-estimation_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nAs expected, this estimate looks identical to the one we obtained through the CRP representation of the process.\n\n##  More information and future development \n\nPlease see our [User Manual](https://r-nimble.org/documentation) for more details.\n\nWe’re in the midst of improvements to the existing BNP functionality as well as adding additional Bayesian nonparametric models, such as hierarchical Dirichlet processes and Pitman-Yor processes, so please add yourself to our [announcement](https://groups.google.com/forum/#!forum/nimble-announce) or [user support/discussion](https://groups.google.com/forum/#!forum/nimble-users) Google groups. \n\n###  References \n\nBlackwell, D. and MacQueen, J. 1973. Ferguson distributions via Polya urn schemes. The Annals of Statistics 1:353-355.\n\nFerguson, T.S. 1974. Prior distribution on the spaces of probability measures. Annals of Statistics 2:615-629.\n\nLo, A.Y. 1984. On a class of Bayesian nonparametric estimates I: Density estimates. The Annals of Statistics 12:351-357.\n\nEscobar, M.D. 1994. Estimating normal means with a Dirichlet process prior. Journal of the American Statistical Association 89:268-277.\n\nEscobar, M.D. and West, M. 1995. Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association 90:577-588.\n\nIshwaran, H. and James, L.F. 2001. Gibbs sampling methods for stick-breaking priors. Journal of the American Statistical Association 96: 161-173.\n\nIshwaran, H. and James, L.F. 2002. Approximate Dirichlet process computing in finite normal mixtures: smoothing and prior information. Journal of Computational and Graphical Statistics 11:508-532.\n\nNeal, R. 2000. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics 9:249-265.\n\nSethuraman, J. 1994. A constructive definition of Dirichlet prior. Statistica Sinica 2: 639-650.",
    "supporting": [
      "bayesian-nonparametric-models-in-nimble-part-1-density-estimation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}