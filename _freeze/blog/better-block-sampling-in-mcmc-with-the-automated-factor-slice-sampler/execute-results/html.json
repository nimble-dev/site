{
  "hash": "5ffa889ec14ecbadcb13bd5b01ea4348",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Better block sampling in MCMC with the Automated Factor Slice Sampler\"\ndate: \"2017-05-09\"\nauthor: \"Nicholas Michaud\"\nformat:\n  html:\n    toc: true\n    toc-depth: 2\ncategories: [\"announcement\", \"tutorial\", \"R\"]\nexecute:\n  freeze: auto\n---\n\nOne nice feature of NIMBLE’s MCMC system is that a user can easily write new samplers from R, combine them with NIMBLE’s samplers, and have them automatically compiled to C++ via the NIMBLE compiler. We’ve observed that block sampling using a simple adaptive multivariate random walk Metropolis-Hastings sampler doesn’t always work well in practice, so we decided to implement the Automated Factor Slice sampler (AFSS) of `Tibbits, Groendyke, Haran, and Liechty (2014)` and see how it does on a (somewhat artificial) example with severe posterior correlation problems.\n\nRoughly speaking, the AFSS works by conducting univariate slice sampling in directions determined by the eigenvectors of the marginal posterior covariance matrix for blocks of parameters in a model. So far, we’ve found the AFSS often outperforms random walk block sampling. To compare performance, we look at MCMC efficiency, which we define for each parameter as effective sample size (ESS) divided by computation time. We define overall MCMC efficiency as the minimum MCMC efficiency of all the parameters, because one needs all parameters to be well mixed.\n\nWe’ll demonstrate the performance of the AFSS on the correlated state space model described in `Turek, de  \nValpine, Paciorek, Anderson-Bergman, and others (2017)`.\n\n# Model Creation\n\nAssume $x_{i}$ is the latent state and $y_{i}$ is the observation at time $i$ for $i=1,\\ldots,100$. We define the state space model as\n\n$$ x_{i} \\sim N(a \\cdot x_{i-1} + b, \\sigma_{PN}) $$\n$$ y_{i} \\sim N(x_{i}, \\sigma_{OE}) $$\n\nfor $i = 2, \\ldots, 100$, with initial states\n\n$$ x_{1} \\sim N\\left(\\frac{b}{1-a}, \\frac{\\sigma_{PN}}{\\sqrt{1-a^2}}\\right) $$\n$$ y_{1} \\sim N(x_{1}, \\sigma_{OE}) $$\n\nand prior distributions\n\n$$ a \\sim \\text{Unif}(-0.999, 0.999) $$\n$$ b \\sim N(0, 1000) $$\n$$ \\sigma_{PN} \\sim \\text{Unif}(0, 1) $$\n$$ \\sigma_{OE} \\sim \\text{Unif}(0, 1) $$\n\nwhere $N(\\mu, \\sigma)$ denotes a normal distribution with mean $\\mu$ and standard deviation $\\sigma$.\n\nA file named `model_SSMcorrelated.RData` with the BUGS model code, data, constants, and initial values for our model can be downloaded [here](https://github.com/danielturek/automated-blocking-examples/blob/master/data/model_SSMcorrelated.RData).\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary('nimble')\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nload('model_SSMcorrelated.RData')\n## build and compile the model\nstateSpaceModel <- nimbleModel(code = code,\n                              data = data,\n                              constants = constants,\n                              inits = inits,\n                              check = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDefining model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nBuilding model\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting data and initial values\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nChecking model sizes and dimensions\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n  [Note] This model is not fully initialized. This is not an error.\n         To see which variables are not initialized, use model$initializeInfo().\n         For more information on model initialization, see help(modelInitialization).\n```\n\n\n:::\n\n```{.r .cell-code}\nC_stateSpaceModel <- compileNimble(stateSpaceModel)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n:::\n\n\n# Comparing two MCMC Samplers\n\nWe next compare the performance of two MCMC samplers on the state space model described above. The first sampler we consider is NIMBLE’s `RW_block` sampler, a Metropolis-Hastings sampler with a multivariate normal proposal distribution. This sampler has an adaptive routine that modifies the proposal covariance to look like the empirical covariance of the posterior samples of the parameters. However, as we shall see below, this proposal covariance adaptation does not lead to efficient sampling for our state space model.\n\nWe first build and compile the MCMC algorithm.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nRW_mcmcConfig <- configureMCMC(stateSpaceModel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n===== Monitors =====\nthin = 1: a, b, sigOE, sigPN\n===== Samplers =====\nRW sampler (3)\n  - a\n  - sigPN\n  - sigOE\nconjugate sampler (101)\n  - b\n  - x[]  (100 elements)\n```\n\n\n:::\n\n```{.r .cell-code}\nRW_mcmcConfig$removeSamplers(c('a', 'b', 'sigOE', 'sigPN'))\nRW_mcmcConfig$addSampler(target = c('a', 'b', 'sigOE', 'sigPN'), type = 'RW_block')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n  [Note] Assigning an RW_block sampler to nodes with very different scales can result in low MCMC efficiency.  If all nodes assigned to RW_block are not on a similar scale, we recommend providing an informed value for the \"propCov\" control list argument, or using the \"barker\" sampler instead.\n```\n\n\n:::\n\n```{.r .cell-code}\nRW_mcmc <- buildMCMC(RW_mcmcConfig)\nC_RW_mcmc <- compileNimble(RW_mcmc, project = stateSpaceModel)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n:::\n\n\nWe next run the compiled MCMC algorithm for 10,000 iterations, recording the overall MCMC efficiency from the posterior output. The overall efficiency here is defined as $\\min(\\frac{ESS}{T})$, where ESS denotes the effective sample size, and $T$ the total run-time of the sampling algorithm. The minimum is taken over all parameters that were sampled. We repeat this process 5 times to get a very rough idea of the average minimum efficiency for this combination of model and sampler.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(coda)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'coda'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked _by_ '.GlobalEnv':\n\n    densplot\n```\n\n\n:::\n\n```{.r .cell-code}\nRW_minEfficiency <- numeric(5)\nfor(i in 1:5){\n  runTime <- system.time(C_RW_mcmc$run(50000, progressBar = FALSE))['elapsed']\n  RW_mcmcOutput <- as.mcmc(as.matrix(C_RW_mcmc$mvSamples))\n  RW_minEfficiency[i] <- min(effectiveSize(RW_mcmcOutput)/runTime)\n}\nsummary(RW_minEfficiency)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.2679  0.3482  0.4693  0.5713  0.5903  1.1809 \n```\n\n\n:::\n:::\n\n    \n\nExamining a trace plot of the output below, we see that the $a$ and $b$ parameters are mixing especially poorly.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nplot(RW_mcmcOutput, density = FALSE)\n```\n\n::: {.cell-output-display}\n![](better-block-sampling-in-mcmc-with-the-automated-factor-slice-sampler_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nPlotting the posterior samples of $a$ against those of $b$ reveals a strong negative correlation. This presents a problem for the Metropolis-Hastings sampler — we have found that adaptive algorithms used to tune the proposal covariance are often slow to reach a covariance that performs well for blocks of strongly correlated parameters.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nplot.default(RW_mcmcOutput[,'a'], RW_mcmcOutput[,'b'])\n```\n\n::: {.cell-output-display}\n![](better-block-sampling-in-mcmc-with-the-automated-factor-slice-sampler_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n    \n\n::: {.cell}\n\n```{.r .cell-code}\ncor(RW_mcmcOutput[,'a'], RW_mcmcOutput[,'b'])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.9964729\n```\n\n\n:::\n:::\n\n\nIn such situations with strong posterior correlation, we’ve found the AFSS to often run much more efficiently, so we next build and compile an MCMC algorithm using the AFSS sampler. Our hope is that the AFSS sampler will be better able to to produce efficient samples in the face of high posterior correlation.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nAFSS_mcmcConfig <- configureMCMC(stateSpaceModel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n===== Monitors =====\nthin = 1: a, b, sigOE, sigPN\n===== Samplers =====\nRW sampler (3)\n  - a\n  - sigPN\n  - sigOE\nconjugate sampler (101)\n  - b\n  - x[]  (100 elements)\n```\n\n\n:::\n\n```{.r .cell-code}\nAFSS_mcmcConfig$removeSamplers(c('a', 'b', 'sigOE', 'sigPN'))\nAFSS_mcmcConfig$addSampler(target = c('a', 'b', 'sigOE', 'sigPN'), type = 'AF_slice')\nAFSS_mcmc<- buildMCMC(AFSS_mcmcConfig)\nC_AFSS_mcmc <- compileNimble(AFSS_mcmc, project = stateSpaceModel, resetFunctions = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n```\n\n\n:::\n:::\n\n\nWe again run the AFSS MCMC algorithm 5 times, each with 10,000 MCMC iterations.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nAFSS_minEfficiency <- numeric(5)\nfor(i in 1:5){\n  runTime <- system.time(C_AFSS_mcmc$run(50000, progressBar = FALSE))['elapsed']\n  AFSS_mcmcOutput <- as.mcmc(as.matrix(C_AFSS_mcmc$mvSamples))\n  AFSS_minEfficiency[i] <- min(effectiveSize(AFSS_mcmcOutput)/runTime)\n}\nsummary(AFSS_minEfficiency)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  23.94   25.77   30.75   29.95   31.56   37.71 \n```\n\n\n:::\n:::\n\n   \n\nNote that the minimum overall efficiency of the AFSS sampler is approximately `28` times that of the `RW_block` sampler. Additionally, trace plots from the output of the AFSS sampler show that the $a$ and $b$ parameters are mixing much more effectively than they were under the `RW_block` sampler.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nplot(AFSS_mcmcOutput, density = FALSE)\n```\n\n::: {.cell-output-display}\n![](better-block-sampling-in-mcmc-with-the-automated-factor-slice-sampler_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\nTibbits, M. M, C. Groendyke, M. Haran, et al. (2014). “Automated factor slice sampling”.  \nIn: _Journal of Computational and Graphical Statistics_ 23.2, pp. 543–563.\n\nTurek, D, P. de Valpine, C. J. Paciorek, et al. (2017).  \n“Automated parameter blocking for efficient Markov chain Monte Carlo sampling”.  \nIn: _Bayesian Analysis_ 12.2, pp. 465–490.",
    "supporting": [
      "better-block-sampling-in-mcmc-with-the-automated-factor-slice-sampler_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}