[
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributions",
    "section": "",
    "text": "We welcome contributions to help improve NIMBLE.\nThese may include:\n\nNew algorithms to the nimble repository.\nUsage examples to the demos repository.\nImprovements or updates to our documentation in the docs repository.\nTutorial or workshop materials to the outreach repository.\n\n\nHow to Contribute\nContributions to NIMBLE can be made by forking the NIMBLE repository and submitting a pull request with your changes.\nIf you have questions about this workflow, or would like to request membership to the NIMBLE Github organization, please contact the development team."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NIMBLE",
    "section": "",
    "text": "Other packages that use the BUGS language are only for Markov chain Monte Carlo (MCMC). With NIMBLE, you can turn BUGS code into model objects and use them for whatever algorithm you want. That includes algorithms provided with NIMBLE and algorithms you write using nimbleFunctions. NIMBLE extends BUGS by allowing multiple parameterizations for distributions, user-written functions and distributions, and more."
  },
  {
    "objectID": "index.html#write-statistical-models-in-the-bugs-language-from-r",
    "href": "index.html#write-statistical-models-in-the-bugs-language-from-r",
    "title": "NIMBLE",
    "section": "",
    "text": "Other packages that use the BUGS language are only for Markov chain Monte Carlo (MCMC). With NIMBLE, you can turn BUGS code into model objects and use them for whatever algorithm you want. That includes algorithms provided with NIMBLE and algorithms you write using nimbleFunctions. NIMBLE extends BUGS by allowing multiple parameterizations for distributions, user-written functions and distributions, and more."
  },
  {
    "objectID": "index.html#use-and-customize-nimbles-statistical-algorithms",
    "href": "index.html#use-and-customize-nimbles-statistical-algorithms",
    "title": "NIMBLE",
    "section": "Use and customize NIMBLE’s statistical algorithms",
    "text": "Use and customize NIMBLE’s statistical algorithms\n\nNIMBLE provides MCMC, sequential Monte Carlo (particle filters), and more.\nNIMBLE algorithms are written so they can adapt to different statistical models. For MCMC, NIMBLE can assign a default set of sampler choices, but you can customize the samplers from R. For example, you can choose what parameters to sample in a block, and you can easily write your own samplers and include them."
  },
  {
    "objectID": "index.html#compile-your-models-and-algorithms-for-fast-execution.",
    "href": "index.html#compile-your-models-and-algorithms-for-fast-execution.",
    "title": "NIMBLE",
    "section": "Compile your models and algorithms for fast execution.",
    "text": "Compile your models and algorithms for fast execution.\n\nNIMBLE generates C++ code customized to your model and algorithms, compiles it, and lets you use it from R.\nYou don’t need to know anything about C++ to use NIMBLE’s compiler. NIMBLE provides R functions to call the compiled algorithms, and you get the output back in R. (You do need to have a C++ compiler and related tools installed. See installation instructions.)"
  },
  {
    "objectID": "index.html#write-your-own-algorithms-in-nimble",
    "href": "index.html#write-your-own-algorithms-in-nimble",
    "title": "NIMBLE",
    "section": "Write your own algorithms in NIMBLE",
    "text": "Write your own algorithms in NIMBLE\n\nWriting new statistical methods using nimbleFunctions in R is similar to writing R functions.\nIf you have a method you’d like to implement, you can program it using nimbleFunctions. The syntax is very similar to R, but you’ll need to learn some details to get started. The nimbleFunction system allows programmers to control how a particular algorithm should adapt to each model and/or variables it is applied to. The NIMBLE compiler can make nimbleFunctions run very efficiently."
  },
  {
    "objectID": "index.html#compile-numerical-work-in-r-via-c-without-coding-any-c.",
    "href": "index.html#compile-numerical-work-in-r-via-c-without-coding-any-c.",
    "title": "NIMBLE",
    "section": "Compile numerical work in R via C++ without coding any C++.",
    "text": "Compile numerical work in R via C++ without coding any C++.\n\nDon’t care about models written in the BUGS language? Just want to try making your R code to go faster?\nnimbleFunctions don’t need to use BUGS models, so you can use them to speed up many kinds of numerical computations for any other purpose. The NIMBLE compiler can handle math, including linear algebra and distributions. It also supports basic iteration, flow control, and data structures."
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "New as of version 0.9.1:\n\nThe NIMBLE cheatsheet — a compact two-page overview and reference guide."
  },
  {
    "objectID": "documentation.html#cheatsheet",
    "href": "documentation.html#cheatsheet",
    "title": "Documentation",
    "section": "",
    "text": "New as of version 0.9.1:\n\nThe NIMBLE cheatsheet — a compact two-page overview and reference guide."
  },
  {
    "objectID": "documentation.html#user-manual",
    "href": "documentation.html#user-manual",
    "title": "Documentation",
    "section": "User Manual",
    "text": "User Manual\n\nAn HTML version of the manual.\nA PDF version of the manual.\n\nThe manual provides information for those wishing to use NIMBLE to work with their own models as well as algorithm developers wishing to write algorithms using NIMBLE. Old versions of the manual can be found here."
  },
  {
    "objectID": "documentation.html#training-materials",
    "href": "documentation.html#training-materials",
    "title": "Documentation",
    "section": "Training materials",
    "text": "Training materials\nPlease see our collection of Github repositories with training materials for materials from our various NIMBLE training workshops, including workshops developed for statisticans and workshops developed for ecologists."
  },
  {
    "objectID": "documentation.html#other-links",
    "href": "documentation.html#other-links",
    "title": "Documentation",
    "section": "Other links",
    "text": "Other links\n\nExamples of spatial and spatio-temporal models (specifically for disease mapping) provided by Andrew Lawson.\nBlog post on converting from JAGS or BUGS to NIMBLE.\nFirst and second blog posts on support for Bayesian non-parametric distributions.\nBlog post on reversible jump sampling for variable selection.\nUsing NIMBLE for the examples in the book Applied Hierarchical Modeling in Ecology.."
  },
  {
    "objectID": "documentation.html#some-papers-about-nimble",
    "href": "documentation.html#some-papers-about-nimble",
    "title": "Documentation",
    "section": "Some papers about NIMBLE",
    "text": "Some papers about NIMBLE\n\nMotivation and design of NIMBLE:\nde Valpine, P., D. Turek, C.J. Paciorek, C. Anderson-Bergman, D. Temple Lang, and R. Bodik. 2017. Programming with models: writing statistical algorithms for general model structures with NIMBLE. Journal of Computational and Graphical Statistics 26:403-413. https://doi.org/10.1080/10618600.2016.1172487.\n\n\nNIMBLE for Hidden Markov Models:\nTurek, D., P. de Valpine, and C.J. Paciorek. 2016. Efficient Markov chain Monte Carlo sampling for hierarchical hidden Markov models. Environmental and Ecological Statistics 23:549–564. https://doi.org/10.1007/s10651-016-0353-z\n\n\nNIMBLE for Ecological Models:\nPonisio, L.C., P. de Valpine, N. Michaud, and D. Turek. 2020. One size does not fit all: Customizing MCMC methods for hierarchical models using NIMBLE. Ecology & Evolution 10: 2385– 2416. https://doi.org/10.1002/ece3.6053\n\n\nSequential Monte Carlo (particle filtering) methods in NIMBLE:\nMichaud, N., P. de Valpine, D. Turek, C.J. Paciorek, and D. Nguyen. 2021. Sequential Monte Carlo Methods in the nimble and nimbleSMC R Packages. Journal of Statistical Software 100(3): 1-39. https://doi.org/10.18637/jss.v100.i03\n\n\nSpatial Epidemiology in NIMBLE:\nLawson, A.B. 2020. NIMBLE for Bayesian Disease Mapping. Spatial and Spatio-temporal Epidemiology 33. https://doi.org/10.1016/j.sste.2020.100323\n\n\nNIMBLE for item response theory models:\nPaganin, S., C.J. Paciorek, C. Wehrhahn, A. Rodríguez, S. Rabe-Hesketh, and P. de Valpine. 2021. Computational methods for Bayesian semiparametric Item Response Theory models. https://arxiv.org/abs/2101.11583.\n\n\nWAIC in NIMBLE\nHug, J.E., and C.J. Paciorek. 2021. A numerically stable online implementation and exploration of WAIC through variations of the predictive density, using NIMBLE. https://arxiv.org/abs/2106.13359."
  },
  {
    "objectID": "documentation.html#packages-with-extensions-and-applications-of-nimble",
    "href": "documentation.html#packages-with-extensions-and-applications-of-nimble",
    "title": "Documentation",
    "section": "Packages with extensions and applications of NIMBLE",
    "text": "Packages with extensions and applications of NIMBLE\nA partial list of packages that extend or use nimble.\n\nnimbleSMC: all of NIMBLE’s sequential Monte Carlo (aka particle filtering) algorithms; migrated out of the core NIMBLE package as of version 0.10.0.\nnimbleEcology: distributions commonly used in ecology for use in nimble models\nnimbleSCR: utility functions, distributions, and methods for improving Markov chain Monte Carlo (MCMC) sampling efficiency for ecological spatial capture-recapture (SCR) models.\nbayesNSGP: Bayesian analysis of (non-stationary) Gaussian processes, using nimble as the computational engine.\nbcgam: Bayesian constrained generalized linear models\nbridgesampling: functions for estimating marginal likelihoods, Bayes factors, posterior model probabilities, and normalizing constants in general, via different versions of bridge sampling.\nnimbleDistance: user-defined distributions that can be used to implement distance sampling models in nimble.\nnimbleCarbon: utility functions and bespoke probability distributions for the Bayesian analyses of radiocarbon dates."
  },
  {
    "objectID": "groups-and-issues.html",
    "href": "groups-and-issues.html",
    "title": "Groups and Issues",
    "section": "",
    "text": "We will post (occasional) announcements on the nimble-announce Google groups site.\nWe have an email list on the nimble-users Google groups site. This is intended for questions about NIMBLE, requests for new features, and other discussion of NIMBLE."
  },
  {
    "objectID": "groups-and-issues.html#groups",
    "href": "groups-and-issues.html#groups",
    "title": "Groups and Issues",
    "section": "",
    "text": "We will post (occasional) announcements on the nimble-announce Google groups site.\nWe have an email list on the nimble-users Google groups site. This is intended for questions about NIMBLE, requests for new features, and other discussion of NIMBLE."
  },
  {
    "objectID": "groups-and-issues.html#issues",
    "href": "groups-and-issues.html#issues",
    "title": "Groups and Issues",
    "section": "Issues",
    "text": "Issues\nIssues and bugs can be reported via the issue tracker of the nimble repository or via the nimble-users Google groups site. General “how to” questions are welcome. If reporting a problem, please provide a reproducible example. For compilation errors, please provide your operating system."
  },
  {
    "objectID": "download.html",
    "href": "download.html",
    "title": "Download",
    "section": "",
    "text": "Before installing NIMBLE, you need a compiler and related tools such as make that R can use. You’ll need these anyway to use NIMBLE.\nOn Windows you can get these by installing Rtools.exe from here. Important: with R version 4.0 or newer, for which you need Rtools40, be sure to follow the instructions in the section “Putting Rtools on the PATH” to set the PATH after the installation is complete. For older versions of R, for which you would use Rtools35.exe, be sure to check the box that will modify your PATH during the installation process.\nOn Mac OS X you can get these by installing the XCode command line tools. (The full XCode development environment is fine but not required.)\nMore details and troubleshooting tips are provided in Section 4 of the User Manual.\nIf you install a new version of R, you will need to reinstall NIMBLE."
  },
  {
    "objectID": "download.html#prerequisites",
    "href": "download.html#prerequisites",
    "title": "Download",
    "section": "",
    "text": "Before installing NIMBLE, you need a compiler and related tools such as make that R can use. You’ll need these anyway to use NIMBLE.\nOn Windows you can get these by installing Rtools.exe from here. Important: with R version 4.0 or newer, for which you need Rtools40, be sure to follow the instructions in the section “Putting Rtools on the PATH” to set the PATH after the installation is complete. For older versions of R, for which you would use Rtools35.exe, be sure to check the box that will modify your PATH during the installation process.\nOn Mac OS X you can get these by installing the XCode command line tools. (The full XCode development environment is fine but not required.)\nMore details and troubleshooting tips are provided in Section 4 of the User Manual.\nIf you install a new version of R, you will need to reinstall NIMBLE."
  },
  {
    "objectID": "download.html#current-version-1.3.0",
    "href": "download.html#current-version-1.3.0",
    "title": "Download",
    "section": "Current version (1.3.0)",
    "text": "Current version (1.3.0)\n\nInstalling from CRAN\nNIMBLE is available from CRAN as a source package and as a binary package for Windows and Mac OS. Please note that even if you install from CRAN, you still need to have a compiler and related tools installed as discussed above and in Section 4 of our User Manual.\ninstall.packages(\"nimble\")\n\n\nInstalling from R-nimble.org\nIf you prefer, you can install the package directly from our repository as follows.\nIf you install in this manner, please first install the igraph, R6, coda, numDeriv, and pracma R packages as these are dependencies and will not be automatically installed when installing from source.\nOnly the installation from source works when installing from our repository. Invoke the following at the R command line:\n# Install from source for MacOS, Linux, or Windows:\ninstall.packages(\"nimble\", repos = \"https://r-nimble.org\", type = \"source\")\n# the 'type = \"source\"' is unnecessary for Linux\nOr download the source package for installation via R CMD INSTALL on Linux, Mac, or Windows\nThe NIMBLE source code can be viewed by unzipping the source package tarball or directly on our GitHub repository."
  },
  {
    "objectID": "download.html#installing-from-github",
    "href": "download.html#installing-from-github",
    "title": "Download",
    "section": "Installing from GitHub",
    "text": "Installing from GitHub\nYou can install either the current release or the latest development version from GitHub.\n# For the current release:\nremotes::install_github(\"nimble-dev/nimble\", ref = \"master\", subdir = \"packages/nimble\")\n\n# For the development release:\nremotes::install_github(\"nimble-dev/nimble\", ref = \"devel\", subdir = \"packages/nimble\")"
  },
  {
    "objectID": "download.html#older-versions",
    "href": "download.html#older-versions",
    "title": "Download",
    "section": "Older versions",
    "text": "Older versions\nPrevious releases of NIMBLE can be found here.\nFor a summary of changes in each version, please see the News page or the NEWS file in the package."
  },
  {
    "objectID": "download.html#beta-testing-of-automatic-differentiation-features",
    "href": "download.html#beta-testing-of-automatic-differentiation-features",
    "title": "Download",
    "section": "Beta testing of automatic differentiation features",
    "text": "Beta testing of automatic differentiation features\nAutomatic differentiation, including support for Hamiltonian Monte Carlo and Laplace (and AGHQ) approximation is part of NIMBLE as of the version 1.0.0 release. Instructions for using it are in our User Manual."
  },
  {
    "objectID": "archived-versions-of-nimble-and-the-user-manual.html",
    "href": "archived-versions-of-nimble-and-the-user-manual.html",
    "title": "Archived Versions of NIMBLE and the User Manual",
    "section": "",
    "text": "This page provides access to previous versions of NIMBLE and their corresponding documentation."
  },
  {
    "objectID": "archived-versions-of-nimble-and-the-user-manual.html#current-version",
    "href": "archived-versions-of-nimble-and-the-user-manual.html#current-version",
    "title": "Archived Versions of NIMBLE and the User Manual",
    "section": "Current Version",
    "text": "Current Version\n\nNIMBLE 1.3.0 - Download | User Manual"
  },
  {
    "objectID": "archived-versions-of-nimble-and-the-user-manual.html#previous-versions",
    "href": "archived-versions-of-nimble-and-the-user-manual.html#previous-versions",
    "title": "Archived Versions of NIMBLE and the User Manual",
    "section": "Previous Versions",
    "text": "Previous Versions\n\nVersion 1.2.x Series\n\nNIMBLE 1.2.1 - Source Package\nNIMBLE 1.2.0 - Source Package\n\n\n\nVersion 1.1.x Series\n\nNIMBLE 1.1.0 - Source Package\n\n\n\nVersion 1.0.x Series\n\nNIMBLE 1.0.1 - Source Package\nNIMBLE 1.0.0 - Source Package\n\n\n\nVersion 0.13.x Series\n\nNIMBLE 0.13.1 - Source Package\nNIMBLE 0.13.0 - Source Package\n\n\n\nVersion 0.12.x Series\n\nNIMBLE 0.12.2 - Source Package\nNIMBLE 0.12.1 - Source Package\n\n\n\nOlder Versions\nFor versions prior to 0.12.x, please check our GitHub releases page or contact the development team."
  },
  {
    "objectID": "archived-versions-of-nimble-and-the-user-manual.html#installation-instructions",
    "href": "archived-versions-of-nimble-and-the-user-manual.html#installation-instructions",
    "title": "Archived Versions of NIMBLE and the User Manual",
    "section": "Installation Instructions",
    "text": "Installation Instructions\nTo install an archived version:\n# Download the source package and install\ninstall.packages(\"path/to/nimble_X.Y.Z.tar.gz\", repos = NULL, type = \"source\")\n\n# Or install directly from URL\ninstall.packages(\"https://r-nimble.org/src/contrib/Archive/nimble/nimble_X.Y.Z.tar.gz\", \n                 repos = NULL, type = \"source\")"
  },
  {
    "objectID": "archived-versions-of-nimble-and-the-user-manual.html#user-manual-archives",
    "href": "archived-versions-of-nimble-and-the-user-manual.html#user-manual-archives",
    "title": "Archived Versions of NIMBLE and the User Manual",
    "section": "User Manual Archives",
    "text": "User Manual Archives\n\nCurrent User Manual: HTML | PDF\nPrevious versions of the manual are included with each package release"
  },
  {
    "objectID": "archived-versions-of-nimble-and-the-user-manual.html#release-notes",
    "href": "archived-versions-of-nimble-and-the-user-manual.html#release-notes",
    "title": "Archived Versions of NIMBLE and the User Manual",
    "section": "Release Notes",
    "text": "Release Notes\nFor detailed information about changes in each version, see our Release Notes page or the NEWS file included with each package."
  },
  {
    "objectID": "archived-versions-of-nimble-and-the-user-manual.html#support-for-older-versions",
    "href": "archived-versions-of-nimble-and-the-user-manual.html#support-for-older-versions",
    "title": "Archived Versions of NIMBLE and the User Manual",
    "section": "Support for Older Versions",
    "text": "Support for Older Versions\nWhile we encourage users to upgrade to the latest version, we understand that some projects may require older versions. Limited support is available through our community forum for older versions."
  },
  {
    "objectID": "blog/nimbles-sequential-monte-carlo-smc-algorithms-are-now-in-the-nimblesmc-package.html",
    "href": "blog/nimbles-sequential-monte-carlo-smc-algorithms-are-now-in-the-nimblesmc-package.html",
    "title": "NIMBLE’s sequential Monte Carlo (SMC) algorithms are now in the nimbleSMC package",
    "section": "",
    "text": "We’ve moved NIMBLE’s various sequential Monte Carlo (SMC) algorithms (bootstrap particle filter, auxiliary particle filter, ensemble Kalman filter, iterated filter2, and particle MCMC algorithms) to the new nimbleSMC package. So if you want to use any of these methods as of nimble version 0.10.0, please make sure to install the nimbleSMC package. Any existing code you have that uses any SMC functionality should continue to work as is.\nAs development work on NIMBLE has proceeded over the years, we’ve added additional functionality to the core nimble package, so we’ve reached the stage where it makes sense to break off some of the functionality into separate packages. Our goal is to make the overall NIMBLE platform more modular and therefore easier to maintain and use."
  },
  {
    "objectID": "blog/nimble-short-course-june-3-4-2020-at-uc-berkeley.html",
    "href": "blog/nimble-short-course-june-3-4-2020-at-uc-berkeley.html",
    "title": "NIMBLE short course, June 3-4, 2020 at UC Berkeley",
    "section": "",
    "text": "We’ll be holding a two-day training workshop on NIMBLE, June 3-4, 2020 in Berkeley, California. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nThe tutorial will cover\n\nthe basic concepts and workflows for using NIMBLE and converting BUGS or JAGS models to work in NIMBLE.\noverview of different MCMC sampling strategies and how to use them in NIMBLE.\nwriting new distributions and functions for more flexible modeling and more efficient computation.\ntips and tricks for improving computational efficiency.\nusing advanced model components, including Bayesian non-parametric distributions (based on Dirichlet process priors), conditional auto-regressive (CAR) models for spatially correlated random fields, and reversible jump samplers for variable selection.\nan introduction to programming new algorithms in NIMBLE.\ncalling R and compiled C++ code from compiled NIMBLE models or functions.\n\nIf participant interests vary sufficiently, the second half-day will be split into two tracks. One of these will likely focus on ecological models. The other will be chosen based on attendee interest from topics such as (a) advanced NIMBLE programming including writing new MCMC samplers, (b) advanced spatial or Bayesian non-parametric modeling, or (c) non-MCMC algorithms in NIMBLE such as sequential Monte Carlo. Prior to the workshop, we will survey attendee interests and adjust content to meet attendee interests.\nIf you are interested in attending, please pre-register to hold a spot at https://forms.gle/6AtNgfdUdvhni32Q6. The form also asks if you are interested in a relatively cheap dormitory-style housing option. No payment is necessary to pre-register. Fees to finalize registration will be $230 (regular) or $115 (student). We hope to be able to offer student travel awards; more information will follow.\nThe workshop will assume attendees have a basic understanding of hierarchical/Bayesian models and MCMC, the BUGS (or JAGS) model language, and some familiarity with R."
  },
  {
    "objectID": "blog/version-0-6-12-of-nimble-released.html",
    "href": "blog/version-0-6-12-of-nimble-released.html",
    "title": "Version 0.6-12 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.6-12 is primarily a maintenance release with various bug fixes.\nChanges include:\n\na fix for the bootstrap particle filter to correctly calculate weights when particles are not resampled (the filter had been omitting the previous weights when calculating the new weights);\naddition of an option to print MCMC samplers of a particular type;\navoiding an overly-aggressive check for ragged arrays when building models; and\navoiding assigning a sampler to non-conjugacy inverse-Wishart nodes (thereby matching our handling of Wishart nodes).\n\nPlease see the NEWS file in the installed package for more details."
  },
  {
    "objectID": "blog/bug-in-newly-released-version-0-13-0-affecting-mcmc-for-models-with-predictive-nodes.html",
    "href": "blog/bug-in-newly-released-version-0-13-0-affecting-mcmc-for-models-with-predictive-nodes.html",
    "title": "Bug in newly-released version 0.13.0 affecting MCMC for models with predictive nodes",
    "section": "",
    "text": "We recently released version 0.13.0, which has some improvements in how we handle predictive nodes in NIMBLE’s MCMC engine.\nUnfortunately, we realized (thanks to a user post from a couple days ago) that there is a bug in this new approach to predictive nodes.\nIf you haven’t upgraded to version 0.13.0, simply wait to upgrade until we release a bug fix in 0.13.1 in the next couple weeks.\nIf you have upgraded to version 0.13.0 and if you have run an MCMC on a model that both (1) has predictive nodes and (2) has multivariate nodes, then the bug might affect your results. Please set:\n\nnimbleOptions(MCMCusePredictiveDependenciesInCalculations = TRUE)\n\nand then reconfigure/rebuild and rerun your MCMC. The option above will ensure that the MCMC behaves as it would in previous versions of NIMBLE."
  },
  {
    "objectID": "blog/version-0-5-1-of-nimble-released.html",
    "href": "blog/version-0-5-1-of-nimble-released.html",
    "title": "Version 0.5-1 of NIMBLE released!",
    "section": "",
    "text": "Version 0.5-1 is officially a minor release, but it actually has quite a bit in it, in particular the addition/improvement of a number of our algorithms. In addition there are some more improvements in our speed in building and compiling models and algorithms.\nChanges as of Version 0.5-1 include:\n\nthe addition of a variety of sequential Monte Carlo (aka particle filtering) algorithms, including particle MCMC samplers for use within an MCMC,\na greatly improved MCEM algorithm with an automated convergence and stopping criterion,\nnew syntax for declaring multivariate variables in the NIMBLE DSL, namely numeric(), integer(), matrix(), and array(), with declare() now deprecated,\naddition of the multivariate-t distribution for use in BUGS and DSL code,\na new binary MCMC sampler for discrete 0/1 nodes,\naddition of functionality to our random walk sampler to allow sampling on the log scale and use of reflection,\nmore flexible use of forwardsolve(), backsolve(), and solve(), including use in BUGS code, and\na variety of other items.\n\nPlease see the NEWS file in the source package."
  },
  {
    "objectID": "blog/writing-reversible-jump-mcmc-in-nimble.html",
    "href": "blog/writing-reversible-jump-mcmc-in-nimble.html",
    "title": "Writing reversible jump MCMC in NIMBLE",
    "section": "",
    "text": "Writing reversible jump MCMC samplers in NIMBLE"
  },
  {
    "objectID": "blog/writing-reversible-jump-mcmc-in-nimble.html#introduction",
    "href": "blog/writing-reversible-jump-mcmc-in-nimble.html#introduction",
    "title": "Writing reversible jump MCMC in NIMBLE",
    "section": "Introduction",
    "text": "Introduction\nReversible jump Markov chain Monte Carlo (RJMCMC) is a powerful method for drawing posterior samples over multiple models by jumping between models as part of the sampling. For a simple example that I’ll use below, think about a regression model where we don’t know which explanatory variables to include, so we want to do variable selection. There may be a huge number of possible combinations of variables, so it would be nice to explore the combinations as part of one MCMC run rather than running many different MCMCs on some chosen combinations of variables. To do it in one MCMC, one sets up a model that includes all possible variables and coefficients. Then “removing” a variable from the model is equivalent to setting its coefficient to zero, and “adding” it back into the model requires a valid move to a non-zero coefficient. Reversible jump MCMC methods provide a way to do that.\nReversible jump is different enough from other MCMC situations that packages like WinBUGS, OpenBUGS, JAGS, and Stan don’t do it. An alternative way to set up the problem, which does not involve the technicality of changing model dimension, is to use indicator variables. An indicator variable is either zero or one and is multiplied by another parameter. Thus when the indicator is 0, the parameter that is multipled by 0 is effectively removed from the model. Darren Wilkinson has a nice old blog post on using indicator variables for Bayesian variable selection in BUGS code. The problem with using indicator variables is that they can create a lot of extra MCMC work and the samplers operating on them may not be well designed for their situation.\nNIMBLE lets one program model-generic algorithms to use with models written in the BUGS language. The MCMC system works by first making a configuration in R, which can be modified by a user or a program, and then building and compiling the MCMC. The nimbleFunction programming system makes it easy to write new kinds of samplers.\nThe aim of this blog post is to illustrate how one can write reversible jump MCMC in NIMBLE. A variant of this may be incorporated into a later version of NIMBLE."
  },
  {
    "objectID": "blog/writing-reversible-jump-mcmc-in-nimble.html#example-model",
    "href": "blog/writing-reversible-jump-mcmc-in-nimble.html#example-model",
    "title": "Writing reversible jump MCMC in NIMBLE",
    "section": "Example model",
    "text": "Example model\nFor illustration, I’ll use an extremely simple model: linear regression with two candidate explanatory variables. I’ll assume the first, x1, should definitely be included. But the analyst is not sure about the second, x2, and wants to use reversible jump to include it or exclude it from the model. I won’t deal with the issue of choosing the prior probability that it should be in the model. Instead I’ll just pick a simple choice and stay focused on the reversible jump aspect of the example. The methods below could be applied en masse to large models.\nHere I’ll simulate data to use:\nN &lt;- 20\nx1 &lt;- runif(N, -1, 1)\nx2 &lt;- runif(N, -1, 1)\nY &lt;- rnorm(N, 1.5 + 0.5 * x1, sd = 1)\nI’ll take two approaches to implementing RJ sampling. In the first, I’ll use a traditional indicator variable and write the RJMCMC sampler to use it. In the second, I’ll write the RJMCMC sampler to incorporate the prior probability of inclusion for the coefficient it is sampling, so the indicator variable won’t be needed in the model.\nFirst we’ll need nimble:\nlibrary(nimble)"
  },
  {
    "objectID": "blog/writing-reversible-jump-mcmc-in-nimble.html#rjmcmc-implementation-1-with-indicator-variable-included",
    "href": "blog/writing-reversible-jump-mcmc-in-nimble.html#rjmcmc-implementation-1-with-indicator-variable-included",
    "title": "Writing reversible jump MCMC in NIMBLE",
    "section": "RJMCMC implementation 1, with indicator variable included",
    "text": "RJMCMC implementation 1, with indicator variable included\nHere is BUGS code for the first method, with an indicator variable written into the model, and the creation of a NIMBLE model object from it. Note that although RJMCMC technically jumps between models of different dimensions, we still start by creating the largest model so that changes of dimension can occur by setting some parameters to zero (or, in the second method, possibly another fixed value).\nsimpleCode1 &lt;- nimbleCode({\n    beta0 ~ dnorm(0, sd = 100)\n    beta1 ~ dnorm(0, sd = 100)\n    beta2 ~ dnorm(0, sd = 100)\n    sigma ~ dunif(0, 100)\n    z2 ~ dbern(0.8)  ## indicator variable for including beta2\n    beta2z2 &lt;- beta2 * z2\n    for(i in 1:N) {\n        Ypred[i] &lt;- beta0 + beta1 * x1[i] + beta2z2 * x2[i]\n        Y[i] ~ dnorm(Ypred[i], sd = sigma)\n    }\n})\n\nsimpleModel1 &lt;- nimbleModel(simpleCode1,\n                            data = list(Y = Y, x1 = x1, x2 = x2),\n                            constants = list(N = N),\n                            inits = list(beta0 = 0, beta1 = 0, beta2 = 0, sigma = sd(Y), z2 = 1))\nNow here are two custom samplers. The first one will sample beta2 only if the indicator variable z2 is 1 (meaning that beta2 is included in the model). It does this by containing a regular random walk sampler but only calling it when the indicator is 1 (we could perhaps set it up to contain any sampler to be used when z2 is 1, but for now it’s a random walk sampler). The second sampler makes reversible jump proposals to move beta2 in and out of the model. When it is out of the model, both beta2 and z2 are set to zero. Since beta2 will be zero every time z2 is zero, we don’t really need beta2z2, but it ensures correct behavior in other cases, like if someone runs default samplers on the model and expects the indicator variable to do its job correctly. For use in reversible jump, z2’s role is really to trigger the prior probability (set to 0.8 in this example) of being in the model.\nDon’t worry about the warning message emitted by NIMBLE. They are there because when a nimbleFunction is defined it tries to make sure the user knows anything else that needs to be defined.\nRW_sampler_nonzero_indicator &lt;- nimbleFunction(\n    contains = sampler_BASE,\n    setup = function(model, mvSaved, target, control) {\n        regular_RW_sampler &lt;- sampler_RW(model, mvSaved, target = target, control = control$RWcontrol)\n        indicatorNode &lt;- control$indicator\n    },\n    run = function() {\n        if(model[[indicatorNode]] == 1) regular_RW_sampler$run()\n    },\n    methods = list(\n        reset = function() {regular_RW_sampler$reset()}\n    ))\n\n\n\n## Warning in nf_checkDSLcode(code): For this nimbleFunction to compile, these\n## functions must be defined as nimbleFunctions or nimbleFunction methods:\n## reset.\n\n\n\nRJindicatorSampler &lt;- nimbleFunction(\n    contains = sampler_BASE,\n    setup = function( model, mvSaved, target, control ) {\n        ## target should be the name of the indicator node, 'z2' above\n        ## control should have an element called coef for the name of the corresponding coefficient, 'beta2' above.  \n        coefNode &lt;- control$coef\n        scale &lt;- control$scale\n        calcNodes &lt;- model$getDependencies(c(coefNode, target))\n    },\n    run = function( ) { ## The reversible-jump updates happen here.\n        currentIndicator &lt;- model[[target]]\n        currentLogProb &lt;- model$getLogProb(calcNodes)\n        if(currentIndicator == 1) {\n            ## propose removing it\n            currentCoef &lt;- model[[coefNode]]\n            logProbReverseProposal &lt;- dnorm(0, currentCoef, sd = scale, log = TRUE)\n            model[[target]] &lt;&lt;- 0\n            model[[coefNode]] &lt;&lt;- 0\n            proposalLogProb &lt;- model$calculate(calcNodes)\n            log_accept_prob &lt;- proposalLogProb - currentLogProb + logProbReverseProposal\n        } else {\n            ## propose adding it\n            proposalCoef &lt;- rnorm(1, 0, sd = scale)\n            model[[target]] &lt;&lt;- 1\n            model[[coefNode]] &lt;&lt;- proposalCoef\n            logProbForwardProposal &lt;- dnorm(0, proposalCoef, sd = scale, log = TRUE)\n            proposalLogProb &lt;- model$calculate(calcNodes)\n            log_accept_prob &lt;- proposalLogProb - currentLogProb - logProbForwardProposal\n        }\n        accept &lt;- decide(log_accept_prob)\n        if(accept) {\n            copy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)\n        } else {\n            copy(from = mvSaved, to = model, row = 1, nodes = calcNodes, logProb = TRUE)\n        }\n    },\n    methods = list(reset = function() {\n    })\n    )\nNow we’ll set up and run the samplers:\nmcmcConf1 &lt;- configureMCMC(simpleModel1)\nmcmcConf1$removeSamplers('z2')\nmcmcConf1$addSampler(target = 'z2',\n                     type = RJindicatorSampler,\n                     control = list(scale = 1, coef = 'beta2'))\nmcmcConf1$removeSamplers('beta2')\nmcmcConf1$addSampler(target = 'beta2',\n                     type = 'RW_sampler_nonzero_indicator',\n                     control = list(indicator = 'z2',\n                                    RWcontrol = list(adaptive = TRUE,\n                                                   adaptInterval = 100,\n                                                   scale = 1,\n                                                   log = FALSE,\n                                                   reflective = FALSE)))\n\nmcmc1 &lt;- buildMCMC(mcmcConf1)\ncompiled1 &lt;- compileNimble(simpleModel1, mcmc1)\ncompiled1$mcmc1$run(10000)\n\n\n\n## |-------------|-------------|-------------|-------------|\n## |-------------------------------------------------------|\n\n\n\n## NULL\n\n\n\nsamples1 &lt;- as.matrix(compiled1$mcmc1$mvSamples)\nHere is a trace plot of the beta2 (slope) samples. The thick line at zero corresponds to having beta2 removed from the model.\nplot(samples1[,'beta2'])\n\nAnd here is a trace plot of the z2 (indicator variable) samples.\nplot(samples1[,'z2'])\n\nThe chains look reasonable.\nAs a quick check of reasonableness, let’s compare the beta2 samples to what we’d get if it was always included in the model. I’ll do that by setting up default samplers and then removing the sampler for z2 (and z2 should be 1).\nmcmcConf1b &lt;- configureMCMC(simpleModel1)\nmcmcConf1b$removeSamplers('z2')\nmcmc1b &lt;- buildMCMC(mcmcConf1b)\ncompiled1b &lt;- compileNimble(simpleModel1, mcmc1b)\ncompiled1b$mcmc1b$run(10000)\n\n\n\n## |-------------|-------------|-------------|-------------|\n## |-------------------------------------------------------|\n\n\n\n## NULL\n\n\n\nsamples1b &lt;- as.matrix(compiled1b$mcmc1b$mvSamples)\nplot(samples1b[,'beta2'])\n\nqqplot(samples1[ samples1[,'z2'] == 1, 'beta2'], samples1b[,'beta2'])\nabline(0,1)\n\nThat looks correct, in the sense that the distribution of beta2 given that it’s in the model (using reversible jump) should match the distribution of beta2 when it is\nalways in the model."
  },
  {
    "objectID": "blog/writing-reversible-jump-mcmc-in-nimble.html#rj-implementation-2-without-indicator-variables",
    "href": "blog/writing-reversible-jump-mcmc-in-nimble.html#rj-implementation-2-without-indicator-variables",
    "title": "Writing reversible jump MCMC in NIMBLE",
    "section": "RJ implementation 2, without indicator variables",
    "text": "RJ implementation 2, without indicator variables\nNow I’ll set up the second version of the model and samplers. I won’t include the indicator variable in the model but will instead include the prior probability for inclusion in the sampler. One added bit of generality is that being “out of the model” will be defined as taking some fixedValue, to be provided, which will typically but not necessarily be zero. These functions are very similar to the ones above.\nHere is the code to define and build a model without the indicator variable:\nsimpleCode2 &lt;- nimbleCode({\n    beta0 ~ dnorm(0, sd = 100)\n    beta1 ~ dnorm(0, sd = 100)\n    beta2 ~ dnorm(0, sd = 100)\n    sigma ~ dunif(0, 100)\n    for(i in 1:N) {\n        Ypred[i] &lt;- beta0 + beta1 * x1[i] + beta2 * x2[i]\n        Y[i] ~ dnorm(Ypred[i], sd = sigma)\n    }\n})\n\nsimpleModel2 &lt;- nimbleModel(simpleCode2,\n                            data = list(Y = Y, x1 = x1, x2 = x2),\n                            constants = list(N = N),\n                            inits = list(beta0 = 0, beta1 = 0, beta2 = 0, sigma = sd(Y)))\nAnd here are the samplers (again, ignore the warning):\nRW_sampler_nonzero &lt;- nimbleFunction(\n    ## \"nonzero\" is a misnomer because it can check whether it sits at any fixedValue, not just 0\n    contains = sampler_BASE,\n    setup = function(model, mvSaved, target, control) {\n        regular_RW_sampler &lt;- sampler_RW(model, mvSaved, target = target, control = control$RWcontrol)\n        fixedValue &lt;- control$fixedValue\n    },\n    run = function() { ## Now there is no indicator variable, so check if the target node is exactly\n                       ## equal to the fixedValue representing \"not in the model\".\n        if(model[[target]] != fixedValue) regular_RW_sampler$run()\n    },\n    methods = list(\n        reset = function() {regular_RW_sampler$reset()}\n    ))\n\n\n\n## Warning in nf_checkDSLcode(code): For this nimbleFunction to compile, these\n## functions must be defined as nimbleFunctions or nimbleFunction methods:\n## reset.\n\n\n\nRJsampler &lt;- nimbleFunction(\n    contains = sampler_BASE,\n    setup = function( model, mvSaved, target, control ) {\n        ## target should be a coefficient to be set to a fixed value (usually zero) or not\n        ## control should have an element called fixedValue (usually 0),\n        ##    a scale for jumps to and from the fixedValue,\n        ##    and a prior prob of taking its fixedValue\n        fixedValue &lt;- control$fixedValue\n        scale &lt;- control$scale\n        ## The control list contains the prior probability of inclusion, and we can pre-calculate\n        ## this log ratio because it's what we'll need later.\n        logRatioProbFixedOverProbNotFixed &lt;- log(control$prior) - log(1-control$prior)\n        calcNodes &lt;- model$getDependencies(target)\n    },\n    run = function( ) { ## The reversible-jump moves happen here\n        currentValue &lt;- model[[target]]\n        currentLogProb &lt;- model$getLogProb(calcNodes)\n        if(currentValue != fixedValue) { ## There is no indicator variable, so check if current value matches fixedValue\n            ## propose removing it (setting it to fixedValue)\n            logProbReverseProposal &lt;- dnorm(fixedValue, currentValue, sd = scale, log = TRUE)\n            model[[target]] &lt;&lt;- fixedValue\n            proposalLogProb &lt;- model$calculate(calcNodes)\n            log_accept_prob &lt;- proposalLogProb - currentLogProb - logRatioProbFixedOverProbNotFixed + logProbReverseProposal\n        } else {\n            ## propose adding it\n            proposalValue &lt;- rnorm(1, fixedValue, sd = scale)\n            model[[target]] &lt;&lt;- proposalValue\n            logProbForwardProposal &lt;- dnorm(fixedValue, proposalValue, sd = scale, log = TRUE)\n            proposalLogProb &lt;- model$calculate(calcNodes)\n            log_accept_prob &lt;- proposalLogProb - currentLogProb + logRatioProbFixedOverProbNotFixed - logProbForwardProposal\n        }\n        accept &lt;- decide(log_accept_prob)\n        if(accept) {\n            copy(from = model, to = mvSaved, row = 1, nodes = calcNodes, logProb = TRUE)\n        } else {\n            copy(from = mvSaved, to = model, row = 1, nodes = calcNodes, logProb = TRUE)\n        }\n    },\n    methods = list(reset = function() {\n    })\n    )\nNow let’s set up and use the samplers\nmcmcConf2 &lt;- configureMCMC(simpleModel2)\nmcmcConf2$removeSamplers('beta2')\n\nmcmcConf2$addSampler(target = 'beta2',\n                     type = 'RJsampler',\n                     control = list(fixedValue = 0, prior = 0.8, scale = 1))\nmcmcConf2$addSampler(target = 'beta2',\n                     type = 'RW_sampler_nonzero',\n                     control = list(fixedValue = 0,\n                                     RWcontrol = list(adaptive = TRUE,\n                                     adaptInterval = 100,\n                                     scale = 1,\n                                     log = FALSE,\n                                     reflective = FALSE)))\n\nmcmc2 &lt;- buildMCMC(mcmcConf2)\ncompiled2 &lt;- compileNimble(simpleModel2, mcmc2)\n\ncompiled2$mcmc2$run(10000)\n\n\n\n## |-------------|-------------|-------------|-------------|\n## |-------------------------------------------------------|\n\n\n\n## NULL\n\n\n\nsamples2 &lt;- as.matrix(compiled2$mcmc2$mvSamples)\nAnd again let’s look at the samples. As above, the horizontal line at 0 represents having beta2 removed from the model.\nplot(samples2[,'beta2'])\n\nNow let’s compare those results to results from the first method, above. They should match.\nmean(samples1[,'beta2']==0)\n\n\n\n## [1] 0.12\n\n\n\nmean(samples2[,'beta2']==0)\n\n\n\n## [1] 0.1173\n\n\n\nqqplot(samples1[ samples1[,'beta2'] != 0,'beta2'], samples2[samples2[,'beta2'] != 0,'beta2'])\nabline(0,1)\n\nThey match well.\n\nHow to apply this for larger models.\nThe samplers above could be assigned to arbitrary nodes in a model. The only additional code would arise from adding more samplers to an MCMC configuration. It would also be possible to refine the reversible-jump step to adapt the scale of its jumps in order to achieve better mixing. For example, one could try this method by Ehlers and Brooks. We’re interested in hearing from you if you plan to try using RJMCMC on your own models.\nEn plus des crises existantes, le monde occidental a été frappé par une pénurie de médicaments. Et nous ne parlons pas principalement de la production de quelques molécules à forte intensité de main-d’œuvre, mais des médicaments fortlapersonne de base qui devraient se trouver dans chaque armoire à pharmacie, comme le paracétamol et les antibiotiques, dont la pénurie peut avoir un impact important sur la santé de la nation."
  },
  {
    "objectID": "blog/version-0-10-0-of-nimble-released.html",
    "href": "blog/version-0-10-0-of-nimble-released.html",
    "title": "Version 0.10.0 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nVersion 0.10.0 provides new features, improvements in speed of building models and algorithms, bug fixes, and various improvements.\nNew features and bug fixes include:\n\ngreatly extended NIMBLE’s Chinese Restaurant Process (CRP)-based Bayesian nonparametrics functionality by allowing multiple observations to be grouped together;\nfixed a bug giving incorrect results in our cross-validation function, runCrossValidate();\nmoved NIMBLE’s sequential Monte Carlo (SMC, aka particle filtering) methods into the nimbleSMC package; and\nimproved the efficiency of model and MCMC building and compilation.\n\nPlease see the release notes on our website for more details."
  },
  {
    "objectID": "blog/nimble-webinar-friday-april-13.html",
    "href": "blog/nimble-webinar-friday-april-13.html",
    "title": "NIMBLE webinar Friday April 13",
    "section": "",
    "text": "We’ll be presenting a webinar on NIMBLE, hosted by the Eastern North America Region of the International Biometric Society. Details are as follows.\nProgramming with hierarchical statistical models: An introduction to the BUGS-compatible NIMBLE system for MCMC and more\nFriday, April 13, 2018\n11:00 a.m. – 1:00 p.m. EST\nMust register before April 12. You can register here. (You’ll need to create an account on the ENAR website and there is a modest fee – from $25 for ENAR student members up through $85 for non-IBS members.)\nThis webinar will introduce attendees to the NIMBLE system for programming with hierarchical models in R. NIMBLE (r-nimble.org) is a system for flexible programming and dissemination of algorithms that builds on the BUGS language for declaring hierarchical models. NIMBLE provides analysts with a flexible system for using MCMC, sequential Monte Carlo and other techniques on user-specified models. It provides developers and methodologists with the ability to write algorithms in an R-like syntax that can be easily disseminated to users. C++ versions of models and algorithms are created for speed, but these are manipulated from R without any need for analysts or algorithm developers to program in C++.\nWhile analysts can use NIMBLE as a drop-in replacement for WinBUGS or JAGS, NIMBLE provides greatly enhanced functionality in a number of ways. The webinar will first show how to specify a hierarchical statistical model using BUGS syntax (including user-defined function and distributions) and fit that model using MCMC (including user customization for better performance). We will demonstrate the use of NIMBLE for biostatistical methods such as semiparametric random effects models and clustering models. We will close with a discussion of how to use the system to write algorithms for use with hierarchical models, including building and disseminating your own methods.\nPresenter:\nChris Paciorek\nAdjunct Professor, Statistical Computing Consultant\nDepartment of Statistics, University of California, Berkeley"
  },
  {
    "objectID": "blog/version-0-7-1-of-nimble-released.html",
    "href": "blog/version-0-7-1-of-nimble-released.html",
    "title": "Version 0.7.1 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nVersion 0.7.1 is primarily a maintenance release with a couple important bug fixes and a few additional features. Users with large models and users of the dCRP Bayesian nonparametric distribution are strongly encouraged to update to this version to pick up the bug fixes related to these uses.\nNew features include:\n\nrecognition of normal-normal conjugacy in additional multivariate regression settings;\nhandling of six-dimensional arrays in models.\n\nPlease see the release notes on our website for more details."
  },
  {
    "objectID": "blog/we-have-a-post-doc-opening.html",
    "href": "blog/we-have-a-post-doc-opening.html",
    "title": "We have a post-doc opening",
    "section": "",
    "text": "We have a 1-year opening for a post-doc interested in developing statistical methods in NIMBLE.\nHere is the official, approved job advertisement:\nPOSTDOCTORAL SCHOLAR POSITION AVAILABLE IN COMPUTATIONAL STATISTICS – UNIVERSITY OF CALIFORNIA, BERKELEY\nThe Departments of Statistics and Environmental Science Policy, and Management have an opening for a Postdoctoral Scholar – Employee to develop and apply statistical algorithms as part of the NIMBLE software development team. NIMBLE is a NSF-funded framework for programming computational methods for general hierarchical models such as Markov chain Monte Carlo, sequential Monte Carlo, and numerical integration and approximation. More information is available at R-nimble.org. The post- doc will be supervised by co-PIs Perry de Valpine and Chris Paciorek. We seek a candidate who will build out NIMBLE’s algorithm library, which includes using it as a platform for methodological and applied research. The successful candidate will be expected to author peer-reviewed publications and contribute to software development.\nBASIC QUALIFICATIONS\nCandidates must have completed all degree requirements except the dissertation or be enrolled in an accredited PhD or equivalent degree in a statistical field such as Statistics or Computer Science or a field of statistical application at the time of application.\nADDITIONAL QUALIFICATIONS\nCandidates must have a PhD or equivalent degree in a statistical field such as Statistics or Computer Science or in a field of statistical application such as biology, ecology, environmental science, political science, psychology, education, public health or related field by appointment start date.\nPREFERRED QUALIFICATIONS\nDemonstrated experience programming complex scientific computing applications using R and/or C++, Python or others. Demonstrated experience advancing computational statistical methodology by appointment start date.\nAPPOINTMENT\nThe position is available to start immediately but we seek the best candidate even if they cannot start until a later date. The initial appointment is for one-year, with renewal based on performance and funding. This is a full-time appointment.\nSALARY AND BENEFITS\nSalary will be commensurate with qualifications and experience. Generous benefits are included (http://vspa.berkeley.edu/postdocs)\nTO APPLY\nVisit: https://aprecruit.berkeley.edu/apply/JPF00860\nInterested individuals should include a 1-2 page cover letter describing their research experience and publications along with a current CV and the names and contact information of three references. Letters of reference may be requested for finalists. It is optional to include a statement addressing past and/or potential contributions to diversity through research, teaching, and/or service.\nThis position will remain open until filled.\nQuestions regarding this recruitment can be directed to Maria P. Aranas, aranas4@berkeley.edu.\nAll letters will be treated as confidential per University of California policy and California state law. Please refer potential referees, including when letters are provided via a third party (i.e. dossier service or career center) to the UC Berkeley Statement of Confidentiality (http://apo.berkeley.edu/evalltr.html ) prior to submitting their letters.\nThe University of California is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, age or protected veteran status. For the complete University of California nondiscrimination and affirmative action policy see: http://policy.ucop.edu/doc/4000376/NondiscrimAffirmAct\nThe Department is interested in candidates who will contribute to diversity and equal opportunity in higher education through their research or teaching.\nThe University of California, Berkeley has an excellent benefits package as well as a number of policies and programs in place to support employees as they balance work and family."
  },
  {
    "objectID": "blog/version-0-6-5-of-nimble-released.html",
    "href": "blog/version-0-6-5-of-nimble-released.html",
    "title": "Version 0.6-5 of NIMBLE released!",
    "section": "",
    "text": "We’ve just released the newest version of NIMBLE on CRAN and on our website. Version 0.6-5 is mostly devoted to bug fixes and packaging fixes for CRAN, but there is some new functionality:\n\naddition of the functions c(), seq(), rep(), :, diag() for use in BUGS code;\naddition of two improper distributions (dflat and dhalfflat) as well as the inverse-Wishart distribution;\nthe ability to estimate the asymptotic covariance of the estimates in NIMBLE’s MCEM algorithm;\nthe ability to use nimbleLists in any nimbleFunction, newly including nimbleFunctions without setup code;\nand a variety of bug fixes and better error trapping.\n\nPlease see the NEWS file in the installed package for more details."
  },
  {
    "objectID": "blog/nimble-online-tutorial-november-18-2021.html",
    "href": "blog/nimble-online-tutorial-november-18-2021.html",
    "title": "NIMBLE online tutorial, November 18, 2021",
    "section": "",
    "text": "We’ll be giving a two-hour tutorial on NIMBLE, sponsored by the environmental Bayes (enviBayes) section of ISBA (The International Society for Bayesian Analysis), on Thursday November 18, from 11 am to 1 pm US Eastern time.\nNIMBLE (r-nimble.org) is a system for fitting and programming with hierarchical models in R that builds on (a new implementation of) the BUGS language for declaring models. NIMBLE provides analysts with a flexible system for using MCMC, sequential Monte Carlo, MCEM, and other techniques on user-specified models. It provides developers and methodologists with the ability to write algorithms in an R-like syntax that can be easily disseminated to users. C++ versions of models and algorithms are created for speed, but these are manipulated from R without any need for analysts or algorithm developers to program in C++. While analysts can use NIMBLE as a nearly drop-in replacement for WinBUGS or JAGS, NIMBLE provides enhanced functionality in a number of ways.\nThis workshop will demonstrate how one can use NIMBLE to:\n\nflexibly specify an MCMC for a specific model, including choosing samplers and blocking approaches (and noting the potential usefulness of this for teaching);\ntailor an MCMC to a specific model using user-defined distributions, user-defined functions, and vectorization;\nwrite your own MCMC sampling algorithms and use them in combination with samplers from NIMBLE’s library of samplers;\ndevelop and disseminate your own algorithms, building upon NIMBLE’s existing algorithms; and\nuse specialized model components such as Dirichlet processes, conditional auto-regressive (CAR) models, and reversible jump for variable selection.\n\nThe tutorial will assume working knowledge of hierarchical models and some familiarity with MCMC. Given the two-hour time frame, we’ll focus on demonstrating some of the key features of NIMBLE, without going into a lot of detail on any given topic.\nTo attend, please register here."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Welcome to the NIMBLE blog! Here you’ll find announcements about new releases, tutorials, and insights into using NIMBLE for your statistical computing needs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nannouncing the nimbleMacros package and the use of macros in NIMBLE models\n\n\n\nannouncement\n\nrelease\n\n\n\n\n\n\n\n\n\nMar 19, 2025\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 1.3.0 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nDec 21, 2024\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 1.2.1 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 1.2.0 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nJun 14, 2024\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 1.1.0 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nnimbleHMC version 0.2.0 released, providing improved HMC performance\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 1.0.1 of NIMBLE released, fixing a bug in version 1.0.0 affecting certain models\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 1.0.0 of NIMBLE released, providing automatic differentiation, Laplace approximation, and HMC sampling\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.13.1 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nBug in newly-released version 0.13.0 affecting MCMC for models with predictive nodes\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.13.0 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nWe’re looking for a programmer\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE virtual short course, January 4-6, 2023\n\n\n\neducation\n\nannouncement\n\n\n\n\n\n\n\n\n\nSep 8, 2022\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nBeta version of NIMBLE with automatic differentiation, including HMC sampling and Laplace approximation\n\n\n\nannouncement\n\nrelease\n\n\n\n\n\n\n\n\n\nJul 15, 2022\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.12.2 of NIMBLE released, including an important bug fix for some models using Bayesian nonparametrics with the dCRP distribution\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nMar 4, 2022\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE in-person short course, June 1-3, Lisbon, Portugal\n\n\n\neducation\n\nannouncement\n\n\n\n\n\n\n\n\n\nMar 2, 2022\n\n\nChris Paciorek\n\n\n\n\n\n\n\n\n\n\n\n\nA close look at some linear model MCMC comparisons\n\n\n\n\n\n\n\n\nNov 11, 2021\n\n\nPerry de Valpine\n\n\n\n\n\n\n\n\n\n\n\n\nA close look at some posted trials of nimble for accelerated failure time models\n\n\n\n\n\n\n\n\nOct 29, 2021\n\n\nPerry de Valpine\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE online tutorial, November 18, 2021\n\n\n\neducation\n\nannouncement\n\n\n\n\n\n\n\n\n\nOct 20, 2021\n\n\nChris Paciorek\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.12.1 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nOct 13, 2021\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior predictive sampling and other post-MCMC use of samples in NIMBLE\n\n\n\nannouncement\n\ntutorial\n\n\n\n\n\n\n\n\n\nAug 10, 2021\n\n\nChris Paciorek and Sally Paganin\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.11.1 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nJun 2, 2021\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.11.0 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nApr 24, 2021\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Nonparametric Models in NIMBLE: General Multivariate Models\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nMar 25, 2021\n\n\nClaudia Wehrhahn\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE virtual short course, May 26-28\n\n\n\neducation\n\nannouncement\n\n\n\n\n\n\n\n\n\nMar 5, 2021\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE is hiring a programmer\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nFeb 2, 2021\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.10.1 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nNov 30, 2020\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE’s sequential Monte Carlo (SMC) algorithms are now in the nimbleSMC package\n\n\n\nannouncement\n\nrelease\n\n\n\n\n\n\n\n\n\nNov 29, 2020\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.10.0 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nOct 14, 2020\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nNew NIMBLE cheatsheat available\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nJun 14, 2020\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.9.1 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nMay 27, 2020\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nnimbleEcology: custom NIMBLE distributions for ecologists\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nApr 18, 2020\n\n\nBen Goldstein\n\n\n\n\n\n\n\n\n\n\n\n\nregistration open for online NIMBLE short course, June 3-5, 2020\n\n\n\neducation\n\nannouncement\n\n\n\n\n\n\n\n\n\nApr 6, 2020\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nregistration open for NIMBLE short course, June 3-4, 2020 at UC Berkeley\n\n\n\neducation\n\nannouncement\n\n\n\n\n\n\n\n\n\nMar 2, 2020\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE short course at March ENAR meeting in Nashville\n\n\n\nannouncement\n\neducation\n\n\n\n\n\n\n\n\n\nJan 4, 2020\n\n\nChris Paciorek\n\n\n\n\n\n\n\n\n\n\n\n\nVariable selection in NIMBLE using reversible jump MCMC\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nJan 2, 2020\n\n\nSally Paganin\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.9.0 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nDec 21, 2019\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nNew NIMBLE code examples\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nDec 19, 2019\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE short course, June 3-4, 2020 at UC Berkeley\n\n\n\nannouncement\n\neducation\n\n\n\n\n\n\n\n\n\nDec 11, 2019\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE short course at Bayes Comp 2020 conference\n\n\n\nannouncement\n\neducation\n\n\n\n\n\n\n\n\n\nSep 13, 2019\n\n\nChris Paciorek\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.8.0 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nJun 7, 2019\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.7.1 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nMar 15, 2019\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.7.0 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nFeb 5, 2019\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Nonparametric Models in NIMBLE, Part 2: Nonparametric Random Effects\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nDec 6, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Nonparametric Models in NIMBLE, Part 1: Density Estimation\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nDec 4, 2018\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nSpread the word: NIMBLE is looking for a post-doc\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nSep 13, 2018\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.6-12 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nJul 29, 2018\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.6-11 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nJun 16, 2018\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nQuick guide for converting from JAGS or BUGS to NIMBLE\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 30, 2018\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nTwo day workshop: Flexible programming of MCMC and other methods for hierarchical and Bayesian models\n\n\n\nannouncement\n\neducation\n\n\n\n\n\n\n\n\n\nApr 19, 2018\n\n\nChris Paciorek\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.6-10 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nMar 29, 2018\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE webinar Friday April 13\n\n\n\nannouncement\n\neducation\n\n\n\n\n\n\n\n\n\nMar 20, 2018\n\n\nChris Paciorek\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.6-9 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nFeb 9, 2018\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE workshop in Switzerland, 23-25 April\n\n\n\nannouncement\n\neducation\n\n\n\n\n\n\n\n\n\nJan 24, 2018\n\n\nPerry de Valpine\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE has a post-doc or software developer position open\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nJan 3, 2018\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.6-8 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nDec 4, 2017\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.6-6 of NIMBLE released!\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nJul 29, 2017\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.6-5 of NIMBLE released!\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nJun 7, 2017\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nBetter block sampling in MCMC with the Automated Factor Slice Sampler\n\n\n\nannouncement\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 9, 2017\n\n\nNicholas Michaud\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.6-4 of NIMBLE released!\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nApr 26, 2017\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nWriting reversible jump MCMC in NIMBLE\n\n\n\nNIMBLE\n\nR\n\nStatistics\n\n\n\n\n\n\n\n\n\nFeb 15, 2017\n\n\nnimble-admin\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE is hiring a programmer\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nJan 11, 2017\n\n\nChris Paciorek\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Particle Filters and Particle MCMC in NIMBLE\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nJan 9, 2017\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.6-3 released.\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nDec 15, 2016\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.6-2 released!\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nNov 23, 2016\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE package for hierarchical modeling (MCMC and more) faster and more flexible in version 0.6-1\n\n\n\nannouncement\n\nrelease\n\n\n\n\n\n\n\n\n\nOct 31, 2016\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.5-1 of NIMBLE released!\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nMay 27, 2016\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE: A new way to do MCMC (and more) from BUGS code in R\n\n\n\nannouncement\n\nrelease\n\n\n\n\n\n\n\n\n\nApr 8, 2016\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.5 released!\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nApr 7, 2016\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE paper in Journal of Computational and Graphical Statistics\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nApr 7, 2016\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.4-1 released!\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nOct 4, 2015\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nWe have a post-doc opening\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nSep 24, 2015\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.4 released!\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nAug 31, 2015\n\n\nNIMBLE Development Team\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Blog Posts",
      "All Posts"
    ]
  },
  {
    "objectID": "blog/version-0-6-10-of-nimble-released.html",
    "href": "blog/version-0-6-10-of-nimble-released.html",
    "title": "Version 0.6-10 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.6-10 primarily contains updates to the NIMBLE internals that may speed up building and compilation of models and algorithms, as well as a few bug fixes.\nChanges include:\n\nsome steps of model and algorithm building and compilation are faster;\ncompiled execution with multivariate distributions or function arguments may be faster;\ndata can now be provided as a numeric data frame rather than a matrix;\nto run WAIC, a user now must set enableWAIC to TRUE, either in NIMBLE’s options or as an argument to buildMCMC();\nif enableWAIC is TRUE, buildMCMC() will now check to make sure that the nodes monitored by the MCMC algorithm will lead to a valid WAIC calculation; and\nthe use of identityMatrix() is deprecated in favor of diag().\n\nPlease see the NEWS file in the installed package for more details"
  },
  {
    "objectID": "blog/were-looking-for-a-programmer.html",
    "href": "blog/were-looking-for-a-programmer.html",
    "title": "We’re looking for a programmer",
    "section": "",
    "text": "The NIMBLE project anticipates having some funding for a part-time programmer to implement statistical algorithms and make improvements in nimble’s core code. Examples may include building adaptive Gaussian quadrature in nimble’s programming system and expanding nimble’s hierarchical model system. Remote work is possible. This is not a formal job solicitation, but please send a CV/resume to nimble.stats@gmail.com if you are interested so we can have you on our list. Important skills will be experience with hierarchical statistical modeling algorithms, R programming, and nimble itself. Experience with C++ will be helpful but not required."
  },
  {
    "objectID": "blog/nimbleecology-custom-nimble-distributions-for-ecologists.html",
    "href": "blog/nimbleecology-custom-nimble-distributions-for-ecologists.html",
    "title": "nimbleEcology: custom NIMBLE distributions for ecologists",
    "section": "",
    "text": "nimbleEcology is an auxiliary nimble package for ecologists.\nnimbleEcology contains a set of distributions corresponding to some common ecological models. When the package is loaded, these distributions are registered to NIMBLE and can be used directly in models.\nnimbleEcology contains distributions often used in modeling abundance, occupancy and capture-recapture studies."
  },
  {
    "objectID": "blog/nimbleecology-custom-nimble-distributions-for-ecologists.html#what-is-nimbleecology",
    "href": "blog/nimbleecology-custom-nimble-distributions-for-ecologists.html#what-is-nimbleecology",
    "title": "nimbleEcology: custom NIMBLE distributions for ecologists",
    "section": "",
    "text": "nimbleEcology is an auxiliary nimble package for ecologists.\nnimbleEcology contains a set of distributions corresponding to some common ecological models. When the package is loaded, these distributions are registered to NIMBLE and can be used directly in models.\nnimbleEcology contains distributions often used in modeling abundance, occupancy and capture-recapture studies."
  },
  {
    "objectID": "blog/nimbleecology-custom-nimble-distributions-for-ecologists.html#why-use-nimbleecology",
    "href": "blog/nimbleecology-custom-nimble-distributions-for-ecologists.html#why-use-nimbleecology",
    "title": "nimbleEcology: custom NIMBLE distributions for ecologists",
    "section": "Why use nimbleEcology?",
    "text": "Why use nimbleEcology?\nEcological models for abundance, occupancy and capture-recapture often involve many discrete latent states. Writing such models can be error-prone and in some cases can lead to slow MCMC mixing. We’ve put together a collection of distributions in nimble to make writing these models easier\n\nEasy to use. Using a nimbleEcology distribution is easier than writing out probabilities or hierarchical model descriptions.\nMinimize errors. You don’t have to lose hours looking for the misplaced minus sign; the distributions are checked and tested.\nIntegrate over latent states. nimbleEcology implementations integrate or sum likelihoods over latent states. This eliminates the need for sampling these latent variables, which in some cases can provide efficiency gains, and allows maximum likelihood (ML) estimation methods with hierarchical models."
  },
  {
    "objectID": "blog/nimbleecology-custom-nimble-distributions-for-ecologists.html#how-to-use",
    "href": "blog/nimbleecology-custom-nimble-distributions-for-ecologists.html#how-to-use",
    "title": "nimbleEcology: custom NIMBLE distributions for ecologists",
    "section": "How to use",
    "text": "How to use\nnimbleEcology can be installed directly from CRAN as follows.\n\ninstall.packages(\"nimbleEcology\")\n\nOnce nimbleEcology is installed, load it using library. It will also load nimble.\n\nlibrary(nimbleEcology)\n\nLoading required package: nimble\n\n\nnimble version 1.3.0 is loaded.\nFor more information on NIMBLE and a User Manual,\nplease visit https://R-nimble.org.\n\n\n\nAttaching package: 'nimble'\n\n\nThe following object is masked from 'package:stats':\n\n    simulate\n\n\nThe following object is masked from 'package:base':\n\n    declare\n\n\nLoading nimbleEcology. Registering multiple variants of the following distributions:\n dOcc, dDynOcc, dCJS, dHMM, dDHMM, dNmixture.\n\n\nNote the message indicating which distribution families have been loaded."
  },
  {
    "objectID": "blog/nimbleecology-custom-nimble-distributions-for-ecologists.html#which-distributions-are-available",
    "href": "blog/nimbleecology-custom-nimble-distributions-for-ecologists.html#which-distributions-are-available",
    "title": "nimbleEcology: custom NIMBLE distributions for ecologists",
    "section": "Which distributions are available?",
    "text": "Which distributions are available?\nThe following distributions are available in nimbleEcology.\n\ndOcc (occupancy model)\ndDynOcc (dynamic occupancy model)\ndHMM (hidden Markov model)\n`dDHMM (dynamic hidden Markov model)\ndCJS (Cormack-Jolly-Seber or mark-recapture model)\n`dNmixture (N-mixture model)\ndYourNewDistribution Do you have a custom distribution that would fit the package? Are we missing a distribution you need? Let us know! We actively encourage contributions through GitHub or direct communication."
  },
  {
    "objectID": "blog/nimbleecology-custom-nimble-distributions-for-ecologists.html#example-code",
    "href": "blog/nimbleecology-custom-nimble-distributions-for-ecologists.html#example-code",
    "title": "nimbleEcology: custom NIMBLE distributions for ecologists",
    "section": "Example code",
    "text": "Example code\nThe following code illustrates a NIMBLE model definition for an occupancy model using nimbleEcology. The model is specified, built, and used to simulate some data according to the occupancy distribution.\n\nocc_code &lt;- nimbleCode({\n  psi ~ dunif(0, 1)\n  p ~ dunif(0, 1)\n  for (s in 1:nsite) {\n    x[s, 1:nvisit] ~ dOcc_s(probOcc = psi, probDetect = p,\n                            len = nvisit)\n  }\n})\n\nocc_model &lt;- nimbleModel(occ_code,\n               constants = list(nsite = 10, nvisit = 5),\n               inits = list(psi = 0.5, p = 0.5))\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\n  [Note] This model is not fully initialized. This is not an error.\n         To see which variables are not initialized, use model$initializeInfo().\n         For more information on model initialization, see help(modelInitialization).\n\nset.seed(94)\nocc_model$simulate(\"x\")\nocc_model$x\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]    0    0    0    0    0\n [2,]    0    0    0    0    0\n [3,]    0    0    0    0    0\n [4,]    0    0    0    0    0\n [5,]    1    1    1    0    1\n [6,]    0    0    0    1    0\n [7,]    0    0    0    0    0\n [8,]    0    0    0    0    1\n [9,]    1    1    1    0    0\n[10,]    0    1    0    0    0"
  },
  {
    "objectID": "blog/nimbleecology-custom-nimble-distributions-for-ecologists.html#how-to-learn-more",
    "href": "blog/nimbleecology-custom-nimble-distributions-for-ecologists.html#how-to-learn-more",
    "title": "nimbleEcology: custom NIMBLE distributions for ecologists",
    "section": "How to learn more",
    "text": "How to learn more\nOnce the package is installed, you can check out the package vignette with vignette(\"nimbleEcology\").\nDocumentation is available for each distribution family using the R syntax ?distribution, for example\n\n?dHMM\n\nFor more detail on marginalization in these distributions, see the paper “One size does not fit all: Customizing MCMC methods for hierarchical models using NIMBLE” (Ponisio et al. 2020)."
  },
  {
    "objectID": "blog/nimble-package-for-hierarchical-modeling-mcmc-and-more-faster-and-more-flexible-in-version-0-6-1.html",
    "href": "blog/nimble-package-for-hierarchical-modeling-mcmc-and-more-faster-and-more-flexible-in-version-0-6-1.html",
    "title": "NIMBLE package for hierarchical modeling (MCMC and more) faster and more flexible in version 0.6-1",
    "section": "",
    "text": "NIMBLE version 0.6-1 has been released on CRAN and at r-nimble.org.\nNIMBLE is a system that allows you to:\n\nWrite general hierarchical statistical models in BUGS code and create a corresponding model object to use in R.\nBuild Markov chain Monte Carlo (MCMC), particle filters, Monte Carlo Expectation Maximization (MCEM), or write generic algorithms that can be applied to any model.\nCompile models and algorithms via problem-specific generated C++ that NIMBLE interfaces to R for you.\n\nMost people associate BUGS with MCMC, but NIMBLE is about much more than that. It implements and extends the BUGS language as a flexible system for model declaration and lets you do what you want with the resulting models. Some of the cool things you can do with NIMBLE include:\n\nExtend BUGS with functions and distributions you write in R as nimbleFunctions, which will be automatically turned into C++ and compiled into your model.\nProgram with models written in BUGS code: get and set values of variables, control model calculations, simulate new values, use different data sets in the same model, and more.\nWrite your own MCMC samplers as nimbleFunctions and use them in combination with NIMBLE’s samplers.\nWrite functions that use MCMC as one step of a larger algorithm.\nUse standard particle filter methods or write your own.\nCombine particle filters with MCMC as Particle MCMC methods.\nWrite other kinds of model-generic algorithms as nimbleFunctions.\nCompile a subset of R’s math syntax to C++ automatically, without writing any C++ yourself.\n\nSome early versions of NIMBLE were not on CRAN because NIMBLE’s system for on-the-fly compilation via generating and compiling C++ from R required some extra work for CRAN packaging, but now it’s there. Compared to earlier versions, the new version is faster and more flexible in a lot of ways. Building and compiling models and algorithms could sometimes get bogged down for large models, so we streamlined those steps quite a lot. We’ve generally increased the efficiency of C++ generated by the NIMBLE compiler. We’ve added functionality to what can be compiled to C++ from nimbleFunctions. And we’ve added a bunch of better error-trapping and informative messages, although there is still a good way to go on that. Give us a holler on the nimble-users list if you run into questions."
  },
  {
    "objectID": "blog/registration-open-for-online-nimble-short-course-june-3-5-2020.html",
    "href": "blog/registration-open-for-online-nimble-short-course-june-3-5-2020.html",
    "title": "registration open for online NIMBLE short course, June 3-5, 2020",
    "section": "",
    "text": "Registration is now open for a three-day online training workshop on NIMBLE, June 3-5, 2020, 9 am – 2 pm California time (noon – 5 pm US EDT). This online workshop will be held in place of our previously planned in-person workshop.\nNIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nThe workshop will cover:\n\nthe basic concepts and workflows for using NIMBLE and converting BUGS or JAGS models to work in NIMBLE.\noverview of different MCMC sampling strategies and how to use them in NIMBLE.\nwriting new distributions and functions for more flexible modeling and more efficient computation.\ntips and tricks for improving computational efficiency.\nusing advanced model components, including Bayesian non-parametric distributions (based on Dirichlet process priors), conditional auto-regressive (CAR) models for spatially correlated random fields, and reversible jump samplers for variable selection.\nan introduction to programming new algorithms in NIMBLE.\ncalling R and compiled C++ code from compiled NIMBLE models or functions.\n\nIf participant interests vary sufficiently, part of the third day will be split into two tracks. One of these will likely focus on ecological models. The other will be chosen based on attendee interest from topics such as (a) advanced NIMBLE programming including writing new MCMC samplers, (b) advanced spatial or Bayesian non-parametric modeling, or (c) non-MCMC algorithms in NIMBLE such as sequential Monte Carlo. Prior to the workshop, we will survey attendee interests and adjust content to meet attendee interests.\nTo register, please go here: https://na.eventscloud.com/540175. Registration is $120 (regular) or $60 (grad student)."
  },
  {
    "objectID": "blog/version-1-2-0-of-nimble-released.html",
    "href": "blog/version-1-2-0-of-nimble-released.html",
    "title": "Version 1.2.0 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC, Laplace approximation, and SMC).\nThis release provides provides extensive new functionality, including:\n\nA Pólya-gamma sampler, sampler_polyagamma, for conjugate sampling of linear predictor parameters in logistic regression model specifications, including handling zero inflation and stochastic design matrices. This sampler must be added to an MCMC configuration manually.\nA new sampler, sampler_noncentered, which samples the mean or standard deviation of a set of random effect values in a transformed space such that the random effects are deterministically shifted or scaled given new values of their hyperparameters. For random effects written in a centered parameterization, sampling is performed as if they had been written in a noncentered parameterization, thereby enabling a variant on the Yu and Meng (2011) interweaving sampling strategy of sampling in both parameterizations.This sampler must be added to an MCMC configuration manually.\nAdaptive Gauss-Hermite quadrature (AGHQ) for integrating over continuous latent effects, as an extension of NIMBLE’s Laplace approximation functionality. We also add user-friendly R functions, runLaplace and runAGHQ, for using Laplace and AGHQ approximation for maximum likelihood estimation.\nA more flexible optimization system via nimOptim, with support for nlminb built in as well as the capability for users to provide potentially arbitrary optimization functions in R.\nAllowing the use of nimbleFunctions with setup code in models, either for user-defined functions via &lt;- or for user-defined distributions via ~. This supports holding large objects outside of model nodes for use in models.\nA completely revamped MCEM algorithm, using automatic derivatives in the maximization when possible, fixing a bug so that any parts of the model not connected to the latent states are included in MLE calculations, giving greater control and adding minor extensions to the ascent-based MCEM approach, and converting buildMCEM to be a nimbleFunction rather than an R function.\n\nIn addition to the new functionality above, other enhancements and bug fixes include:\n\nImproving the speed of MCMC and MCMC building in certain cases.\nAdding an argument to buildMCMC controlling whether to initialize values in the model.\nProviding the ability to control the number of digits printed in C++ output.\nAllowing use of a categorical MCMC sampler with user-specified dcat-like distributions.\nWarning of use of backward indexing in models.\nImprove documentation of the LKJ distribution and of advanced aspects of writing code for derivative tracking using the AD system.\nFixing an insufficient check for conjugacy in stick-breaking specifications of Bayesian nonparametric distributions.\nFixing compilation failures occurring on Red Hat Linux.\nReenabling functionality for user-provided Eigen library and related updates to the autoconf configuration used in package building.\nEnhancing functionality to support model macros, which will be fully released and documented in the future.\nRemoving deprecated is.na.vec and is.nan.vec functions.\nImproving some warnings and error messages.\n\nPlease see the release notes on our website for more details.",
    "crumbs": [
      "Home",
      "Blog Posts",
      "Version 1.2.0 of NIMBLE released"
    ]
  },
  {
    "objectID": "blog/version-0-11-0-of-nimble-released.html",
    "href": "blog/version-0-11-0-of-nimble-released.html",
    "title": "Version 0.11.0 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nVersion 0.11.0 provides a variety of new functionality, improved error trapping, and bug fixes, including:\n\nadded the posterior_predictive_branch MCMC sampler, which samples jointly from the predictive distribution of networks of entirely non-data nodes, to improve MCMC mixing,\nadded a model method to find parent nodes, called getParents(), analogous to getDependencies(),\nimproved efficiency of conjugate samplers,\nallowed use of the elliptical slice sampler for univariate nodes, which can be useful for posteriors with multiple modes,\nallowed model definition using if-then-else without an else clause, and\nfixed a bug giving incorrect node names and potentially affecting algorithm behavior for models with more than 100,000 elements in a vector node or any dimension of a multi-dimensional node.\n\nPlease see the release notes on our website for more details."
  },
  {
    "objectID": "blog/version-0-7-0-of-nimble-released.html",
    "href": "blog/version-0-7-0-of-nimble-released.html",
    "title": "Version 0.7.0 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC). Version 0.7.0 provides a variety of new features, as well as various bug fixes.\nNew features include:\n\ngreatly improved efficiency of sampling for Bayesian nonparametric (BNP) mixture models that use the dCRP (Chinese Restaurant process) distribution;\naddition of the double exponential (Laplace) distribution for use in models and nimbleFunctions;\na new RW_wishart MCMC sampler, for sampling non-conjugate Wishart and inverse-Wishart nodes;\nhandling of the normal-inverse gamma conjugacy for BNP mixture models using the dCRP distribution;\nenhanced functionality of the getSamplesDPmeasure function for posterior sampling from BNP random measures with Dirichlet process priors.\nhandling of five-dimensional arrays in models;\nenhanced warning messages; and\nan HTML version of the NIMBLE manual.\n\nPlease see the NEWS file in the installed package for more details."
  },
  {
    "objectID": "blog/registration-open-for-nimble-short-course-june-3-4-2020-at-uc-berkeley.html",
    "href": "blog/registration-open-for-nimble-short-course-june-3-4-2020-at-uc-berkeley.html",
    "title": "registration open for NIMBLE short course, June 3-4, 2020 at UC Berkeley",
    "section": "",
    "text": "Registration is now open for a two-day training workshop on NIMBLE, June 3-4, 2020 in Berkeley, California. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nThe workshop will cover:\n\nthe basic concepts and workflows for using NIMBLE and converting BUGS or JAGS models to work in NIMBLE.\noverview of different MCMC sampling strategies and how to use them in NIMBLE.\nwriting new distributions and functions for more flexible modeling and more efficient computation.\ntips and tricks for improving computational efficiency.\nusing advanced model components, including Bayesian non-parametric distributions (based on Dirichlet process priors), conditional auto-regressive (CAR) models for spatially correlated random fields, and reversible jump samplers for variable selection.\nan introduction to programming new algorithms in NIMBLE.\ncalling R and compiled C++ code from compiled NIMBLE models or functions.\n\nIf participant interests vary sufficiently, the second half-day will be split into two tracks. One of these will likely focus on ecological models. The other will be chosen based on attendee interest from topics such as (a) advanced NIMBLE programming including writing new MCMC samplers, (b) advanced spatial or Bayesian non-parametric modeling, or (c) non-MCMC algorithms in NIMBLE such as sequential Monte Carlo. Prior to the workshop, we will survey attendee interests and adjust content to meet attendee interests.\nTo register, please go here: https://na.eventscloud.com/528790. Registration is $230 (regular) or $115 (student).\nWe have also reserved a block of rooms on campus for $137 per night, including breakfast and lunch. To reserve a room, go here: https://na.eventscloud.com/528792."
  },
  {
    "objectID": "blog/better-block-sampling-in-mcmc-with-the-automated-factor-slice-sampler.html",
    "href": "blog/better-block-sampling-in-mcmc-with-the-automated-factor-slice-sampler.html",
    "title": "Better block sampling in MCMC with the Automated Factor Slice Sampler",
    "section": "",
    "text": "One nice feature of NIMBLE’s MCMC system is that a user can easily write new samplers from R, combine them with NIMBLE’s samplers, and have them automatically compiled to C++ via the NIMBLE compiler. We’ve observed that block sampling using a simple adaptive multivariate random walk Metropolis-Hastings sampler doesn’t always work well in practice, so we decided to implement the Automated Factor Slice sampler (AFSS) of Tibbits, Groendyke, Haran, and Liechty (2014) and see how it does on a (somewhat artificial) example with severe posterior correlation problems.\nRoughly speaking, the AFSS works by conducting univariate slice sampling in directions determined by the eigenvectors of the marginal posterior covariance matrix for blocks of parameters in a model. So far, we’ve found the AFSS often outperforms random walk block sampling. To compare performance, we look at MCMC efficiency, which we define for each parameter as effective sample size (ESS) divided by computation time. We define overall MCMC efficiency as the minimum MCMC efficiency of all the parameters, because one needs all parameters to be well mixed.\nWe’ll demonstrate the performance of the AFSS on the correlated state space model described in Turek, de   Valpine, Paciorek, Anderson-Bergman, and others (2017).\n\nModel Creation\nAssume \\(x_{i}\\) is the latent state and \\(y_{i}\\) is the observation at time \\(i\\) for \\(i=1,\\ldots,100\\). We define the state space model as\n\\[ x_{i} \\sim N(a \\cdot x_{i-1} + b, \\sigma_{PN}) \\] \\[ y_{i} \\sim N(x_{i}, \\sigma_{OE}) \\]\nfor \\(i = 2, \\ldots, 100\\), with initial states\n\\[ x_{1} \\sim N\\left(\\frac{b}{1-a}, \\frac{\\sigma_{PN}}{\\sqrt{1-a^2}}\\right) \\] \\[ y_{1} \\sim N(x_{1}, \\sigma_{OE}) \\]\nand prior distributions\n\\[ a \\sim \\text{Unif}(-0.999, 0.999) \\] \\[ b \\sim N(0, 1000) \\] \\[ \\sigma_{PN} \\sim \\text{Unif}(0, 1) \\] \\[ \\sigma_{OE} \\sim \\text{Unif}(0, 1) \\]\nwhere \\(N(\\mu, \\sigma)\\) denotes a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nA file named model_SSMcorrelated.RData with the BUGS model code, data, constants, and initial values for our model can be downloaded here.\n\nlibrary('nimble')\n\n\nset.seed(1)\nload('model_SSMcorrelated.RData')\n## build and compile the model\nstateSpaceModel &lt;- nimbleModel(code = code,\n                              data = data,\n                              constants = constants,\n                              inits = inits,\n                              check = FALSE)\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\n  [Note] This model is not fully initialized. This is not an error.\n         To see which variables are not initialized, use model$initializeInfo().\n         For more information on model initialization, see help(modelInitialization).\n\nC_stateSpaceModel &lt;- compileNimble(stateSpaceModel)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\n\n\nComparing two MCMC Samplers\nWe next compare the performance of two MCMC samplers on the state space model described above. The first sampler we consider is NIMBLE’s RW_block sampler, a Metropolis-Hastings sampler with a multivariate normal proposal distribution. This sampler has an adaptive routine that modifies the proposal covariance to look like the empirical covariance of the posterior samples of the parameters. However, as we shall see below, this proposal covariance adaptation does not lead to efficient sampling for our state space model.\nWe first build and compile the MCMC algorithm.\n\nRW_mcmcConfig &lt;- configureMCMC(stateSpaceModel)\n\n===== Monitors =====\nthin = 1: a, b, sigOE, sigPN\n===== Samplers =====\nRW sampler (3)\n  - a\n  - sigPN\n  - sigOE\nconjugate sampler (101)\n  - b\n  - x[]  (100 elements)\n\nRW_mcmcConfig$removeSamplers(c('a', 'b', 'sigOE', 'sigPN'))\nRW_mcmcConfig$addSampler(target = c('a', 'b', 'sigOE', 'sigPN'), type = 'RW_block')\n\n  [Note] Assigning an RW_block sampler to nodes with very different scales can result in low MCMC efficiency.  If all nodes assigned to RW_block are not on a similar scale, we recommend providing an informed value for the \"propCov\" control list argument, or using the \"barker\" sampler instead.\n\nRW_mcmc &lt;- buildMCMC(RW_mcmcConfig)\nC_RW_mcmc &lt;- compileNimble(RW_mcmc, project = stateSpaceModel)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\nWe next run the compiled MCMC algorithm for 10,000 iterations, recording the overall MCMC efficiency from the posterior output. The overall efficiency here is defined as \\(\\min(\\frac{ESS}{T})\\), where ESS denotes the effective sample size, and \\(T\\) the total run-time of the sampling algorithm. The minimum is taken over all parameters that were sampled. We repeat this process 5 times to get a very rough idea of the average minimum efficiency for this combination of model and sampler.\n\nlibrary(coda)\n\n\nAttaching package: 'coda'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    densplot\n\nRW_minEfficiency &lt;- numeric(5)\nfor(i in 1:5){\n  runTime &lt;- system.time(C_RW_mcmc$run(50000, progressBar = FALSE))['elapsed']\n  RW_mcmcOutput &lt;- as.mcmc(as.matrix(C_RW_mcmc$mvSamples))\n  RW_minEfficiency[i] &lt;- min(effectiveSize(RW_mcmcOutput)/runTime)\n}\nsummary(RW_minEfficiency)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.2490  0.3129  0.4415  0.5301  0.5466  1.1003 \n\n\nExamining a trace plot of the output below, we see that the \\(a\\) and \\(b\\) parameters are mixing especially poorly.\n\nplot(RW_mcmcOutput, density = FALSE)\n\n\n\n\n\n\n\n\nPlotting the posterior samples of \\(a\\) against those of \\(b\\) reveals a strong negative correlation. This presents a problem for the Metropolis-Hastings sampler — we have found that adaptive algorithms used to tune the proposal covariance are often slow to reach a covariance that performs well for blocks of strongly correlated parameters.\n\nplot.default(RW_mcmcOutput[,'a'], RW_mcmcOutput[,'b'])\n\n\n\n\n\n\n\n\n\ncor(RW_mcmcOutput[,'a'], RW_mcmcOutput[,'b'])\n\n[1] -0.9964729\n\n\nIn such situations with strong posterior correlation, we’ve found the AFSS to often run much more efficiently, so we next build and compile an MCMC algorithm using the AFSS sampler. Our hope is that the AFSS sampler will be better able to to produce efficient samples in the face of high posterior correlation.\n\nAFSS_mcmcConfig &lt;- configureMCMC(stateSpaceModel)\n\n===== Monitors =====\nthin = 1: a, b, sigOE, sigPN\n===== Samplers =====\nRW sampler (3)\n  - a\n  - sigPN\n  - sigOE\nconjugate sampler (101)\n  - b\n  - x[]  (100 elements)\n\nAFSS_mcmcConfig$removeSamplers(c('a', 'b', 'sigOE', 'sigPN'))\nAFSS_mcmcConfig$addSampler(target = c('a', 'b', 'sigOE', 'sigPN'), type = 'AF_slice')\nAFSS_mcmc&lt;- buildMCMC(AFSS_mcmcConfig)\nC_AFSS_mcmc &lt;- compileNimble(AFSS_mcmc, project = stateSpaceModel, resetFunctions = TRUE)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\nWe again run the AFSS MCMC algorithm 5 times, each with 10,000 MCMC iterations.\n\nAFSS_minEfficiency &lt;- numeric(5)\nfor(i in 1:5){\n  runTime &lt;- system.time(C_AFSS_mcmc$run(50000, progressBar = FALSE))['elapsed']\n  AFSS_mcmcOutput &lt;- as.mcmc(as.matrix(C_AFSS_mcmc$mvSamples))\n  AFSS_minEfficiency[i] &lt;- min(effectiveSize(AFSS_mcmcOutput)/runTime)\n}\nsummary(AFSS_minEfficiency)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.50   23.42   29.11   27.83   29.15   35.95 \n\n\nNote that the minimum overall efficiency of the AFSS sampler is approximately 28 times that of the RW_block sampler. Additionally, trace plots from the output of the AFSS sampler show that the \\(a\\) and \\(b\\) parameters are mixing much more effectively than they were under the RW_block sampler.\n\nplot(AFSS_mcmcOutput, density = FALSE)\n\n\n\n\n\n\n\n\nTibbits, M. M, C. Groendyke, M. Haran, et al. (2014). “Automated factor slice sampling”.\nIn: Journal of Computational and Graphical Statistics 23.2, pp. 543–563.\nTurek, D, P. de Valpine, C. J. Paciorek, et al. (2017).\n“Automated parameter blocking for efficient Markov chain Monte Carlo sampling”.\nIn: Bayesian Analysis 12.2, pp. 465–490."
  },
  {
    "objectID": "blog/nimble-short-course-at-bayes-comp-2020-conference.html",
    "href": "blog/nimble-short-course-at-bayes-comp-2020-conference.html",
    "title": "NIMBLE short course at Bayes Comp 2020 conference",
    "section": "",
    "text": "We’ll be giving a short course on NIMBLE on January 7, 2020 at the Bayes Comp 2020 conference being held January 7-10 in Gainesville, Florida, USA.\nBayes Comp is a popular biennial ISBA-sponsored conference focused on computational methods/algorithms/technologies for Bayesian inference.\nThe short course focuses on programming algorithms in NIMBLE and is titled:\n“Developing, modifying, and sharing Bayesian algorithms (MCMC samplers, SMC, and more) using the NIMBLE platform in R.”\nMore details on the conference and the short course are available at the conference website."
  },
  {
    "objectID": "blog/nimble-has-a-post-doc-or-software-developer-position-open.html",
    "href": "blog/nimble-has-a-post-doc-or-software-developer-position-open.html",
    "title": "NIMBLE has a post-doc or software developer position open",
    "section": "",
    "text": "The NIMBLE statistical software project at the University of California, Berkeley is looking for a post-doc or statistical software developer. NIMBLE is a tool for writing hierarchical statistical models and algorithms from R, with compilation via code-generated C++. Major methods currently include MCMC and sequential Monte Carlo, which users can customize and extend. More information can be found at https://R-nimble.org. Currently we seek someone with experience in computational statistical methods such as MCMC and excellent software development skills in R and C++. This could be someone with a Ph.D. in Statistics, Computer Science, or an applied statistical field in which they have done relevant work. Alternatively it could be someone with relevant experience in computational statistics and software engineering. The scope of work can include both core development of NIMBLE and development and application of innovative methods using NIMBLE, with specific focus depending on the background of the successful candidate. Applicants must have either a Ph.D. in a relevant field or have a proven record of relevant work. Please send cover letter, CV, and the names and contact information for three references to nimble.stats@gmail.com. Applications will be considered on a rolling basis starting 30 January, 2018."
  },
  {
    "objectID": "blog/version-0-8-0-of-nimble-released.html",
    "href": "blog/version-0-8-0-of-nimble-released.html",
    "title": "Version 0.8.0 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC). Version 0.8.0 provides some new features, speed improvements, and a variety of bug fixes and better error/warning messages.\nNew features include:\n\na reversible jump MCMC sampler for variable selection via configureRJ();\ngreatly improved speed of MCMC sampling for Bayesian nonparametric models with a dCRP distribution by not sampling parameters for empty clusters;\nexperimental faster MCMC configuration, available by setting nimbleOptions(oldConjugacyChecking = FALSE) and nimbleOptions(useNewConfigureMCMC = TRUE);\nand improved warning and error messages for MCEM and slice sampling.\n\nPlease see the release notes on our website for more details."
  },
  {
    "objectID": "blog/spread-the-word-nimble-is-looking-for-a-post-doc.html",
    "href": "blog/spread-the-word-nimble-is-looking-for-a-post-doc.html",
    "title": "Spread the word: NIMBLE is looking for a post-doc",
    "section": "",
    "text": "We have an opening for a post-doctoral scholar to work on methods for and applications of hierarchical statistical models using the NIMBLE software (https://r-nimble.org) at the University of California, Berkeley. NIMBLE is an R package that combines a new implementation of a model language similar to BUGS/JAGS, a system for writing new algorithms and MCMC samplers, and a compiler that generates C++ for each model and set of algorithms. The successful candidate will work with Chris Paciorek, Perry de Valpine, and potentially other NIMBLE collaborators to pursue a research program with a combination of building and applying methods in NIMBLE. Specific methods and application areas will be determined based on interests of the successful candidate. Applicants should have a Ph.D. in Statistics or a related discipline. We are open to non-Ph.D. candidates who can make a compelling case that they have relevant experience. The position is funded for two years, with an expected start date between October 2018 and June 2019. Applicants should send a cover letter, including a statement of how their interests relate to NIMBLE, the names of three references, and a CV to nimble.stats@gmail.com, with “NIMBLE post-doc application” in the subject. Applications will be considered on a rolling basis starting 15 October, 2018."
  },
  {
    "objectID": "blog/bayesian-nonparametric-models-in-nimble-part-1-density-estimation.html",
    "href": "blog/bayesian-nonparametric-models-in-nimble-part-1-density-estimation.html",
    "title": "Bayesian Nonparametric Models in NIMBLE, Part 1: Density Estimation",
    "section": "",
    "text": "Bayesian Nonparametric Models in NIMBLE, Part 1: Density Estimation"
  },
  {
    "objectID": "blog/bayesian-nonparametric-models-in-nimble-part-1-density-estimation.html#overview",
    "href": "blog/bayesian-nonparametric-models-in-nimble-part-1-density-estimation.html#overview",
    "title": "Bayesian Nonparametric Models in NIMBLE, Part 1: Density Estimation",
    "section": "Overview",
    "text": "Overview\nNIMBLE is a hierarchical modeling package that uses nearly the same language for model specification as the popular MCMC packages WinBUGS, OpenBUGS and JAGS, while making the modeling language extensible — you can add distributions and functions — and also allowing customization of the algorithms used to estimate the parameters of the model.\nRecently, we added support for Markov chain Monte Carlo (MCMC) inference for Bayesian nonparametric (BNP) mixture models to NIMBLE. In particular, starting with version 0.6-11, NIMBLE provides functionality for fitting models involving Dirichlet process priors using either the Chinese Restaurant Process (CRP) or a truncated stick-breaking (SB) representation of the Dirichlet process prior.\nIn this post we illustrate NIMBLE’s BNP capabilities by showing how to use nonparametric mixture models with different kernels for density estimation. In a later post, we will take a parametric generalized linear mixed model and show how to switch to a nonparametric representation of the random effects that avoids the assumption of normally-distributed random effects.\nFor more detailed information on NIMBLE and Bayesian nonparametrics in NIMBLE, see the NIMBLE User Manual."
  },
  {
    "objectID": "blog/bayesian-nonparametric-models-in-nimble-part-1-density-estimation.html#basic-density-estimation-using-dirichlet-process-mixture-models",
    "href": "blog/bayesian-nonparametric-models-in-nimble-part-1-density-estimation.html#basic-density-estimation-using-dirichlet-process-mixture-models",
    "title": "Bayesian Nonparametric Models in NIMBLE, Part 1: Density Estimation",
    "section": "Basic density estimation using Dirichlet Process Mixture models",
    "text": "Basic density estimation using Dirichlet Process Mixture models\nNIMBLE provides the machinery for nonparametric density estimation by means of Dirichlet process mixture (DPM) models (Ferguson, 1974; Lo, 1984; Escobar, 1994; Escobar and West, 1995). For an independent and identically distributed sample \\(y_1, \\ldots, y_n\\), the model takes the form\n\\[y_i \\mid \\theta_i \\sim p(y_i \\mid \\theta_i), \\quad \\theta_i \\mid G \\sim G, \\quad G \\mid  \\alpha, H \\sim \\mbox{DP}(\\alpha, H), \\quad i=1,\\ldots, n .\\]\nThe NIMBLE implementation of this model is flexible and allows for mixtures of arbitrary kernels, \\(p(y_i \\mid \\theta)\\), which can be either conjugate or non-conjugate to the (also arbitrary) base measure \\(H\\). In the case of conjugate kernel / base measure pairs, NIMBLE is able to detect the presence of the conjugacy and use it to improve the performance of the sampler.\nTo illustrate these capabilities, we consider the estimation of the probability density function of the waiting time between eruptions of the Old Faithful volcano data set available in R.\n\ndata(faithful)\n\nThe observations \\(y_1, \\ldots, y_n\\) correspond to the second column of the dataframe, and \\(n = 272\\)."
  },
  {
    "objectID": "blog/bayesian-nonparametric-models-in-nimble-part-1-density-estimation.html#fitting-a-location-scale-mixture-of-gaussian-distributions-using-the-crp-representation",
    "href": "blog/bayesian-nonparametric-models-in-nimble-part-1-density-estimation.html#fitting-a-location-scale-mixture-of-gaussian-distributions-using-the-crp-representation",
    "title": "Bayesian Nonparametric Models in NIMBLE, Part 1: Density Estimation",
    "section": "Fitting a location-scale mixture of Gaussian distributions using the CRP representation",
    "text": "Fitting a location-scale mixture of Gaussian distributions using the CRP representation\n\nModel specification\nWe first consider a location-scale Dirichlet process mixture of normal distributionss fitted to the transformed data \\(y_i^{*} = \\log (y_i)\\):\n\\[y^{*}_i \\mid \\mu_i, \\sigma^2_i \\sim \\mbox{N}(\\mu_i, \\sigma^2_i), \\quad (\\mu_i, \\sigma^2_i) \\mid G \\sim G, \\quad G \\mid \\alpha, H \\sim \\mbox{DP}(\\alpha, H), \\quad i=1,\\ldots, n,\\]\nwhere \\(H\\) corresponds to a normal-inverse-gamma distribution. This model can be interpreted as providing a Bayesian version of kernel density estimation for \\(y^{*}_i\\) using Gaussian kernels and adaptive bandwidths. On the original scale of the data, this translates into an adaptive log-Gaussian kernel density estimate.\nIntroducing auxiliary variables \\(\\xi_1, \\ldots, \\xi_n\\) that indicate which component of the mixture generates each observation, and integrating over the random measure \\(G\\), we obtain the CRP representation of the model (Blackwell and MacQueen, 1973):\n\\[y_i^{*} \\mid \\{ \\tilde{\\mu}_k \\}, \\{ \\tilde{\\sigma}_k^{2} \\} \\sim \\mbox{N}\\left( \\tilde{\\mu}_{\\xi_i}, \\tilde{\\sigma}^2_{\\xi_i} \\right), \\quad\\quad \\xi \\mid \\alpha \\sim \\mbox{CRP}(\\alpha), \\quad\\quad (\\tilde{\\mu}_k, \\tilde{\\sigma}_k^2) \\mid H \\sim H, \\quad\\quad i=1,\\ldots, n ,\\]\nwhere\n\\[p(\\xi \\mid \\alpha) = \\frac{\\Gamma(\\alpha)}{\\Gamma(\\alpha + n)} \\alpha^{K(\\xi)} \\prod_k  \\Gamma\\left(m_k(\\xi)\\right),\\]\n\\(K(\\xi) \\le n\\) is the number of unique values in the vector \\(\\xi\\), and \\(m_k(\\xi)\\) is the number of times the \\(k\\)-th unique value appears in \\(\\xi\\). This specification makes it clear that each observation belongs to any of at most \\(n\\) normally distributed clusters, and that the CRP distribution corresponds to the prior distribution on the partition structure.\nNIMBLE’s specification of this model is given by\n\nlibrary(nimble)\n\n\ncode &lt;- nimbleCode({\n  for(i in 1:n) {\n    y[i] ~ dnorm(mu[i], var = s2[i])\n    mu[i] &lt;- muTilde[xi[i]]\n    s2[i] &lt;- s2Tilde[xi[i]]\n  }\n  xi[1:n] ~ dCRP(alpha, size = n)\n  for(i in 1:n) {\n    muTilde[i] ~ dnorm(0, var = s2Tilde[i])\n    s2Tilde[i] ~ dinvgamma(2, 1)\n  }\n  alpha ~ dgamma(1, 1)\n})\n\nNote that in the model code the length of the parameter vectors muTilde and s2Tilde has been set to \\(n\\). We do this because the current implementation of NIMBLE requires that the length of vector of parameters be set in advance and does not allow for their number to change between iterations. Hence, if we are to ensure that the algorithm always performs as intended we need to work with the worst case scenario, i.e., the case where there are as many components as observations. While this ensures that the algorithm always works as intended, it is also somewhat inefficient, both in terms of memory requirements (when \\(n\\) is large a large number of unoccupied components need to be maintained) and in terms of computational burden (a large number of parameters that are not required for posterior inference need to be updated at every iteration). When we use a mixture of gamma distributions below, we will show a computational shortcut that improves the efficiency.\nNote also that the value of \\(\\alpha\\) controls the number of components we expect a priori, with larger values of \\(\\alpha\\) corresponding to a larger number of components occupied by the data. Hence, by assigning a prior to \\(\\alpha\\) we add flexibility to the model specification. The particular choice of a Gamma prior allows NIMBLE to use a data-augmentation scheme to efficiently sample from the corresponding full conditional distribution. Alternative prior specifications for \\(\\alpha\\) are possible, in which case the default sampler for this parameter is an adaptive random-walk Metropolis-Hastings algorithm.\n\n\nRunning the MCMC algorithm\nThe following code sets up the data and constants, initializes the parameters, defines the model object, and builds and runs the MCMC algorithm. Because the specification is in terms of a Chinese restaurant process, the default sampler selected by NIMBLE is a collapsed Gibbs sampler (Neal, 2000).\n\nset.seed(1)\n# Model Data\nlFaithful &lt;- log(faithful$waiting)\nstandlFaithful &lt;- (lFaithful - mean(lFaithful)) / sd(lFaithful)\ndata &lt;- list(y = standlFaithful)\n# Model Constants\nconsts &lt;- list(n = length(standlFaithful))\n# Parameter initialization\ninits &lt;- list(xi = sample(1:10, size=consts$n, replace=TRUE),\n              muTilde = rnorm(consts$n, 0, sd = sqrt(10)),\n              s2Tilde = rinvgamma(consts$n, 2, 1),\n              alpha = 1)\n# Model creation and compilation\nrModel &lt;- nimbleModel(code, data = data, inits = inits, constants = consts)\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\ncModel &lt;- compileNimble(rModel)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n# MCMC configuration, creation, and compilation\nconf &lt;- configureMCMC(rModel, monitors = c(\"xi\", \"muTilde\", \"s2Tilde\", \"alpha\"))\n\n===== Monitors =====\nthin = 1: alpha, muTilde, s2Tilde, xi\n===== Samplers =====\nCRP_concentration sampler (1)\n  - alpha\nCRP_cluster_wrapper sampler (544)\n  - s2Tilde[]  (272 elements)\n  - muTilde[]  (272 elements)\nCRP sampler (1)\n  - xi[1:272] \n\nmcmc &lt;- buildMCMC(conf)\ncmcmc &lt;- compileNimble(mcmc, project = rModel)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\nsamples &lt;- runMCMC(cmcmc, niter = 7000, nburnin = 2000, setSeed = TRUE)\n\nrunning chain 1...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\n\nWe can extract the samples from the posterior distributions of the parameters and create trace plots, histograms, and any other summary of interest. For example, for the concentration parameter \\(\\alpha\\) we have:\n\n# Trace plot for the concentration parameter\nts.plot(samples[ , \"alpha\"], xlab = \"iteration\", ylab = expression(alpha))\n\n\n\n\n\n\n\n# Posterior histogram\nhist(samples[ , \"alpha\"], xlab = expression(alpha), main = \"\", ylab = \"Frequency\")\n\n\n\n\n\n\n\nquantile(samples[ , \"alpha\"], c(0.5, 0.025, 0.975))\n\n      50%      2.5%     97.5% \n0.4230550 0.0580579 1.5608958 \n\n\nUnder this model, the posterior predictive distribution for a new observation \\(\\tilde{y}\\), \\(p(\\tilde{y} \\mid y_1, \\ldots, y_n)\\), is the optimal density estimator (under squared error loss). Samples for this estimator can be easily computed from the samples generated by our MCMC:\n\n# posterior samples of the concentration parameter\nalphaSamples &lt;- samples[ , \"alpha\"]\n# posterior samples of the cluster means\nmuTildeSamples &lt;- samples[ , grep('muTilde', colnames(samples))]\n# posterior samples of the cluster variances\ns2TildeSamples &lt;- samples[ , grep('s2Tilde', colnames(samples))]\n# posterior samples of the cluster memberships\nxiSamples &lt;- samples [ , grep('xi', colnames(samples))]\n\nstandlGrid &lt;- seq(-2.5, 2.5, len = 200) # standardized grid on log scale\n\ndensitySamplesStandl &lt;- matrix(0, ncol = length(standlGrid), nrow = nrow(samples))\nfor(i in 1:nrow(samples)){\n  k &lt;- unique(xiSamples[i, ])\n  kNew &lt;- max(k) + 1\n  mk &lt;- c()\n  li &lt;- 1\n  for(l in 1:length(k)) {\n    mk[li] &lt;- sum(xiSamples[i, ] == k[li])\n    li &lt;- li + 1\n  }\n  alpha &lt;- alphaSamples[i]\n\n  muK &lt;-  muTildeSamples[i, k]\n  s2K &lt;-  s2TildeSamples[i, k]\n  muKnew &lt;-  muTildeSamples[i, kNew]\n  s2Knew &lt;-  s2TildeSamples[i, kNew]\n\n  densitySamplesStandl[i, ] &lt;- sapply(standlGrid,\n                function(x)(sum(mk * dnorm(x, muK, sqrt(s2K))) +\n                alpha * dnorm(x, muKnew, sqrt(s2Knew)) )/(alpha+consts$n))\n}\n\nhist(data$y, freq = FALSE, xlim = c(-2.5, 2.5), ylim = c(0,0.75), main = \"\",\n     xlab = \"Waiting times on standardized log scale\")\n## pointwise estimate of the density for standardized log grid\nlines(standlGrid, apply(densitySamplesStandl, 2, mean), lwd = 2, col = 'black')\nlines(standlGrid, apply(densitySamplesStandl, 2, quantile, 0.025), lty = 2, col = 'black')\nlines(standlGrid, apply(densitySamplesStandl, 2, quantile, 0.975), lty = 2, col = 'black')\n\n\n\n\n\n\n\n\nRecall, however, that this is the density estimate for the logarithm of the waiting time. To obtain the density on the original scale we need to apply the appropriate transformation to the kernel.\n\nlgrid &lt;- standlGrid*sd(lFaithful) + mean(lFaithful) # grid on log scale\ndensitySamplesl &lt;- densitySamplesStandl / sd(lFaithful) # density samples for grid on log scale\n\nhist(faithful$waiting, freq = FALSE, xlim = c(40, 100), ylim=c(0, 0.05),\n     main = \"\", xlab = \"Waiting times\")\nlines(exp(lgrid), apply(densitySamplesl, 2, mean)/exp(lgrid), lwd = 2, col = 'black')\nlines(exp(lgrid), apply(densitySamplesl, 2, quantile, 0.025)/exp(lgrid), lty = 2,\n      col = 'black')\nlines(exp(lgrid), apply(densitySamplesl, 2, quantile, 0.975)/exp(lgrid), lty = 2,\n      col = 'black')\n\n\n\n\n\n\n\n\nIn either case, there is clear evidence that the data has two components for the waiting times.\n\n\nGenerating samples from the mixing distribution\nWhile samples from the posterior distribution of linear functionals of the mixing distribution \\(G\\) (such as the predictive distribution above) can be computed directly from the realizations of the collapsed sampler, inference for non-linear functionals of \\(G\\) requires that we first generate samples from the mixing distribution. In NIMBLE we can get posterior samples from the random measure \\(G\\), using the getSamplesDPmeasure function. Note that, in order to get posterior samples from \\(G\\), we need to monitor all the random variables involved in its computations, i.e., the membership variable, xi, the cluster parameters, muTilde and s2Tilde, and the concentration parameter, alpha.\nThe following code generates posterior samples from the random measure \\(G\\). The cMCMC object includes the model and posterior samples from the parameters. The getSamplesDPmeasure returns posterior samples from the random measure as a list (of length equal to the number of samples) of matrices, where each matrix represents the mixture for that iteration, with \\(p+1\\) columns, where \\(p\\) is the dimension of the vector of parameters with distribution \\(G\\) (in this example \\(p=2\\)).\n\nsamplesG &lt;- getSamplesDPmeasure(cmcmc)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\n\nThe following code computes posterior samples of \\(P(\\tilde{y} &gt; 70)\\) using the posterior samples from the random measure \\(G\\). Note that these samples are computed based on the transformed model and a value larger than 70 corresponds to a value larger than 0.03557236 on the above defined grid.\n\nweightIndex &lt;- grep('weight', colnames(samplesG[[1]]))\nmuTildeIndex &lt;- grep('muTilde', colnames(samplesG[[1]]))\ns2TildeIndex &lt;- grep('s2Tilde', colnames(samplesG[[1]]))\n\nprobY70 &lt;- rep(0, nrow(samples))  # posterior samples of P(y.tilde &gt; 70)\nfor(i in seq_len(nrow(samples))) {\n  probY70[i] &lt;- sum(samplesG[[i]][, weightIndex] *\n                    pnorm(0.03557236, mean = samplesG[[i]][, muTildeIndex],\n                      sd = sqrt(samplesG[[i]][, s2TildeIndex]), lower.tail = FALSE))\n}\n\nhist(probY70,  xlab = \"Probability\", ylab = \"P(yTilde &gt; 70 | data)\" , main = \"\" )"
  },
  {
    "objectID": "blog/bayesian-nonparametric-models-in-nimble-part-1-density-estimation.html#fitting-a-mixture-of-gamma-distributions-using-the-crp-representation",
    "href": "blog/bayesian-nonparametric-models-in-nimble-part-1-density-estimation.html#fitting-a-mixture-of-gamma-distributions-using-the-crp-representation",
    "title": "Bayesian Nonparametric Models in NIMBLE, Part 1: Density Estimation",
    "section": "Fitting a mixture of gamma distributions using the CRP representation",
    "text": "Fitting a mixture of gamma distributions using the CRP representation\nNIMBLE is not restricted to using Gaussian kernels in DPM models. In the case of the Old Faithful data, an alternative to the mixture of Gaussian kernels on the logarithmic scale that we presented in the previous section is a (scale-and-shape) mixture of Gamma distributions on the original scale of the data.\n\nModel specification\nIn this case, the model takes the form\n\\[y_i \\mid \\{ \\tilde{\\beta}_k \\}, \\{ \\tilde{\\lambda}_k \\} \\sim \\mbox{Gamma}\\left( \\tilde{\\beta}_{\\xi_i}, \\tilde{\\lambda}_{\\xi_i} \\right), \\quad\\quad \\xi \\mid \\alpha \\sim \\mbox{CRP}(\\alpha), \\quad\\quad (\\tilde{\\beta}_k, \\tilde{\\lambda}_k) \\mid H \\sim H ,\\]\nwhere \\(H\\) corresponds to the product of two independent Gamma distributions. The following code provides the NIMBLE specification for the model:\n\ncode &lt;- nimbleCode({\n  for(i in 1:n) {\n    y[i] ~ dgamma(shape = beta[i], scale = lambda[i])\n    beta[i] &lt;- betaTilde[xi[i]]\n    lambda[i] &lt;- lambdaTilde[xi[i]]\n  }\n  xi[1:n] ~ dCRP(alpha, size = n)\n  for(i in 1:50) { # only 50 cluster parameters\n    betaTilde[i] ~ dgamma(shape = 71, scale = 2)\n    lambdaTilde[i] ~ dgamma(shape = 2, scale = 2)\n  }\n  alpha ~ dgamma(1, 1)\n})\n\nNote that in this case the vectors betaTilde and lambdaTilde have length \\(50 \\ll n = 272\\). This is done to reduce the computational and storage burdens associated with the sampling algorithm. You could think about this approach as truncating the process, except that it can be thought of as an exact truncation. Indeed, under the CRP representation, using parameter vector(s) with a length that is shorter than the number of observations in the sample will lead to a proper algorithm as long as the number of components instatiated by the sampler is strictly lower than the length of the parameter vector(s) for every iteration of the sampler.\n\n\nRunning the MCMC algorithm\nThe following code sets up the model data and constants, initializes the parameters, defines the model object, and builds and runs the MCMC algorithm for the mixture of Gamma distributions. Note that, when building the MCMC, a warning message about the number of cluster parameters is generated. This is because the lengths of betaTilde and lambdaTilde are smaller than \\(n\\). Also, note that no error message is generated during execution, which indicates that the number of clusters required never exceeded the maximum of 50.\n\ndata &lt;- list(y = faithful$waiting)\nset.seed(1)\ninits &lt;- list(xi = sample(1:10, size=consts$n, replace=TRUE),\n              betaTilde = rgamma(50, shape = 71, scale = 2),\n              lambdaTilde = rgamma(50, shape = 2, scale = 2),\n              alpha = 1)\nrModel &lt;- nimbleModel(code, data = data, inits = inits, constants = consts)\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\ncModel &lt;- compileNimble(rModel)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\nconf &lt;- configureMCMC(rModel, monitors = c(\"xi\", \"betaTilde\", \"lambdaTilde\", \"alpha\"))\n\n===== Monitors =====\nthin = 1: alpha, betaTilde, lambdaTilde, xi\n===== Samplers =====\nCRP_concentration sampler (1)\n  - alpha\nCRP_cluster_wrapper sampler (100)\n  - betaTilde[]  (50 elements)\n  - lambdaTilde[]  (50 elements)\nCRP sampler (1)\n  - xi[1:272] \n\nmcmc &lt;- buildMCMC(conf)\n\n  [Warning] sampler_CRP: The number of clusters based on the cluster parameters\n            is less than the number of potential clusters. The MCMC is not\n            strictly valid if it ever proposes more components than cluster\n            parameters exist; NIMBLE will warn you if this occurs.\n\ncmcmc &lt;- compileNimble(mcmc, project = rModel)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\nsamples &lt;- runMCMC(cmcmc, niter = 7000, nburnin = 2000, setSeed = TRUE)\n\nrunning chain 1...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\n\nIn this case we use the posterior samples of the parameters to construct a trace plot and estimate the posterior distribution of \\(\\alpha\\):\n\n# Trace plot of the posterior samples of the concentration parameter\nts.plot(samples[ , 'alpha'], xlab = \"iteration\", ylab = expression(alpha))\n\n\n\n\n\n\n\n# Histogram of the posterior samples for the concentration parameter \nhist(samples[ , 'alpha'], xlab = expression(alpha), ylab = \"Frequency\", main = \"\")\n\n\n\n\n\n\n\n\n\n\nGenerating samples from the mixing distribution\nAs before, we obtain samples from the posterior distribution of \\(G\\) using the getSamplesDPmeasure function.\n\nsamplesG &lt;- getSamplesDPmeasure(cmcmc)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\n## sampleDPmeasure: Approximating the random measure by a finite stick-breaking representation with and error smaller than 1e-10, leads to a truncation level of 28.\n\nWe use these samples to create an estimate of the density of the data along with a pointwise 95% credible band:\n\ngrid &lt;- seq(40, 100, len = 200)\n\nweightIndex &lt;- grep('weight', colnames(samplesG[[1]]))\nbetaTildeIndex &lt;- grep('betaTilde', colnames(samplesG[[1]]))\nlambdaTildeIndex &lt;- grep('lambdaTilde', colnames(samplesG[[1]]))\n\ndensitySamples &lt;- matrix(0, ncol = length(grid), nrow = nrow(samples))\nfor(iter in seq_len(nrow(samples))) {\n  densitySamples[iter, ] &lt;- sapply(grid, function(x)\n    sum(samplesG[[iter]][ , weightIndex] * dgamma(x, shape = samplesG[[iter]][ , betaTildeIndex],\n                scale = samplesG[[iter]][ , lambdaTildeIndex])))\n}\n\nhist(faithful$waiting, freq = FALSE, xlim = c(40,100), ylim = c(0, .05), main = \"\",\n   ylab = \"\", xlab = \"Waiting times\")\nlines(grid, apply(densitySamples, 2, mean), lwd = 2, col = 'black')\nlines(grid, apply(densitySamples, 2, quantile, 0.025), lwd = 2, lty = 2, col = 'black')\nlines(grid, apply(densitySamples, 2, quantile, 0.975), lwd = 2, lty = 2, col = 'black')\n\n\n\n\n\n\n\n\nAgain, we see that the density of the data is bimodal, and looks very similar to the one we obtained before."
  },
  {
    "objectID": "blog/bayesian-nonparametric-models-in-nimble-part-1-density-estimation.html#fitting-a-dp-mixture-of-gammas-using-a-stick-breaking-representation",
    "href": "blog/bayesian-nonparametric-models-in-nimble-part-1-density-estimation.html#fitting-a-dp-mixture-of-gammas-using-a-stick-breaking-representation",
    "title": "Bayesian Nonparametric Models in NIMBLE, Part 1: Density Estimation",
    "section": "Fitting a DP mixture of Gammas using a stick-breaking representation",
    "text": "Fitting a DP mixture of Gammas using a stick-breaking representation\n\nModel specification\nAn alternative representation of the Dirichlet process mixture uses the stick-breaking representation of the random distribution \\(G\\) (Sethuraman, 1994). NIMBLE allows us to specify an approximation that involves a truncation of the Dirichlet process to a finite number of atoms, \\(L\\). The resulting model therefore reduces to a finite mixture with \\(L\\) components and a very particular prior on the weights of the mixture components.\nIntroducing auxiliary variables, \\(z_1, \\ldots, z_n\\), that indicate which component generated each observation, the corresponding model for the mixture of Gamma densities discussed in the previous section takes the form\n\\[y_i \\mid \\{ {\\beta}_k^{\\star} \\}, \\{ {\\lambda}_k^{\\star} \\}, z_i \\sim \\mbox{Gamma}\\left( {\\beta}_{z_i}^{\\star}, {\\lambda}_{z_i}^{\\star} \\right), \\quad\\quad \\boldsymbol{z} \\mid \\boldsymbol{w} \\sim \\mbox{Discrete}(\\boldsymbol{w}), \\quad\\quad ({\\beta}_k^{\\star}, {\\lambda}_k^{\\star}) \\mid H \\sim H ,\\]\nwhere \\(H\\) is again the product of two independent Gamma distributions,\n\\[w_1=v_1, \\quad\\quad w_l=v_l\\prod_{m=1}^{l-1}(1-v_m), \\quad l=2, \\ldots, L-1,\\quad\\quad w_L=\\prod_{m=1}^{L-1}(1-v_m)\\]\nwith \\(v_l \\mid \\alpha\\sim \\mbox{Beta}(1, \\alpha), l=1, \\ldots, L-1\\). The following code provides the NIMBLE specification for the model:\n\ncode &lt;- nimbleCode(\n  {\n    for(i in 1:n) {\n      y[i] ~ dgamma(shape = beta[i], scale = lambda[i])\n      beta[i] &lt;- betaStar[z[i]]\n      lambda[i] &lt;- lambdaStar[z[i]]\n      z[i] ~ dcat(w[1:Trunc])\n    }\n    for(i in 1:(Trunc-1)) { # stick-breaking variables\n      v[i] ~ dbeta(1, alpha)\n    }\n    w[1:Trunc] &lt;- stick_breaking(v[1:(Trunc-1)]) # stick-breaking weights\n    for(i in 1:Trunc) {\n      betaStar[i] ~ dgamma(shape = 71, scale = 2)\n      lambdaStar[i] ~ dgamma(shape = 2, scale = 2)\n    }\n    alpha ~ dgamma(1, 1)\n  }\n)\n\nNote that the truncation level \\(L\\) of \\(G\\) has been set to a value Trunc, which is to be defined in the constants argument of the nimbleModel function.\n\n\nRunning the MCMC algorithm\nThe following code sets up the model data and constants, initializes the parameters, defines the model object, and builds and runs the MCMC algorithm for the mixture of Gamma distributions. When a stick-breaking representation is used, a blocked Gibbs sampler is assigned (Ishwaran, 2001; Ishwaran and James, 2002).\n\ndata &lt;- list(y = faithful$waiting)\nset.seed(1)\nconsts &lt;- list(n = length(faithful$waiting), Trunc = 50)\ninits &lt;- list(betaStar = rgamma(consts$Trunc, shape = 71, scale = 2),\n              lambdaStar = rgamma(consts$Trunc, shape = 2, scale = 2),\n              v = rbeta(consts$Trunc-1, 1, 1),\n              z = sample(1:10, size = consts$n, replace = TRUE),\n              alpha = 1)\n\nrModel &lt;- nimbleModel(code, data = data, inits = inits, constants = consts)\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\ncModel &lt;- compileNimble(rModel)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\nconf &lt;- configureMCMC(rModel, monitors = c(\"w\", \"betaStar\", \"lambdaStar\", 'z', 'alpha'))\n\n===== Monitors =====\nthin = 1: alpha, betaStar, lambdaStar, w, z\n===== Samplers =====\nRW sampler (101)\n  - betaStar[]  (50 elements)\n  - lambdaStar[]  (50 elements)\n  - alpha\nconjugate sampler (49)\n  - v[]  (49 elements)\ncategorical sampler (272)\n  - z[]  (272 elements)\n\nmcmc &lt;- buildMCMC(conf)\ncmcmc &lt;- compileNimble(mcmc, project = rModel)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\nsamples &lt;- runMCMC(cmcmc, niter = 24000, nburnin = 4000, setSeed = TRUE)\n\nrunning chain 1...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\n\nUsing the stick-breaking approximation automatically provides an approximation, \\(G_L\\), of the random distribution \\(G\\). The following code computes posterior samples of \\(G_L\\) using posterior samples from the samples object, and from them, a density estimate for the data.\n\nbetaStarSamples &lt;- samples[ , grep('betaStar', colnames(samples))]\nlambdaStarSamples &lt;- samples[ , grep('lambdaStar', colnames(samples))]\nweightSamples &lt;- samples[ , grep('w', colnames(samples))]\n\ngrid &lt;- seq(40, 100, len = 200)\n\ndensitySamples &lt;- matrix(0, ncol = length(grid), nrow = nrow(samples))\nfor(i in 1:nrow(samples)) {\n  densitySamples[i, ] &lt;- sapply(grid, function(x)\n    sum(weightSamples[i, ] * dgamma(x, shape = betaStarSamples[i, ],\n                                    scale = lambdaStarSamples[i, ])))\n}\n\nhist(faithful$waiting, freq = FALSE,  xlab = \"Waiting times\", ylim=c(0,0.05),\n     main = '')\nlines(grid, apply(densitySamples, 2, mean), lwd = 2, col = 'black')\nlines(grid, apply(densitySamples, 2, quantile, 0.025), lwd = 2, lty = 2, col = 'black')\nlines(grid, apply(densitySamples, 2, quantile, 0.975), lwd = 2, lty = 2, col = 'black')\n\n\n\n\n\n\n\n\nAs expected, this estimate looks identical to the one we obtained through the CRP representation of the process."
  },
  {
    "objectID": "blog/bayesian-nonparametric-models-in-nimble-part-1-density-estimation.html#more-information-and-future-development",
    "href": "blog/bayesian-nonparametric-models-in-nimble-part-1-density-estimation.html#more-information-and-future-development",
    "title": "Bayesian Nonparametric Models in NIMBLE, Part 1: Density Estimation",
    "section": "More information and future development",
    "text": "More information and future development\nPlease see our User Manual for more details.\nWe’re in the midst of improvements to the existing BNP functionality as well as adding additional Bayesian nonparametric models, such as hierarchical Dirichlet processes and Pitman-Yor processes, so please add yourself to our announcement or user support/discussion Google groups.\n\nReferences\nBlackwell, D. and MacQueen, J. 1973. Ferguson distributions via Polya urn schemes. The Annals of Statistics 1:353-355.\nFerguson, T.S. 1974. Prior distribution on the spaces of probability measures. Annals of Statistics 2:615-629.\nLo, A.Y. 1984. On a class of Bayesian nonparametric estimates I: Density estimates. The Annals of Statistics 12:351-357.\nEscobar, M.D. 1994. Estimating normal means with a Dirichlet process prior. Journal of the American Statistical Association 89:268-277.\nEscobar, M.D. and West, M. 1995. Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association 90:577-588.\nIshwaran, H. and James, L.F. 2001. Gibbs sampling methods for stick-breaking priors. Journal of the American Statistical Association 96: 161-173.\nIshwaran, H. and James, L.F. 2002. Approximate Dirichlet process computing in finite normal mixtures: smoothing and prior information. Journal of Computational and Graphical Statistics 11:508-532.\nNeal, R. 2000. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics 9:249-265.\nSethuraman, J. 1994. A constructive definition of Dirichlet prior. Statistica Sinica 2: 639-650."
  },
  {
    "objectID": "blog/posterior-predictive-sampling-and-other-post-mcmc-use-of-samples-in-nimble.html",
    "href": "blog/posterior-predictive-sampling-and-other-post-mcmc-use-of-samples-in-nimble.html",
    "title": "Posterior predictive sampling and other post-MCMC use of samples in NIMBLE",
    "section": "",
    "text": "Once one has samples from an MCMC, one often wants to do some post hoc manipulation of the samples. An important example is posterior predictive sampling, which is needed for posterior predictive checking.\nWith posterior predictive sampling, we need to simulate new data values, once for each posterior sample. These samples can then be compared with the actual data as a model check.\nIn this example, we’ll follow the posterior predictive checking done in the Gelman et al. Bayesian Data Analysis book, using Newcomb’s speed of light measurements (Section 6.3)."
  },
  {
    "objectID": "blog/posterior-predictive-sampling-and-other-post-mcmc-use-of-samples-in-nimble.html#posterior-predictive-sampling-using-a-loop-in-r",
    "href": "blog/posterior-predictive-sampling-and-other-post-mcmc-use-of-samples-in-nimble.html#posterior-predictive-sampling-using-a-loop-in-r",
    "title": "Posterior predictive sampling and other post-MCMC use of samples in NIMBLE",
    "section": "Posterior predictive sampling using a loop in R",
    "text": "Posterior predictive sampling using a loop in R\nSimon Newcomb made 66 measurements of the speed of light, which one might model using a normal distribution. One question discussed in Gelman et al. is whether the lowest measurements, which look like outliers, could have reasonably come from a normal distribution.\n\nSetup\nWe set up the nimble model.\n\nlibrary(nimble)\n\n\ncode &lt;- nimbleCode({\n    ## noninformative priors\n    mu ~ dflat()\n    sigma ~ dhalfflat()\n    ## likelihood\n    for(i in 1:n) {\n        y[i] ~ dnorm(mu, sd = sigma)\n    }\n})\n\ndata &lt;- list(y = MASS::newcomb)\ninits &lt;- list(mu = 0, sigma = 5)\nconstants &lt;- list(n = length(data$y))\n\nmodel &lt;- nimbleModel(code = code, data = data, constants = constants, inits = inits)\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\nNext we’ll create some vectors of node names that will be useful for our manipulations.\n\n## Ensure we have the nodes needed to simulate new datasets\ndataNodes &lt;- model$getNodeNames(dataOnly = TRUE)\nparentNodes &lt;- model$getParents(dataNodes, stochOnly = TRUE)  # `getParents` is new in nimble 0.11.0\n## Ensure we have both data nodes and deterministic intermediates (e.g., lifted nodes)\nsimNodes &lt;- model$getDependencies(parentNodes, self = FALSE)\n\nNow run the MCMC.\n\ncmodel  &lt;- compileNimble(model)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\nmcmc    &lt;- buildMCMC(model, monitors = parentNodes)\n\n===== Monitors =====\nthin = 1: mu, sigma\n===== Samplers =====\nconjugate sampler (2)\n  - mu\n  - sigma\n\ncmcmc   &lt;- compileNimble(mcmc, project = model)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\nsamples &lt;- runMCMC(cmcmc, niter = 1000, nburnin = 500)\n\nrunning chain 1...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\n\n\n\nPosterior predictive sampling by direct variable assignment\nWe’ll loop over the samples and use the compiled model (uncompiled would be ok too, but slower) to simulate new datasets.\n\nnSamp &lt;- nrow(samples)\nn &lt;- length(data$y)\nppSamples &lt;- matrix(0, nSamp, n)\n\nset.seed(1)\nfor(i in 1:nSamp){\n  cmodel[[\"mu\"]] &lt;- samples[i, \"mu\"]             ## or cmodel$mu &lt;- samples[i, \"mu\"]\n  cmodel[[\"sigma\"]] &lt;- samples[i, \"sigma\"]\n  cmodel$simulate(simNodes, includeData = TRUE)\n  ppSamples[i, ] &lt;- cmodel[[\"y\"]]\n}\n\n\n\nPosterior predictive sampling using values\nThat’s fine, but we needed to manually insert values for the different variables. For a more general solution, we can use nimble’s values function as follows.\n\nppSamples &lt;- matrix(0, nrow = nSamp, ncol =\n          length(model$expandNodeNames(dataNodes, returnScalarComponents = TRUE)))\npostNames &lt;- colnames(samples)\n\nset.seed(1)\nsystem.time({\nfor(i in seq_len(nSamp)) {\n    values(cmodel, postNames) &lt;- samples[i, ]  # assign 'flattened' values\n    cmodel$simulate(simNodes, includeData = TRUE)\n    ppSamples[i, ] &lt;- values(cmodel, dataNodes)\n}\n})\n\n   user  system elapsed \n  4.324   0.003   4.327 \n\n\nSide note: For large models, it might be faster to use the variable names as the second argument to values() rather than the names of all the elements of the variables. If one chooses to do this, it’s important to check that the ordering of variables in the ‘flattened’ values in samples is the same as the ordering of variables in the second argument to values so that the first line of the for loop assigns the values from samples correctly into the model.\n\n\nDoing the posterior predictive check\nAt this point, we can implement the check we want using our chosen discrepancy measure. Here a simple check uses the minimum observation.\n\nobsMin &lt;- min(data$y)\nppMin &lt;- apply(ppSamples, 1, min)\n\n# ## Check with plot in Gelman et al. (3rd edition), Figure 6.3\nhist(ppMin, xlim = c(-50, 20),\n    main = \"Discrepancy = min(y)\",\n    xlab = \"min(y_rep)\")\nabline(v = obsMin, col = 'red')"
  },
  {
    "objectID": "blog/posterior-predictive-sampling-and-other-post-mcmc-use-of-samples-in-nimble.html#fast-posterior-predictive-sampling-using-a-nimblefunction",
    "href": "blog/posterior-predictive-sampling-and-other-post-mcmc-use-of-samples-in-nimble.html#fast-posterior-predictive-sampling-using-a-nimblefunction",
    "title": "Posterior predictive sampling and other post-MCMC use of samples in NIMBLE",
    "section": "Fast posterior predictive sampling using a nimbleFunction",
    "text": "Fast posterior predictive sampling using a nimbleFunction\nThe approach above could be slow, even with a compiled model, because the loop is carried out in R. We could instead do all the work in a compiled nimbleFunction.\n\nWriting the nimbleFunction\nLet’s set up a nimbleFunction. In the setup code, we’ll manipulate the nodes and variables, similarly to the code above. In the run code, we’ll loop through the samples and simulate, also similarly.\nRemember that all querying of the model structure needs to happen in the setup code. We also need to pass the MCMC object to the nimble function, so that we can determine at setup time the names of the variables we are copying from the posterior samples into the model.\nThe run code takes the actual samples as the input argument, so the nimbleFunction will work regardless of how long the MCMC was run for.\n\nppSamplerNF &lt;- nimbleFunction(\n          setup = function(model, mcmc) {\n              dataNodes &lt;- model$getNodeNames(dataOnly = TRUE)\n              parentNodes &lt;- model$getParents(dataNodes, stochOnly = TRUE)\n              cat(\"Stochastic parents of data are:\", paste(parentNodes, collapse = ','), \".\\n\")\n              simNodes &lt;- model$getDependencies(parentNodes, self = FALSE)\n              vars &lt;- mcmc$mvSamples$getVarNames()  # need ordering of variables in mvSamples / samples matrix\n              cat(\"Using posterior samples of:\", paste(vars, collapse = ','), \".\\n\")\n              n &lt;- length(model$expandNodeNames(dataNodes, returnScalarComponents = TRUE))\n          },\n          run = function(samples = double(2)) {\n              nSamp &lt;- dim(samples)[1]\n              ppSamples &lt;- matrix(nrow = nSamp, ncol = n)\n              for(i in 1:nSamp) {\n                    values(model, vars) &lt;&lt;- samples[i, ]\n                    model$simulate(simNodes, includeData = TRUE)\n                    ppSamples[i, ] &lt;- values(model, dataNodes)\n              }\n              returnType(double(2))\n              return(ppSamples)\n          })\n\n\n\nUsing the nimbleFunction\nWe’ll create the instance of the nimbleFunction for this model and MCMC.\nThen we run the compiled nimbleFunction.\n\n## Create the sampler for this model and this MCMC.\nppSampler &lt;- ppSamplerNF(model, mcmc)\n\nStochastic parents of data are: mu,sigma .\nUsing posterior samples of: mu,sigma .\n\ncppSampler &lt;- compileNimble(ppSampler, project = model)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n## Check ordering of variables is same in 'vars' and in 'samples'.\ncolnames(samples)\n\n[1] \"mu\"    \"sigma\"\n\nidentical(colnames(samples), model$expandNodeNames(mcmc$mvSamples$getVarNames()))\n\n[1] TRUE\n\nset.seed(1)\nsystem.time(ppSamples_via_nf &lt;- cppSampler$run(samples))\n\n   user  system elapsed \n  0.002   0.000   0.002 \n\nidentical(ppSamples, ppSamples_via_nf)\n\n[1] TRUE\n\n\nSo we get exactly the same results (note the use of set.seed to ensure this) but much faster.\nHere the speed doesn’t really matter but for more samples and larger models it often will, even after accounting for the time spent to compile the nimbleFunction."
  },
  {
    "objectID": "blog/version-0-9-1-of-nimble-released.html",
    "href": "blog/version-0-9-1-of-nimble-released.html",
    "title": "Version 0.9.1 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC). Version 0.9.1 is primarily a bug fix release but also provides some minor improvements in functionality.\nUsers of NIMBLE in R 4.0 on Windows MUST upgrade to this release for NIMBLE to work.\nNew features and bug fixes include:\n\nswitched to use of system2() from system() to avoid an issue on Windows in R 4.0;\nmodified various adaptive MCMC samplers so the exponent controlling the scale decay of the adaptation is adjustable by user;\nallowed pmin() and pmax() to be used in models;\nimproved handling of NA values in the dCRP distribution; and\nimproved handling of cases where indexing goes beyond the extent of a variable in expandNodeNames() and related queries of model structure.\n\nPlease see the release notes on our website for more details."
  },
  {
    "objectID": "blog/version-0-6-2-released.html",
    "href": "blog/version-0-6-2-released.html",
    "title": "Version 0.6-2 released!",
    "section": "",
    "text": "Version 0.6-2 is a minor release with a variety of useful functionality for users.\nChanges as of Version 0.6-2 include:\n\nuser-defined distributions can be used in BUGS code without needing to call the registerDistributions() function (unless one wants to specify alternative parameterizations, distribution range or that the distribution is discrete),\nusers can now specify the use of conjugate (Gibbs) samplers for nodes in a model,\nNIMBLE will now check the run code of nimbleFunctions for functions (in particular R functions) that are not part of the DSL and will not compile,\nadded getBound() functionality to find the lower and upper bounds of a node either from R or in DSL code,\nadded functionality to get distributional information about a node in a model or information about a distribution based on the name of the density function; these may be useful in setup code for algorithms,\nmultinomial and categorical distributions now allow probs arguments that do not sum to one (these will be internally normalized) and\na variety of bug fixes.\n\nPlease see the NEWS file in the installed package for more details."
  },
  {
    "objectID": "blog/version-0-6-11-of-nimble-released.html",
    "href": "blog/version-0-6-11-of-nimble-released.html",
    "title": "Version 0.6-11 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website.\nVersion 0.6-11 has important new features, notably support for Bayesian nonparametric mixture modeling, and more are on the way in the next few months.\nNew features include:\n\nsupport for Bayesian nonparametric mixture modeling using Dirichlet process mixtures, with specialized MCMC samplers automatically assigned in NIMBLE’s default MCMC (See Chapter 10 of the manual for details);\nadditional resampling methods available with the auxiliary and bootstrap particle filters;\nuser-defined filtering algorithms can be used with NIMBLE’s particle MCMC samplers;\nMCMC thinning intervals can be modified at MCMC run-time;\nboth runMCMC() and nimbleMCMC() now drop burn-in samples before thinning, making their behavior consistent with each other;\nincreased functionality for the setSeed argument in nimbleMCMC() and runMCMC();\nnew functionality for specifying the order in which sampler functions are executed in an MCMC; and\ninvalid dynamic indexes now result in a warning and NaN values but do not cause execution to error out, allowing MCMC sampling to continue.\n\nPlease see the NEWS file in the installed package for more details"
  },
  {
    "objectID": "blog/nimble-workshop-in-switzerland-23-25-april.html",
    "href": "blog/nimble-workshop-in-switzerland-23-25-april.html",
    "title": "NIMBLE workshop in Switzerland, 23-25 April",
    "section": "",
    "text": "There will be a three-day NIMBLE workshop in Sempach, Switzerland, 23-25 April, hosted at the Swiss Ornithological Institute. More information can be found here: http://www.phidot.org/forum/viewtopic.php?f=8&t=3586. Examples will be oriented towards ecological applications, but otherwise the workshop content will be general."
  },
  {
    "objectID": "blog/bayesian-nonparametric-models-in-nimble-general-multivariate-models.html",
    "href": "blog/bayesian-nonparametric-models-in-nimble-general-multivariate-models.html",
    "title": "Bayesian Nonparametric Models in NIMBLE: General Multivariate Models",
    "section": "",
    "text": "NIMBLE is a hierarchical modeling package that uses nearly the same language for model specification as the popular MCMC packages WinBUGS, OpenBUGS and JAGS, while making the modeling language extensible — you can add distributions and functions — and also allowing customization of the algorithms used to estimate the parameters of the model.\nNIMBLE supports Markov chain Monte Carlo (MCMC) inference for Bayesian nonparametric (BNP) mixture models. Specifically, NIMBLE provides functionality for fitting models involving Dirichlet process priors using either the Chinese Restaurant Process (CRP) or a truncated stick-breaking (SB) representation.\nIn version 0.10.1, we’ve extended NIMBLE to be able to handle more general multivariate models when using the CRP prior. In particular, one can now easily use the CRP prior when multiple observations (or multiple latent variables) are being jointly clustered. For example, in a longitudinal study, one may want to cluster at the individual level, i.e., to jointly cluster all of the observations for each of the individuals in the study. (Formerly this was only possible in NIMBLE by specifying the observations for each individual as coming from a single multivariate distribution.)\nThis allows one to specify a multivariate mixture kernel as the product of univariate ones. This is particularly useful when working with discrete data. In general, multivariate extensions of well-known univariate discrete distributions, such as the Bernoulli, Poisson and Gamma, are not straightforward. For example, for multivariate count data, a multivariate Poisson distribution might appear to be a good fit, yet its definition is not trivial, inference is cumbersome, and the model lacks flexibility to deal with overdispersion. See Inouye et al. (2017) for a review on multivariate distributions for count data based on the Poisson distribution.\nIn this post, we illustrate NIMBLE’s new extended BNP capabilities by modelling multivariate discrete data. Specifically, we show how to model multivariate count data from a longitudinal study under a nonparametric framework. The modeling approach is simple and introduces correlation in the measurements within subjects.\nFor more detailed information on NIMBLE and Bayesian nonparametrics in NIMBLE, see the User Manual."
  },
  {
    "objectID": "blog/bayesian-nonparametric-models-in-nimble-general-multivariate-models.html#overview",
    "href": "blog/bayesian-nonparametric-models-in-nimble-general-multivariate-models.html#overview",
    "title": "Bayesian Nonparametric Models in NIMBLE: General Multivariate Models",
    "section": "",
    "text": "NIMBLE is a hierarchical modeling package that uses nearly the same language for model specification as the popular MCMC packages WinBUGS, OpenBUGS and JAGS, while making the modeling language extensible — you can add distributions and functions — and also allowing customization of the algorithms used to estimate the parameters of the model.\nNIMBLE supports Markov chain Monte Carlo (MCMC) inference for Bayesian nonparametric (BNP) mixture models. Specifically, NIMBLE provides functionality for fitting models involving Dirichlet process priors using either the Chinese Restaurant Process (CRP) or a truncated stick-breaking (SB) representation.\nIn version 0.10.1, we’ve extended NIMBLE to be able to handle more general multivariate models when using the CRP prior. In particular, one can now easily use the CRP prior when multiple observations (or multiple latent variables) are being jointly clustered. For example, in a longitudinal study, one may want to cluster at the individual level, i.e., to jointly cluster all of the observations for each of the individuals in the study. (Formerly this was only possible in NIMBLE by specifying the observations for each individual as coming from a single multivariate distribution.)\nThis allows one to specify a multivariate mixture kernel as the product of univariate ones. This is particularly useful when working with discrete data. In general, multivariate extensions of well-known univariate discrete distributions, such as the Bernoulli, Poisson and Gamma, are not straightforward. For example, for multivariate count data, a multivariate Poisson distribution might appear to be a good fit, yet its definition is not trivial, inference is cumbersome, and the model lacks flexibility to deal with overdispersion. See Inouye et al. (2017) for a review on multivariate distributions for count data based on the Poisson distribution.\nIn this post, we illustrate NIMBLE’s new extended BNP capabilities by modelling multivariate discrete data. Specifically, we show how to model multivariate count data from a longitudinal study under a nonparametric framework. The modeling approach is simple and introduces correlation in the measurements within subjects.\nFor more detailed information on NIMBLE and Bayesian nonparametrics in NIMBLE, see the User Manual."
  },
  {
    "objectID": "blog/bayesian-nonparametric-models-in-nimble-general-multivariate-models.html#bnp-analysis-of-epileptic-seizure-count-data",
    "href": "blog/bayesian-nonparametric-models-in-nimble-general-multivariate-models.html#bnp-analysis-of-epileptic-seizure-count-data",
    "title": "Bayesian Nonparametric Models in NIMBLE: General Multivariate Models",
    "section": "BNP analysis of epileptic seizure count data",
    "text": "BNP analysis of epileptic seizure count data\nWe illustrate the use of nonparametric multivariate mixture models for modeling counts of epileptic seizures from a longitudinal study of the drug progabide as an adjuvant antiepileptic chemotherapy. The data, originally reported in Leppik et al. (1985), arise from a clinical trial of 59 people with epilepsy. At four clinic visits, subjects reported the number of seizures occurring over successive two-week periods. Additional data include the baseline seizure count and the age of the patient. Patients were randomized to receive either progabide or a placebo, in addition to standard chemotherapy.\n\nload(\"../nimbleExamples/seizures.Rda\")\nnames(seizures)\n\n[1] \"id\"    \"seize\" \"visit\" \"trt\"   \"age\"  \n\n\n\nhead(seizures)\n\n   id seize visit trt age\n1 101    76     0   1  18\n2 101    11     1   1  18\n3 101    14     2   1  18\n4 101     9     3   1  18\n5 101     8     4   1  18\n6 102    38     0   1  32\n\n\n\nModel formulation\nWe model the joint distribution of the baseline number of seizures and the counts from each of the two-week periods as a Dirichlet Process mixture (DPM) of products of Poisson distributions. Let \\(\\boldsymbol{y}_i=(y_{i, 1}, \\ldots, y_{i,5})\\), where \\(y_{i,j}\\) denotes the seizure count for patient \\(i\\) measured at visit \\(j\\), for \\(i=1, \\ldots, 59\\), and \\(j=1, \\ldots, 5\\). The value for \\(j=1\\) is the baseline count. The model takes the form\n\\[  \\boldsymbol{y}_i \\mid \\boldsymbol{\\lambda}_{i} \\sim \\prod_{j=1}^5 \\mbox{Poisson}(\\lambda_{i, j}),  \\quad\\quad  \\boldsymbol{\\lambda}_{i} \\mid G \\sim G,  \\quad\\quad  G \\sim DP(\\alpha, H),  \\]\nwhere \\(\\boldsymbol{\\lambda}_{i}=(\\lambda_{i,1}, \\ldots\\lambda_{i,5})\\) and \\(H\\) corresponds to a product of Gamma distributions.\nOur specification uses a product of Poisson distributions as the kernel in the DPM which, at first sight, would suggest independence of the repeated seizure count measurements. However, because we are mixing over the parameters, this specification in fact induces dependence within subjects, with the strength of the dependence being inferred from the data. In order to specify the model in NIMBLE, first we translate the information in seize into a matrix and then we write the NIMBLE code.\nWe specify this model in NIMBLE with the following code in R. The vector xi contains the latent cluster IDs, one for each patient.\n\nlibrary(nimble)\n\n\nn &lt;- 59\nJ &lt;- 5\ndata &lt;- list(y = matrix(seizures$seize, ncol = J, nrow = n, byrow = TRUE))\nconstants &lt;- list(n = n, J = J)\n\ncode &lt;- nimbleCode({\n  for(i in 1:n) {\n    for(j in 1:J) {\n      y[i, j] ~ dpois(lambda[xi[i], j])\n    }\n  }\n  for(i in 1:n) {\n    for(j in 1:J) {\n      lambda[i, j] ~ dgamma(shape = 1, rate = 0.1)\n    }\n  }\n  xi[1:n] ~ dCRP(conc = alpha, size = n)\n  alpha ~ dgamma(shape = 1, rate = 1)\n})\n\n\n\nRunning the MCMC\nThe following code sets up the data and constants, initializes the parameters, defines the model object, and builds and runs the MCMC algorithm. For speed, the MCMC runs using compiled C++ code, hence the calls to compileNimble to create compiled versions of the model and the MCMC algorithm.\nBecause the specification is in terms of a Chinese restaurant process, the default sampler selected by NIMBLE is a collapsed Gibbs sampler. More specifically, because the baseline distribution \\(H\\) is conjugate to the product of Poisson kernels, Algorithm 2 from Neal (2000) is used.\n\nset.seed(1)\ninits &lt;- list(xi = 1:n, alpha = 1,\n       lambda = matrix(rgamma(J*n, shape = 1, rate = 0.1), ncol = J, nrow = n))\nmodel &lt;- nimbleModel(code, data=data, inits = inits, constants = constants, dimensions = list(lambda = c(n, J)))\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\ncmodel &lt;- compileNimble(model)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\nconf &lt;- configureMCMC(model, monitors = c('xi','lambda', 'alpha'), print = TRUE)\n\n===== Monitors =====\nthin = 1: alpha, lambda, xi\n===== Samplers =====\nCRP_concentration sampler (1)\n  - alpha\nCRP_cluster_wrapper sampler (295)\n  - lambda[]  (295 elements)\nCRP sampler (1)\n  - xi[1:59] \n\nmcmc &lt;- buildMCMC(conf)\ncmcmc &lt;- compileNimble(mcmc, project = model)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\nsamples &lt;- runMCMC(cmcmc,  niter=55000, nburnin = 5000, thin=10)\n\nrunning chain 1...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\n\nWe can extract posterior samples for some parameters of interest. The following are trace plots of the posterior samples for the concentration parameter, \\(\\alpha\\) and the number of clusters.\n\nxiSamples &lt;- samples[, grep('xi', colnames(samples))]    # samples of cluster IDs\nnGroups &lt;- apply(xiSamples, 1, function(x)  length(unique(x)))\nconcSamples &lt;- samples[, grep('alpha', colnames(samples))]\n\npar(mfrow=c(1, 2))\nts.plot(concSamples, xlab = \"Iteration\", ylab = expression(alpha), main = expression(paste('Traceplot for ', alpha)))\nts.plot(nGroups,  xlab = \"Iteration\", ylab = \"Number of components\", main = \"Number of clusters\")\n\n\n\n\n\n\n\n\n\n\nAssessing the posterior\nWe can compute the posterior predictive distribution for a new observation \\(\\tilde{\\boldsymbol{y}}\\), \\(p(\\tilde{\\boldsymbol{y}}\\mid \\boldsymbol{y}_1, \\ldots, \\boldsymbol{y}_n)\\), which in turn allows us to obtain univariate or multivariate marginals or conditionals, or any other density estimate of interest. As an illustration, we compute the bivariate posterior predictive distribution for the number of seizures at baseline and at the 4th hospital visit. This is done in two steps. First, we compute posterior samples of the random measure \\(G\\), which can be done using the getSamplesDPmeasure() function. Based on the MCMC output, getSamplesDPmeasure() returns a list of matrices, each of them corresponding to a single posterior sample from \\(G\\), using its stick-breaking (SB) representation. The first column of each of these matrices contains the weights of the SB representation of \\(G\\) while the rest of the columns contain the atoms of the SB representation of \\(G\\), here \\((\\lambda_1, \\lambda_2, \\ldots, \\lambda_5)\\). Second, we compute the bivariate posterior predictive distribution of the seizure counts at baseline and at the fourth visit, based on the posterior samples of \\(G\\). We use a compiled nimble function, called ‘bivariate’, to speed up the computations of the bivariate posterior predictive density.\n\n# samples from the random measure\nsamplesG &lt;- getSamplesDPmeasure(cmcmc)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\nniter &lt;- length(samplesG)\nweightsIndex &lt;- grep('weights', colnames(samplesG[[1]]))\nlambdaIndex &lt;- grep('lambda', colnames(samplesG[[1]]))\nygrid &lt;- 0:45\n\n# function used to compute bivariate posterior predictive\nbivariateFun &lt;- nimbleFunction(\n  run = function(w = double(1),\n               lambda1 = double(1),\n               lambda5 = double(1),\n               ytilde = double(1)) {\n    returnType(double(2))\n\n    ngrid &lt;- length(ytilde)\n    out &lt;- matrix(0, ncol = ngrid, nrow = ngrid)\n\n    for(i in 1:ngrid) {\n      for(j in 1:ngrid) {\n        out[i, j] &lt;- sum(w * dpois(ytilde[i], lambda1) * dpois(ytilde[j], lambda5))\n      }\n    }\n\n    return(out)\n  }\n)\ncbivariateFun &lt;- compileNimble(bivariateFun)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n# computing bivariate posterior predictive of seizure counts are baseline and fourth visit\nbivariate &lt;- matrix(0, ncol = length(ygrid), nrow = length(ygrid))\nfor(iter in 1:niter) {\n  weights &lt;- samplesG[[iter]][, weightsIndex] # posterior weights\n  lambdaBaseline &lt;- samplesG[[iter]][, lambdaIndex[1]] # posterior rate of baseline\n  lambdaVisit4 &lt;- samplesG[[iter]][, lambdaIndex[5]] # posterior rate at fourth visit\n  bivariate &lt;- bivariate + cbivariateFun(weights, lambdaBaseline, lambdaVisit4, ygrid)\n}\nbivariate &lt;- bivariate / niter\n\nThe following code creates a heatmap of the posterior predictive bivariate distribution of the number of seizures at baseline and at the fourth hospital visit, showing that there is a positive correlation between these two measurements.\n\ncollist &lt;- colorRampPalette(c('white', 'grey', 'black'))\nfields::image.plot(ygrid, ygrid, bivariate, col = collist(6),\n           xlab = 'Baseline', ylab = '4th visit', ylim = c(0, 15), axes = TRUE)\n\n\n\n\n\n\n\n\nIn order to describe the uncertainty in the posterior clustering structure of the individuals in the study, we present a heat map of the posterior probability of two subjects belonging to the same cluster. To do this, first we compute the posterior pairwise clustering matrix that describes the probability of two individuals belonging to the same cluster, then we reorder the observations and finally plot the associated heatmap.\n\npairMatrix &lt;- apply(xiSamples, 2, function(focal) {\n                                   colSums(focal == xiSamples)\n                                  })\npairMatrix &lt;- pairMatrix / niter\n\n\nnewOrder &lt;- c(1, 35, 13, 16, 32, 33,  2, 29, 39, 26, 28, 52, 17, 15, 23,  8, 31,\n              38,  9, 46, 45, 11, 49, 44, 50, 41, 54, 21,  3, 40, 47, 48, 12,\n              6, 14,  7, 18, 22, 30, 55, 19, 34, 56, 57,  4,  5, 58, 10, 43, 25,\n              59, 20, 27, 24, 36, 37, 42, 51, 53)\n\nreordered_pairMatrix &lt;- pairMatrix[newOrder, newOrder]\nfields::image.plot(1:n, 1:n, reordered_pairMatrix , col = collist(6),\n           xlab = 'Patient', ylab = 'Patient',  axes = TRUE)\naxis(1, at = 1:n, labels = FALSE, tck = -.02)\naxis(2, at = 1:n, labels = FALSE, tck = -.02)\naxis(3, at = 1:n, tck = 0, labels = FALSE)\naxis(4, at = 1:n, tck = 0, labels = FALSE)"
  },
  {
    "objectID": "blog/bayesian-nonparametric-models-in-nimble-general-multivariate-models.html#references",
    "href": "blog/bayesian-nonparametric-models-in-nimble-general-multivariate-models.html#references",
    "title": "Bayesian Nonparametric Models in NIMBLE: General Multivariate Models",
    "section": "References",
    "text": "References\nInouye, D.I., E. Yang, G.I. Allen, and P. Ravikumar. 2017. A Review of Multivariate Distributions for Count Data Derived from the Poisson Distribution. Wiley Interdisciplinary Reviews: Computational Statistics 9: e1398.\nLeppik, I., F. Dreifuss, T. Bowman, N. Santilli, M. Jacobs, C. Crosby, J. Cloyd, et al. 1985. A Double-Blind Crossover Evaluation of Progabide in Partial Seizures: 3: 15 Pm8. Neurology 35.\nNeal, R. 2000. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics 9: 249–65."
  },
  {
    "objectID": "blog/version-0-6-6-of-nimble-released.html",
    "href": "blog/version-0-6-6-of-nimble-released.html",
    "title": "Version 0.6-6 of NIMBLE released!",
    "section": "",
    "text": "We’ve just released the newest version of NIMBLE on CRAN and on our website. Version 0.6-6 has some important new features, and more are on the way in the next few months.\nNew features include:\n\ndynamic indexes are now allowed in BUGS code — indexes of a variable no longer need to be constants but can be other nodes or functions of other nodes; for this release this is a beta feature that needs to be enabled with nimbleOptions(allowDynamicIndexing = TRUE);\nthe intrinsic Gaussian CAR (conditional autoregressive) model can now be used in BUGS code as dcar_normal, which behaves similarly to BUGS’ car.normal distribution;\noptim is now part of the NIMBLE language and can be used in nimbleFunctions;\nit is possible to call out to external compiled code or back to R functions from a nimbleFunction using nimbleExternalCall() and nimbleRcall() (this is an experimental feature);\nthe WAIC model selection criterion can be calculated using the calculateWAIC() method for MCMC objects;\nthe bootstrap and auxiliary particle filters can now return their ESS values;\nand a variety of bug fixes.\n\nPlease see the NEWS file in the installed package for more details.\nFinally, we’re deep in the midst of development work to enable automatic differentiation, Tensorflow as an alternative back-end computational engine, additional spatial models, and Bayesian nonparametrics."
  },
  {
    "objectID": "blog/variable-selection-in-nimble-using-reversible-jump-mcmc.html",
    "href": "blog/variable-selection-in-nimble-using-reversible-jump-mcmc.html",
    "title": "Variable selection in NIMBLE using reversible jump MCMC",
    "section": "",
    "text": "Reversible Jump MCMC (RJMCMC) is a general framework for MCMC simulation in which the dimension of the parameter space (i.e., the number of parameters) can vary between iterations of the Markov chain. It can be viewed as an extension of the Metropolis-Hastings algorithm onto more general state spaces. A common use case for RJMCMC is for variable selection in regression-style problems, where the dimension of the parameter space varies as variables are included or excluded from the regression specification.\nRecently we added an RJMCMC sampler for variable selection to NIMBLE, using a univariate normal distribution as proposal distribution. There are two ways to use RJMCMC variable selection in your model. If you know the prior probability for inclusion of a variable in the model, you can use that directly in the RJMCMC without modifying your model. If you need the prior probability for inclusion in the model to be a model node itself, such as if it will have a prior and be estimated, you will need to write the model with indicator variables. In this post we will illustrate the basic usage of NIMBLE RJMCMC in both situations.\nMore information can be found in the NIMBLE User Manual and via help(configureRJ)."
  },
  {
    "objectID": "blog/variable-selection-in-nimble-using-reversible-jump-mcmc.html#reversible-jump-mcmc-overview",
    "href": "blog/variable-selection-in-nimble-using-reversible-jump-mcmc.html#reversible-jump-mcmc-overview",
    "title": "Variable selection in NIMBLE using reversible jump MCMC",
    "section": "",
    "text": "Reversible Jump MCMC (RJMCMC) is a general framework for MCMC simulation in which the dimension of the parameter space (i.e., the number of parameters) can vary between iterations of the Markov chain. It can be viewed as an extension of the Metropolis-Hastings algorithm onto more general state spaces. A common use case for RJMCMC is for variable selection in regression-style problems, where the dimension of the parameter space varies as variables are included or excluded from the regression specification.\nRecently we added an RJMCMC sampler for variable selection to NIMBLE, using a univariate normal distribution as proposal distribution. There are two ways to use RJMCMC variable selection in your model. If you know the prior probability for inclusion of a variable in the model, you can use that directly in the RJMCMC without modifying your model. If you need the prior probability for inclusion in the model to be a model node itself, such as if it will have a prior and be estimated, you will need to write the model with indicator variables. In this post we will illustrate the basic usage of NIMBLE RJMCMC in both situations.\nMore information can be found in the NIMBLE User Manual and via help(configureRJ)."
  },
  {
    "objectID": "blog/variable-selection-in-nimble-using-reversible-jump-mcmc.html#linear-regression-example",
    "href": "blog/variable-selection-in-nimble-using-reversible-jump-mcmc.html#linear-regression-example",
    "title": "Variable selection in NIMBLE using reversible jump MCMC",
    "section": "Linear regression example",
    "text": "Linear regression example\nIn the following we consider a linear regression example in which we have 15 explanatory variables, and five of those are real effects while the others have no effect. First we simulate some data.\n\nN &lt;- 100\np &lt;- 15\nset.seed(1)\nX &lt;- matrix(rnorm(N*p), nrow = N, ncol = p)\ntrue_betas &lt;- c(c(0.1, 0.2, 0.3, 0.4, 0.5),\n                rep(0, 10))\n\ny &lt;- rnorm(N, X%*%true_betas, sd = 1)\n\nsummary(lm(y ~ X))\n\n\nCall:\nlm(formula = y ~ X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.64569 -0.50496  0.01613  0.59480  1.97766 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.04190    0.11511  -0.364   0.7168    \nX1           0.06574    0.12113   0.543   0.5888    \nX2           0.21162    0.11469   1.845   0.0686 .  \nX3           0.16606    0.10823   1.534   0.1287    \nX4           0.66582    0.11376   5.853 9.06e-08 ***\nX5           0.51343    0.09351   5.490 4.18e-07 ***\nX6           0.01506    0.11399   0.132   0.8952    \nX7          -0.12203    0.10127  -1.205   0.2316    \nX8           0.18177    0.10168   1.788   0.0774 .  \nX9          -0.09645    0.10617  -0.908   0.3663    \nX10          0.15986    0.11294   1.416   0.1606    \nX11          0.03806    0.10530   0.361   0.7186    \nX12          0.05354    0.10834   0.494   0.6225    \nX13         -0.02510    0.10516  -0.239   0.8119    \nX14         -0.07184    0.12842  -0.559   0.5774    \nX15         -0.04327    0.11236  -0.385   0.7011    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.035 on 84 degrees of freedom\nMultiple R-squared:  0.5337,    Adjusted R-squared:  0.4505 \nF-statistic:  6.41 on 15 and 84 DF,  p-value: 7.726e-09"
  },
  {
    "objectID": "blog/variable-selection-in-nimble-using-reversible-jump-mcmc.html#reversible-jump-with-indicator-variables",
    "href": "blog/variable-selection-in-nimble-using-reversible-jump-mcmc.html#reversible-jump-with-indicator-variables",
    "title": "Variable selection in NIMBLE using reversible jump MCMC",
    "section": "Reversible jump with indicator variables",
    "text": "Reversible jump with indicator variables\nNext we set up the model. In this case we explicitly include indicator variables that include or exclude the corresponding predictor variable. For this example we assume the indicator variables are exchangeable and we include the inclusion probability in the inference.\n\nlibrary(nimble)\n\n\nlmIndicatorCode &lt;- nimbleCode({\n  sigma ~ dunif(0, 20)  ## uniform prior per Gelman (2006)\n  psi ~ dunif(0,1)    ## prior on inclusion probability\n\n  for(i in 1:numVars) {\n    z[i] ~ dbern(psi) ## indicator variable for each coefficient\n    beta[i] ~ dnorm(0, sd = 100)\n    zbeta[i] &lt;- z[i] * beta[i]  ## indicator * beta\n  }\n  for(i in 1:N) {\n    pred.y[i] &lt;- inprod(X[i, 1:numVars], zbeta[1:numVars])\n    y[i] ~ dnorm(pred.y[i], sd = sigma)\n  }\n})\n\n## Set up the model.\nlmIndicatorConstants &lt;- list(N = 100, numVars = 15)\nlmIndicatorInits &lt;- list(sigma = 1, psi = 0.5,\n                         beta = rnorm(lmIndicatorConstants$numVars),\n                         z = sample(0:1, lmIndicatorConstants$numVars, 0.5))\n\nlmIndicatorData  &lt;- list(y = y, X = X)\nlmIndicatorModel &lt;- nimbleModel(code = lmIndicatorCode, constants = lmIndicatorConstants,\n                                    inits = lmIndicatorInits, data = lmIndicatorData)\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\nThe above model code can potentially be used to set up variable selection in NIMBLE without using RJMCMC, since the indicator variables can turn the regression parameters off and on. However, in that case the MCMC sampling can be inefficient because a given regression parameter can wander widely in the parameter space when the corresponding variable is not in the model. This can make it difficult for the variable to be put back into the model, unless the prior for that variable is (perhaps artificially) made somewhat informative. Configuring RJMCMC sampling via our NIMBLE function configureRJ results in the MCMC not sampling the regression coefficients for variables for iterations where the variables are not in the model.\n\nConfiguring RJMCMC\nThe RJMCMC sampler can be added to the MCMC configuration by calling the function configureRJ(). In the example considered we introduced z as indicator variables associated with the regression coefficients beta. We can pass these, respectively, to configureRJ using the arguments indicatorNodes and targetNodes. The control arguments allow one to specify the mean and the scale of the normal proposal distribution used when proposing to put a coefficient back into the model.\n\nlmIndicatorConf &lt;- configureMCMC(lmIndicatorModel)\n\n===== Monitors =====\nthin = 1: beta, psi, sigma\n===== Samplers =====\nRW sampler (2)\n  - sigma\n  - psi\nconjugate sampler (15)\n  - beta[]  (15 elements)\nbinary sampler (15)\n  - z[]  (15 elements)\n\nlmIndicatorConf$addMonitors('z')\n\nthin = 1: beta, psi, sigma, z\n\nconfigureRJ(lmIndicatorConf,\n            targetNodes = 'beta',\n            indicatorNodes = 'z',\n            control = list(mean = 0, scale = .2))\n\nChecking the assigned samplers we see that the indicator variables are each assigned an RJ_indicator sampler whose targetNode is the corresponding coefficient, while the beta parameters have a RJ_toggled sampler. The latter sampler is a modified version of the original sampler to the targetNode that is invoked only when the variable is currently in the model.\n\n## Check the assigned samplers\nlmIndicatorConf$printSamplers(c(\"z[1]\", \"beta[1]\"))\n\n[4] RJ_toggled sampler: beta[1],  samplerType: conjugate_dnorm_dnorm_linear\n[3] RJ_indicator sampler: z[1],  mean: 0,  scale: 0.20000000000000001,  targetNode: beta[1]\n\n### Build and run the RJMCMC\nmcmcIndicatorRJ &lt;- buildMCMC(lmIndicatorConf)\ncIndicatorModel &lt;- compileNimble(lmIndicatorModel)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\nCMCMCIndicatorRJ &lt;- compileNimble(mcmcIndicatorRJ, project = lmIndicatorModel)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\nset.seed(1)\nsystem.time(samplesIndicator &lt;- runMCMC(CMCMCIndicatorRJ, niter = 10000, nburnin = 1000))\n\nrunning chain 1...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\n\n   user  system elapsed \n  1.900   0.005   1.904 \n\n\n\n\nLooking at the results\nWe can look at the sampled values of the indicator and corresponding coefficient for some of the variables.\n\npar(mfrow = c(2, 2))\nplot(samplesIndicator[,'beta[3]'], pch = 16, cex = 0.4, main = \"beta[3] traceplot\")\nplot(samplesIndicator[,'z[3]'], pch = 16, cex = 0.4, main = \"z[3] traceplot\")\nplot(samplesIndicator[,'beta[5]'], pch = 16, cex = 0.4, main = \"beta[5] traceplot\")\nplot(samplesIndicator[,'z[5]'], pch = 16, cex = 0.4, main = \"z[5] traceplot\")\n\n\n\n\n\n\n\n\n\n\nIndividual inclusion probabilities\nNow let’s look at the inference on the variable selection problem. We see that the fourth and fifth predictors are almost always included (these are the ones with the largest true coefficient values), while the others, including some variables that are truly associated with the outcome but have smaller true coefficient values, are almost never included.\n\npar(mfrow = c(1, 1))\nzCols &lt;- grep(\"z\\\\[\", colnames(samplesIndicator))\nposterior_inclusion_prob &lt;- colMeans(samplesIndicator[, zCols])\nplot(1:length(true_betas), posterior_inclusion_prob,\n        xlab = \"beta\", ylab = \"inclusion probability\",\n        main = \"Inclusion probabilities for each beta\")"
  },
  {
    "objectID": "blog/variable-selection-in-nimble-using-reversible-jump-mcmc.html#reversible-jump-without-indicator-variables",
    "href": "blog/variable-selection-in-nimble-using-reversible-jump-mcmc.html#reversible-jump-without-indicator-variables",
    "title": "Variable selection in NIMBLE using reversible jump MCMC",
    "section": "Reversible jump without indicator variables",
    "text": "Reversible jump without indicator variables\nIf we assume that the inclusion probabilities for the coefficients are known, we can use the RJMCMC with model code written without indicator variables.\n\nlmNoIndicatorCode &lt;- nimbleCode({\n  sigma ~ dunif(0, 20)\n\n  for(i in 1:numVars) {\n    beta[i] ~ dnorm(0, sd = 100)\n  }\n  for(i in 1:N) {\n    pred.y[i] &lt;- inprod(X[i, 1:numVars], beta[1:numVars])\n    y[i] ~ dnorm(pred.y[i], sd = sigma)\n  }\n})\n\n## Define model constants, inits and data\nlmNoIndicatorConstants &lt;- list(N = 100, numVars = 15)\nlmNoIndicatorInits &lt;- list(sigma = 1,\n                           beta = rnorm(lmNoIndicatorConstants$numVars))\nlmNoIndicatorData  &lt;- list(y = y, X = X)\n\nlmNoIndicatorModel &lt;- nimbleModel(code = lmNoIndicatorCode,\n                                  constants = lmNoIndicatorConstants,\n                                  inits = lmNoIndicatorInits,\n                                  data = lmNoIndicatorData)\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\n\nConfiguring RJMCMC with no indicator variables\nAgain, the RJMCMC sampler can be added to the MCMC configuration by calling the function configureRJ() for nodes specified in targetNodes, but since there are no indicator variables we need to provide the prior inclusion probabilities. We use the priorProb argument, and we can provide either a vector of values or a common value.\n\nlmNoIndicatorConf &lt;- configureMCMC(lmNoIndicatorModel)\n\n===== Monitors =====\nthin = 1: beta, sigma\n===== Samplers =====\nRW sampler (1)\n  - sigma\nconjugate sampler (15)\n  - beta[]  (15 elements)\n\nconfigureRJ(lmNoIndicatorConf,\n            targetNodes = 'beta',\n            priorProb = 0.5,\n            control = list(mean = 0, scale = .2))\n\nSince there are no indicator variables in this case, a RJ_fixed_prior sampler is assigned directly to each of coefficents along with the RJ_toggled sampler, which still uses the default sampler for the node, but only if the corresponding variable is in the model at a given iteration. In addition in this case one can set the coefficient to a value different from zero via the fixedValue argument in the control list.\n\n## Check the assigned samplers\nlmNoIndicatorConf$printSamplers(c(\"beta[1]\"))\n\n[2] RJ_fixed_prior sampler: beta[1],  priorProb: 0.5,  mean: 0,  scale: 0.20000000000000001,  fixedValue: 0\n[3] RJ_toggled sampler: beta[1],  samplerType: conjugate_dnorm_dnorm_linear,  fixedValue: 0\n\nmcmcNoIndicatorRJ &lt;- buildMCMC(lmNoIndicatorConf)\ncNoIndicatorModel &lt;- compileNimble(lmNoIndicatorModel)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\nCMCMCNoIndicatorRJ &lt;- compileNimble(mcmcNoIndicatorRJ, project = lmNoIndicatorModel)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\nset.seed(100)\nsystem.time(samplesNoIndicator &lt;- runMCMC(CMCMCNoIndicatorRJ, niter = 10000, nburnin = 1000))\n\nrunning chain 1...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\n\n   user  system elapsed \n  1.459   0.000   1.460 \n\n\n\n\nLooking at the results\nIn this case we just look at one of the model coefficients.\n\nplot(samplesNoIndicator[,'beta[1]'], pch = 16, cex = 0.4, main = \"beta[1] traceplot\")\n\n\n\n\n\n\n\n\n\n\nIndividual inclusion proportion\nWe can calculate the proportion of times each coefficient is included in the model.\n\nbetaCols &lt;- grep(\"beta\\\\[\", colnames(samplesNoIndicator))\nposterior_inclusion_proportions &lt;- colMeans(apply(samplesNoIndicator[, betaCols],\n                                                      2, function(x) x != 0))\nposterior_inclusion_proportions\n\n     beta[1]      beta[2]      beta[3]      beta[4]      beta[5]      beta[6] \n0.0017777778 0.0097777778 0.0017777778 1.0000000000 0.9996666667 0.0015555556 \n     beta[7]      beta[8]      beta[9]     beta[10]     beta[11]     beta[12] \n0.0015555556 0.0031111111 0.0018888889 0.0028888889 0.0006666667 0.0015555556 \n    beta[13]     beta[14]     beta[15] \n0.0007777778 0.0024444444 0.0007777778"
  },
  {
    "objectID": "blog/version-0-6-9-of-nimble-released.html",
    "href": "blog/version-0-6-9-of-nimble-released.html",
    "title": "Version 0.6-9 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.6-9 is primarily a maintenance release with various bug fixes and fixes for CRAN packaging issues.\nNew features include:\n\ndimensions in a model will now be determined from either inits or data if not otherwise available;\none can now specify nBootReps = NA in the runCrossValidate() function, which will prevent the Monte Carlo error from being calculated;\nrunCrossValidate() now returns the averaged loss over all k folds, instead of the summed loss;\nWe’ve added the besselK function to the NIMBLE language;\nand a variety of bug fixes.\n\nPlease see the NEWS file in the installed package for more details"
  },
  {
    "objectID": "release-notes.html",
    "href": "release-notes.html",
    "title": "Release notes",
    "section": "",
    "text": "December 19, 2024: We’re released Version 1.3.0\nWe’ve released the newest version of NIMBLE on CRAN and on our website.\nVersion 1.3.0 provides some new and improved functionality, plus some bug fixes and improved error trapping.\nThe new and improved functionality includes:\n\nA new multivariate sampler, the Barker proposal sampler (sampler_barker). We encourage users to try this sampler in place of the block Metropolis RW_block sampler and let us know how well it works. The Barker sampler uses gradient information and may improve adaptation behavior, including better mixing when parameters are on different scales or the initial proposal scale is too large.\nAn improved Laplace/AGHQ implementation that includes use of the nlminb optimizer for both inner and outer optimization (for better optimization performance), improved messaging and output naming, returning the log-likelihood and degrees of freedom for model selection calculations, and unified control of optimization method and other controls at either the build stage or through the updateSettings method.\nThe addition of the BOBYQA optimization method through nimOptim, registered via nimOptimMethod.\n\nIn addition to the new and improved functionality above, other bug fixes, improved error trapping, and enhancements include:\n\nPreventing the use of nimbleFunction method names and nimbleFunction names that conflict with names in the nimble language (DSL).\nMore carefully checking for and warning of cases of NaN and non-finite log probability values in various samplers that in some cases may indicate invalid MCMC sampling.\nMore carefully handling of NaN and non-finite log probability values in the CRP sampler.\nError trapping cases of dynamic indices producing a non-scalar result in AD-enabled models, and provide a suggested work-around.\nError trapping use of non-existent nimbleList.\nPreventing use of a single seed when running multiple chains via runMCMC.\nImproving messaging related to lack of derivative support for functions.\nAdding information about model macros to the manual.\nFixing bug in caching values in the CRP sampler when maximum number of clusters is exceeded, which would have caused incorrect sampling (albeit with the user having been warned that they should increase the maximum number of clusters).\nFixing an issue preventing use of nimbleList elements in nimCat.\nPreventing an adaptation interval of one for various block samplers for which an interval of one leads to an error.\nAllowing runLaplace to use an uncompiled Laplace object.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nJuly 31, 2024: We’ve released Version 1.2.1\nWe’ve released the newest version of NIMBLE on CRAN and on our website. This is a micro release that primarily addresses some packaging changes requested by CRAN. In addition this release includes:\n\nA multinomial MCMC sampler, sampler_RW_multinomial, for random variables following a multinomial distribution.\nSome enhancements to error trapping and warning messages.\nA variety of minor bug fixes.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nJune 9, 2024: We’ve released Version 1.2.0\nWe’ve released the newest version of NIMBLE on CRAN and on our website.\nVersion 1.2.0 provides extensive new functionality, including:\n\nA Pólya-gamma sampler, sampler_polyagamma, for conjugate sampling of linear predictor parameters in logistic regression model specifications, including handling zero inflation and stochastic design matrices.\nA new sampler, sampler_noncentered, which samples the mean or standard deviation of a set of random effect values in a transformed space such that the random effects are deterministically shifted or scaled given new values of their hyperparameters. For random effects written in a centered parameterization, sampling is performed as if they had been written in a noncentered parameterization, thereby enabling a variant on the Yu and Meng (2011) interweaving sampling strategy of sampling in both parameterizations.\nA completely revamped MCEM algorithm, fixing a bug so that any parts of the model not connected to the latent states are included in MLE calculations, giving greater control and adding minor extensions to the ascent-based MCEM approach, using automatic derivatives in the maximization when possible, and converting buildMCEM to be a nimbleFunction rather than an R function.\nAdaptive Gauss-Hermite quadrature (AGHQ) for integrating over latent effects, as an extension of NIMBLE’s Laplace approximation functionality. Also adds user-friendly R functions, runLaplace and runAGHQ, for using Laplace and AGHQ approximation for maximum likelihood estimation.\nA more flexible optimization system via nimOptim, with support for nlminb built in as well as the capability for users to provide potentially arbitrary optimization functions in R.\nAllowing the use of nimbleFunctions with setup code in models either for user-defined functions via &lt;- or for user-defined distributions via ~. This supports holding large objects outside of model nodes for use in models.\n\nIn addition to the new functionality above, other enhancements and bug fixes include:\n\nSome internal improvements to increase speed of model and MCMC building in certain cases.\nSome fixes and improvements in error-trapping and warnings.\nAdding an argument to buildMCMC controlling whether to initialize values in the model.\nProviding ability to control number of digits printed in C++ output.\nAllowing use of categorical MCMC sampler with user-specified dcat-like distributions.\nImprove documentation of LKJ distribution and derivative tracking in the AD system.\nFixing an insufficient check for conjugacy in stickbreaking specifications.\nFixing compilation failures occurring on Red Hat Linux.\nReenabling functionality for user-provided Eigen library and related updates to autoconf configuration.\nEnhancing functionality to support model macros.\nRemoving deprecated is.na.vec and is.nan.vec.\nRemoving deprecated dummy functions for compareMCMCs functions.\n\n\n\nJanuary 31, 2024: We’ve released Version 1.1.0\nWe’ve released the newest version of NIMBLE on CRAN and on our website. This version provides new functionality and a variety of bug fixes.\n\nImproving our automatic differentiation (AD) system so it can be used in a wider range of models, including models with stochastic indexing, discrete latent states, and CAR distributions. Support for AD for these models means that HMC sampling and Laplace approximation can be used.\nAllowing distributions and functions (whether user-defined or built-in) that lack AD support (such as dinterval, dconstraint, and truncated distributions) to be used and compiled in AD-enabled models. The added flexibility increases the range of models in which one can use AD methods (HMC or Laplace) on some parts of a model and other samplers or methods on other parts.\nAdding nimIntegrate to the NIMBLE language, providing one-dimensional numerical integration via adaptive quadrature, equivalent to R’s integrate. This can, for example, be used in a user-defined function or distribution for use in model code, such as to implement certain point process or survival models that involve a one-dimensional integral.\nAdding a “prior samples” MCMC sampler, which uses an existing set of numerical samples to define the prior distribution of model node(s).\nBetter support of the dCRP distribution in non-standard model structures.\nAdding error trapping to prevent accidental use of C++ keywords as model variable names.\nRemoving the RW_multinomial MCMC sampler, which was found to generate incorrect posterior results (in cases when a latent state followed a multinomial distribution)\nFixing a bug in conjugacy checking in a case of subsets of multivariate nodes.\nFixing is.na and is.nan to operate in the expected vectorized fashion.\nImproving documentation of AD, nimbleHMC, and nimbleSMC in the manual.\nUpdating Eigen (the C++ linear algebra library used by nimble) to version 3.4.0.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nJune 15, 2023: We’ve released Version 1.0.1\nWe’ve released the newest version of NIMBLE on CRAN and on our website. Version 1.0.1 follows shortly after 1.0.0 and fixes an issue and a bug introduced in version 1.0.0 causing data to be set incorrectly in certain models.\nBoth cases occur only when a variable (e.g., “x”) contains both stochastic nodes (e.g. “x[2] ~ ”) and either deterministic nodes (e.g. “x[3] &lt;- ”) or right-hand-side-only nodes (e.g. “x[4]” appears only on the right-hand-side, like an explanatory value).\nThe issue involves a change of behavior (relative to previous nimble versions) when both setting data values for some nodes and initial values for other nodes within the same variable (that satisfies the previous condition). Data values for right-hand-side-only nodes were replaced by initial values (inits) if both were provided. Version 1.0.1 reverts to previous behavior that data values are not replaced by initial values in that situation.\nThe bug involves models where (for a variable satisfying the previous condition) not every scalar element within the variable is used as a node and some of the nodes in the variable are data. In that situation, data values may be set incorrectly. This could typically occur in models with autoregressive structure directly on some data nodes (such as may be the case for capture-recapture models involving many individual capture histories within the same variable, indexed by individual and time, with some individuals not present for the entire time series, resulting in unused scalar elements of the variable).\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nMay 26, 2023: We’ve released Version 1.0.0\nWe’re very pleased to announce the release of version 1.0.0 of NIMBLE. This version provides substantial new functionality. This includes:\n\nA Laplace approximation algorithm that allows one to find the MLE for model parameters based on approximating the marginal likelihood in models with continuous random effects/latent process values.\nA Hamiltonian Monte Carlo (HMC) MCMC sampler implementing the NUTS algorithm (available in the nimbleHMC package).\nSupport in NIMBLE’s algorithm programming system to obtain derivatives of functions and arbitrary calculations within models.\nA parameter transformation system allowing algorithms to work in unconstrained parameter spaces when model parameters have constrained domains.\n\nThese are documented via the R help system and a new section at the end of our User Manual. We’re excited for users to try out the new features and let us know of their experiences. In particular, given these major additions to the NIMBLE system, we anticipate the possibility of minor glitches. The best place to reach out for support is still the nimble-users list.\nIn addition to the new functionality above, other enhancements and bug fixes include:\n\nFixing a bug (previously reported in a nimble-users message) giving incorrect results in NIMBLE’s cross-validation function (runCrossValidate) for all but the ‘predictive’ loss function for NIMBLE versions 0.10.0 – 0.13.2.\nFixing a bug in conjugacy checking causing incorrect identification of conjugate relationships in models with unusual uses of subsets, supersets, and slices of multivariate normal nodes.\nImproving control of the addSampler method for MCMC.\nImproving the WAIC system in a few small ways.\nEnhancing error trapping and warning messages.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nMay 15, 2023: We’ve released Version 0.13.2\nWe’ve released the newest version of NIMBLE on CRAN. This version exists solely to address some developer-level issues required by CRAN and is identical to version 0.13.2 from a user perspective.\n\n\nDecember 13, 2022: We’ve released Version 0.13.1\nWe’ve released the newest version of NIMBLE on CRAN and on our website. This is purely a bug fix release that fixes a bug in our new handling of predictive nodes introduced in the recently released Version 0.13.0. The bug could affect MCMC sampling in models that both (1) have predictive nodes and (2) have multivariate nodes.\n\n\nNovember 15, 2022: We’ve released Version 0.13.0\nWe’ve released the newest version of NIMBLE on CRAN and on our website. The main highlight of this version is a major change to how predictive nodes are handled in MCMC sampling that should improve mixing for models with predictive nodes. The changes in version 0.13.0 include:\n\nThoroughly revamping handling of posterior predictive nodes in the MCMC system, in particular that MCMC samplers, by default, will now exclude predictive dependencies from internal sampler calculations. This should improve MCMC mixing for models with predictive nodes. Posterior predictive nodes are now sampled conditional on all other model nodes at the end of each MCMC iteration.\nAdding functionality to the MCMC configuration system, including a new replaceSamplers method and arguments default and nodes for the addSamplers method.\nAdding an option to the WAIC system to allow additional burnin (in addition to standard MCMC burnin) before calculating online WAIC, thereby allowing inspection of initial samples without forcing them to be used for WAIC\nWarning users of unused constants during model building.\nFixing bugs that prevented use of variables starting with ‘logProb’ or named ‘i’ in model code.\nFixing a bug to prevent infinite recursion in particular cases in conjugacy checking.\nFixing a bug in simulating from dcar_normal nodes when multiple nodes passed to simulate.\n\n\n\nFebruary 24, 2022: We’ve released Version 0.12.2, which includes an important bug fix for some models using Bayesian nonparametrics with the dCRP distribution\nWe’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.12.2 is a bug fix release. In particular, this release fixes a bug in our Bayesian nonparametric distribution (BNP) functionality that gives incorrect MCMC results when using the dCRP distribution when the parameters of the mixture components (i.e., the clusters) have hyperparameters (i.e., the base measure parameters) that are unknown and sampled during the MCMC. Here is an example basic model structure that is affected by the bug:\nk[1:n] ~ dCRP(alpha, n)\nfor(i in 1:n) {\n  y[i] ~ dnorm(mu[k[i]], 1)\n  mu[i] ~ dnorm(mu0, 1)    ## mixture component parameters with hyperparameter\n}\nmu0 ~ dnorm(0, 1)    ## unknown cluster hyperparameter\n(There is no problem without the hyperparameter layer – i.e., if mu0 is a fixed value – which is the situation in many models.) We strongly encourage users using models with this structure to rerun their analyses.\nOther changes in this release include:\n\nFixing an issue with reversible jump variable selection under a similar situation to the BNP issue discussed above (in particular where there are unknown hyperparameters of the regression coefficients being considered, which would likely be an unusual use case).\nFixing a bug preventing setup of conjugate samplers for dwishart or dinvwishart nodes when using dynamic indexing.\nFixing a bug preventing use of truncation bounds specified via data or constants.\nFixing a bug preventing MCMC sampling with the LKJ prior for 2×2 matrices.\nFixing a bug in runCrossValidate affecting extraction of multivariate nodes.\nFixing a bug producing incorrect subset assignment into logical vectors in nimbleFunction code.\nFixing a bug preventing use of nimbleExternalCall with a constant expression.\nFixing a bug preventing use of recursion in nimbleFunctions without setup code.\nFixing handling nimSeq default by value.\nFixing access to member data more than two dimensions in a nested nimbleFunction.\n\nPlease see the NEWS file in the source package or the nimble-dev Github site for more detailed information.\n\n\nOctober 12, 2021: We’ve released Version 0.12.1 (including changes in Version 0.12.0)\nWe’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.12.1, in combination with version 0.12.0 (which was released just last week), provides a variety of new functionality (in particular enhanced WAIC functionality and adding the LKJ distribution) plus bug fixes affecting MCMC in specific narrow cases described below and that warrant upgrading for some users. The changes include:\n\nCompletely revamping WAIC in NIMBLE, creating an online version that does not require any particular variable monitors. The new WAIC can calculate conditional or marginal WAIC and can group data nodes into joint likelihood terms if desired. In addition there is a new calculateWAIC() function that will calculate the basic conditional WAIC from MCMC output without having to enable the WAIC when creating the MCMC.\nAdding the LKJ distribution, useful for prior distributions for correlation matrices, along with random walk samplers for them. These samplers operate in an unconstrained transformed parameter space and are assigned by default during MCMC configuration.\nFixing a bug introduced in conjugacy processing in version 0.11.0 that causes incorrect MCMC sampling only in specific cases. The impacted cases have terms of the form “a[i] + x[i] * beta” (or more simply “x[i] * beta”), with beta subject to conjugate sampling and either (i) ‘x’ provided via NIMBLE’s constants argument and x[1] == 1 or (ii) ‘a’ provided via NIMBLE’s constants argument and a[1] == 0.\nFixing an error in the sampler for the proper CAR distribution (dcar_proper) that gives incorrect MCMC results when the mean of the proper CAR is not the same value for all locations, e.g., when embedding covariate effects directly in the mu parameter of the dcar_proper distribution.\nFixing isData(‘y’) to return TRUE whenever any elements of a multivariate data node (‘y’) are flagged as data. As a result, attempting to carry out MCMC on the non-data elements will now fail. Formerly if only some elements were flagged as data, isData would only check the first element, potentially leading to other elements that were flagged as data being overwritten.\nError trapping cases where a BNP model has a differing number of dependent stochastic nodes (e.g., observations) or dependent deterministic nodes per group of elements clustered jointly (using functionality introduced in version 0.10.0). Previously we were not error trapping this, and incorrect MCMC results would be obtained.\nImproving the formatting of standard logging messages.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nMay 24, 2021: We’ve released Version 0.11.1\nWe’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.11.1 is primarily a bug fix release that fixes a bug that was introduced in Version 0.11.0 (which was released on April 17, 2021) that affected MCMC sampling in MCMCs using the “posterior_predictive_branch” sampler introduced in version 0.11.0. This sampler would be listed by name when the MCMC configuration object is created and would be assigned to any set of multiple nodes that (as a group of nodes) have no data dependencies and are therefore sampled as a group from their predictive distributions.\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nApril 17, 2021: We’ve released Version 0.11.0\nWe’ve released the newest version of NIMBLE on CRAN and on our website.\nWe’ve released version 0.11.0. Version 0.11.0 provides a variety of new functionality, improved error trapping, and bug fixes, including:\n\nadded the ‘posterior_predictive_branch’ MCMC sampler, which samples jointly from the predictive distribution of networks of entirely non-data nodes, to improve MCMC mixing,\nadded a model method to find parent nodes, getParents(), analogous to getDependencies(),\nimproved efficiency of conjugate samplers,\nallowed use of the elliptical slice sampler for univariate nodes, which can be useful for posteriors with multiple modes,\nallowed model definition using if-then-else without an else clause, and\nfixed a bug giving incorrect node names and potentially affecting algorithm behavior for models with more than 100,000 elements in a vector node or in any dimension of a multi-dimensional node.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nNovember 30, 2020: We’ve released Version 0.10.1\nWe’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.10.1 is primarily a bug fix release:\n\nIn particular, it fixes a bug in retrieving parameter values from distributions that was introduced in version 0.10.0. The bug can cause incorrect behavior of conjugate MCMC samplers under certain model structures (such as particular state-space models), so we strongly encourage users to upgrade to 0.10.1.\nIn addition, version 0.10.1 restricts use of WAIC to the conditional version of WAIC (conditioning on all parameters directly involved in the likelihood). Previous versions of nimble gave incorrect results when not conditioning on all parameters directly involved in the likelihood (i.e., when not monitoring all such parameters). In a future version of nimble we plan to make a number of improvements to WAIC, including allowing use of marginal versions of WAIC, where the WAIC calculation integrates over random effects.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nOctober 12, 2020: We’ve released Version 0.10.0\nWe’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.10.0 provides new features, improvements in speed of building models and algorithms, bug fixes, and various improvements.\nNew features and bug fixes include:\n\ngreatly extended NIMBLE’s CRP-based BNP functionality by allowing multiple observations to be grouped together;\nfixed a bug giving incorrect results in our cross-validation function, runCrossValidate();\nmoved NIMBLE’s sequential Monte Carlo (SMC, aka particle filtering) methods into the nimbleSMC package; and\nimproved the efficiency of model and MCMC building and compilation.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nMay 22, 2020: We’ve released Version 0.9.1\nWe’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.9.1 is primarily a bug fix release, but also provides some minor improvements in functionality.\nUsers of NIMBLE in R 4.0 on Windows MUST upgrade to this release for NIMBLE to work.\nNew features and bug fixes include:\n\nswitched to use of system2() from system() to avoid an issue on Windows in R 4.0;\nmodified various adaptive MCMC samplers so the exponent controlling the scale decay of the adaptation is adjustable by user;\nallowed pmin() and pmax() to be used in models;\nimproved handling of NA values in the dCRP distribution; and\nimproved handling of cases where indexing goes beyond the extent of a variable in expandNodeNames() and related queries of model structure.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nDecember 20, 2019: We’ve released Version 0.9.0\nWe’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.9.0 provides some new features as well as providing a variety of speed improvements, better output handling, and bug fixes.\nNew features and bug fixes include:\n\nadded an iterated filtering 2 (IF2) algorithm (a new sequential Monte Carlo (SMC) method) for parameter estimation via maximum likelihood;\nfixed several bugs in our SMC algorithms;\nimproved the speed of MCMC configuration;\nimproved the user interface for interacting with the MCMC configuration; and\nimproved our conjugacy checking system to detect some additional cases of conjugacy.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nJune 3, 2019: We’ve released Version 0.8.0\nWe’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.8.0 provides some new features as well as providing a variety of speed improvements, better errors/warnings and bug fixes.\nNew features and bug fixes include:\n\nadded a reversible jump MCMC sampler for variable selection via configureRJ();\ngreatly improved the speed of MCMC sampling for Bayesian nonparametric models with a dCRP distribution by not sampling parameters for empty clusters;\nadded experimental faster MCMC configuration, available by setting nimbleOptions(oldConjugacyChecking = FALSE) and nimbleOptions(useNewConfigureMCMC = TRUE);\nremoved compareMCMCs() and MCMCsuite(), which will be provided in a separate package — see https://github.com/nimble-dev/compareMCMCs; and\nimproved warning and error messages for MCEM and slice sampling.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nMarch 12, 2019: We’ve released Version 0.7.1\nWe’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.7.1 is primarily a maintenance release with a couple important bug fixes, but also provides a bit of additional functionality.\nNew features and bug fixes include:\n\nfixing a bug in MCMC sampling of dCRP nodes in non-conjugate situations that was introduced in Version 0.7.0;\navoiding a protection stack overflow in working with large models, also introduced in Version 0.7.0;\nsupport for 6-dimensional arrays in model code and nimbleFunctions;\nrecognition of normal-normal conjugacy in multivariate regression structures.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nFebruary 5, 2019: We’ve released Version 0.7.0 (essentially the same as Version 0.6.13)\nWe’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.7.0 is a slight (and rapid) update to Version 0.6.13. Together they provide a variety of new features, as well as various bug fixes.\nNew features and bug fixes include:\n\ngreatly improved efficiency of sampling for Bayesian nonparametric (BNP) mixture models that use the dCRP (Chinese Restaurant process) distribution;\naddition of the double exponential (Laplace) distribution for use in models and nimbleFunctions;\na new “RW_wishart” MCMC sampler, for sampling non-conjugate Wishart and inverse-Wishart nodes;\nhandling of the normal-inverse gamma conjugacy for BNP mixture models using the dCRP distribution;\nenhanced functionality of the getSamplesDPmeasure function for posterior sampling from BNP random measures with Dirichlet process priors.\nhandling of five-dimensional arrays in models;\nfixing a bug producing incorrect WAIC calculations when using multiple chains for models with at least one non-scalar monitored variable;\nfixing a bug in conjugate samplers for CRP distribution: CRP_conjugate_dgamma_dnorm, CRP_conjugate_dbeta_dbin, CRP_conjugate_dbeta_dnegbin, CRP_conjugate_dgamma_dinvgamma, CRP_conjugate_ddirch_dmulti;\nenhanced warning messages; and\nan HTML version of the NIMBLE manual.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nJune 27, 2018: We’ve released Version 0.6-12\nWe’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.6-12 is primarily a maintenance release with a variety of bug fixes.\nChanges include:\n\nfix the bootstrap particle filter to correctly calculate weights when particles are not resampled (the filter had been omitting the previous weights when calculating the new weights);\nadd an option to print MCMC samplers of a particular type;\navoid an overly-aggressive check for ragged arrays when building models;\navoid assigning a sampler to non-conjugacy inverse-Wishart nodes (thereby matching our handling of Wishart nodes);\nand a variety of bug fixes and internal changes.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nJune 14, 2018: We’ve released Version 0.6-11\nWe’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.6-11 has new features, notably support for Bayesian nonparametric mixture modeling, as well as a variety of bug fixes.\nChanges include:\n\nsupport for Bayesian nonparametric mixture modeling using Dirichlet process mixtures, with specialized MCMC samplers automatically assigned in NIMBLE’s default MCMC (See Chapter 10 of the manual for details);\nadditional resampling methods available with the auxiliary and bootstrap particle filters;\nuser-defined filtering algorithms can be used with NIMBLE’s particle MCMC samplers;\nMCMC thinning intervals can be modified at MCMC run-time;\nboth runMCMC() and nimbleMCMC() now drop burn-in samples before thinning, making their behavior consistent with each other;\nincreased functionality for the ‘setSeed’ argument in nimbleMCMC() and runMCMC();\nnew functionality for specifying the order in which sampler functions are executed in an MCMC;\ninvalid dynamic indexes now result in a warning and NaN values but do not cause execution to error out, allowing MCMC sampling to continue; and\nand a variety of bug fixes and improved error trapping/messages.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nMarch 26, 2018: We’ve released Version 0.6-10\nWe’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.6-10 is primarily a maintenance release with various bug fixes and internal changes to speed up model and algorithm building and compilation.\nChanges include:\n\nsome steps of model and algorithm building and compilation are faster;\ncompiled execution with multivariate distributions or function arguments may be faster;\ndata can now be provided as a numeric data frame rather than a matrix;\nto run WAIC, a user now must set ‘enableWAIC’ to TRUE, either in NIMBLE’s options or as an argument to buildMCMC();\nif ‘enableWAIC’ is TRUE, buildMCMC() will now check to make sure that the nodes monitored by the MCMC algorithm will lead to a valid WAIC calculation; and\nthe use of identityMatrix() is deprecated in favor of diag().\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nFebruary 9, 2018: We’ve released Version 0.6-9\nWe’ve released the newest version of NIMBLE on CRAN and on our website. Version 0.6-9 is primarily a maintenance release with various bug fixes and fixes for CRAN packaging issues.\nNew features include:\n\ndimensions in a model will now be determined from either ‘inits’ or ‘data’ if not otherwise available;\none can now specify “nBootReps = NA” in the runCrossValidate() function, which will prevent the Monte Carlo error from being calculated;\nrunCrossValidate() now returns the averaged loss over all k folds, instead of the summed loss;\nwe’ve added the besselK function to the NIMBLE language; and\nand a variety of bug fixes.\n\nPlease see the NEWS file in the installed package for more details\n\n\nNovember 24, 2017: We’ve released Version 0.6-8\n(Note that version 0.6-7 was the version on CRAN for about a week. Version 0.6-8 is essentially identical to version 0.6-7 but contains a minor fix to resolve a CRAN packaging issue.)\nVersion 0.6-8 has a few new features.\nChanges as of Version 0.6-8 include:\n\nthe proper Gaussian CAR (conditional autoregressive) model can now be used in BUGS code as dcar_proper, which behaves similarly to BUGS’ car.proper distribution;\na new nimbleMCMC function that provides one-line invocation of NIMBLE’s MCMC engine, akin to usage of JAGS and WinBUGS through R;\na new runCrossValidate function that will conduct k-fold cross-validation of NIMBLE models fit by MCMC;\ndynamic indexing in BUGS code is now allowed by default;\nand a variety of bug fixes.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nJuly 29, 2017: We’ve released Version 0.6-6\nVersion 0.6-6 has many new features.\nChanges as of Version 0.6-6 include:\n\ndynamic indexes are now allowed in BUGS code — indexes of a variable no longer need to be constants but can be other nodes or functions of other nodes; for this release this is a beta feature that needs to be enabled with nimbleOptions(allowDynamicIndexing = TRUE);\nthe intrinsic Gaussian CAR (conditional autoregressive) model can now be used in BUGS code as dcar_normal, which behaves similarly to BUGS’ car.normal distribution;\noptim is now part of the NIMBLE language and can be used in nimbleFunctions;\nthe WAIC model selection criterion can be calculated using the calculateWAIC method for MCMC objects;\nit is possible to call out to external compiled code or back to R functions from a nimbleFunction using nimbleExternalCall and nimbleRcall (this is an experimental feature);\nthe bootstrap and auxiliary particle filters can now return their ESS values;\nand a variety of bug fixes.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nJune 7, 2017: We’ve released Version 0.6-5\nVersion 0.6-5 is a minor release that primarily fixes some bugs and addresses some issues with packaging for CRAN. However, there are some new capabilities in this version.\nChanges as of Version 0.6-5 include:\n\nnimbleLists can now also be used in nimbleFunctions without setup code;\nbuildMCEM() can now estimate the asymptotic covariance of the model parameter estimates;\nvarious additional R-style functions (c(), rep(), seq(), diag() and ‘:’) can now be used in BUGS code;\nnew (improper) distributions, dflat and dhalfflat, are now available;\nan inverse-Wishart distribution is now available;\nand a variety of bug fixes and better error trapping.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nApril 25, 2017: We’ve released Version 0.6-4\nVersion 0.6-4 has many new features.\nChanges as of Version 0.6-4 include:\n\naddition of the functions c(), seq(), rep(), :, diag(), dim(), and which() for use in the NIMBLE language (i.e., run code) — usage generally mimics usage in R;\na complete reorganization of the User Manual, with the goal of clarifying how one can write nimbleFunctions to program with models;\naddition of the adaptive factor slice sampler, which can improve MCMC sampling for correlated blocks of parameters;\naddition of a new sampler that can handle non-conjugate Dirichlet settings;\naddition of a nimbleList data structure that behaves like R lists for use in nimbleFunctions;\naddition of eigendecomposition and SVD functions for use in the NIMBLE language;\nadditional flexibility in providing initial values for numeric(), logical(), integer(), matrix(), and array();\nlogical vectors and operators can now be used in the NIMBLE language;\nindexing of vectors and matrices can now use arbitrary numeric and logical vectors;\none can now index a vector of node names provided to values(), and more general indexing of node names in calculate(), simulate(), calculateDiff() and getLogProb();\naddition of the inverse-gamma distribution;\nuse of recycling for distribution functions used in the NIMBLE language;\nenhanced MCMC configuration functionality;\nusers can specify a user-defined BUGS distribution by simply providing a user-defined ‘d’ function without an ‘r’ function for use when an algorithm doesn’t need the ‘r’ function;\nand a variety of bug fixes, speedups, and better error trapping and checking.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nDecember 15, 2016: We’ve released Version 0.6-3\nVersion 0.6-3 is a very minor release primarily intended to address some CRAN packaging issues that do not affect users.\nWe also fixed a bug involving MCEM functionality and a bug that prevented use of the sd() and var() functions in BUGS code.\nFor most users, there is probably no need to upgrade from version 0.6-2.\n\n\nNovember 23, 2016: We’ve released Version 0.6-2\nVersion 0.6-2 is a minor release with a variety of useful functionality for users.\nChanges as of Version 0.6-2 include:\n\nuser-defined distributions can be used in BUGS code without needing to call the registerDistributions() function (unless one wants to specify alternative parameterizations, distribution range or that the distribution is discrete),\nusers can now specify the use of conjugate (Gibbs) samplers for nodes in a model,\nNIMBLE will now check the run code of nimbleFunctions for functions (in particular R functions) that are not part of the DSL and will not compile,\nadded getBound() functionality to find the lower and upper bounds of a node either from R or in DSL code,\nadded functionality to get distributional information about a node in a model or information about a distribution based on the name of the density function; these may be useful in setup code for algorithms,\nmultinomial and categorical distributions now allow ‘probs’ arguments that do not sum to one (these will be internally normalized) and\na variety of bug fixes.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nOctober 17, 2016: We’ve released Version 0.6-1\n(Note that version 0.6 has been on CRAN for about a month and contains essentially the same features as version 0.6-1. The reason for skipping a version number here on the website is because of some internal packaging issues we need to resolve with CRAN.)\nVersion 0.6-1 is a major release, with many of the changes focused on our internal implementation, some of which should speed building and compiling models and algorithms.\nChanges as of Version 0.6-1 include:\n\nincreased speed and reduced memory use in building and models, as well as compiling nimbleFunctions,\nenhanced MCMC functionality including a new runMCMC function for easily running multiple chains, a new multinomial random walk sampler, the ability to time each sampler in an MCMC, and a progress bar,\nthe addition of the ability to handle syntax such as model$calculate(nodes[i]) in the DSL,\na variety of changes to clean up our compilation system to comply with CRAN rules (including that NIMBLE should now work with gcc on Solaris), and\na variety of other items.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nMay 27, 2016: We’ve released Version 0.5-1\nVersion 0.5-1 is officially a minor release, but it actually has quite a bit in it, in particular the addition/improvement of a number of our algorithms. In addition there are some more improvements in our speed in building and compiling models and algorithms.\nChanges as of Version 0.5-1 include:\n\nthe addition of a variety of sequential Monte Carlo (aka particle filtering) algorithms, including particle MCMC samplers for use within an MCMC,\na greatly improved MCEM algorithm with an automated convergence and stopping criterion,\nnew syntax for declaring multivariate variables in the NIMBLE DSL, namely numeric(), integer(), matrix(), and array(), with declare() now deprecated,\naddition of the multivariate-t distribution for use in BUGS and DSL code,\na new binary MCMC sampler for discrete 0/1 nodes,\naddition of functionality to our random walk sampler to allow sampling on the log scale and use of reflection,\nmore flexible use of forwardsolve(), backsolve(), and solve(), including use in BUGS code, and\na variety of other items.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nApril 7, 2016: We’ve released Version 0.5\nVersion 0.5 is a major release. It is faster, uses less memory, and provides better syntax.\nChanges as of Version 0.5 include:\n\nmore efficient computations for conjugate sampling,\nadditional automated checking of BUGS syntax to improve NIMBLE’s warning/error messages,\nnew DSL (nimbleFunction programming) functionality to allow the use of syntax such as model$calculate(), etc. (syntax such as calculate(model) still works),\nnew functionality for MCMC sampler specification,\nimprovements in speed and memory use in building models,\naddition of forwardsolve, backsolve, and solve to the NIMBLE DSL, and\na variety of other items.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nOctober 4, 2015: We’ve released Version 0.4-1\nVersion 0.4-1 is a minor release. It fixes some logistical issues and adds a small amount of MCMC-related functionality.\nChanges as of Version 0.4-1 include:\n\nadded an elliptical slice sampler to the MCMC engine\nfixed bug preventing use of nimbleFunctions in packages depending on NIMBLE\nreduced C++ compiler warnings on Windows during use of compileNimble.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nAugust 2, 2015: We’ve released Version 0.4\nVersion 0.4 is a major release. We’ve added a number of user-level features and sped up most of the steps of building and compiling models and algorithms in the system.\nChanges as of Version 0.4 include:\n\nadded support for user-defined functions in BUGS code\nadded support for user-defined distributions in BUGS code\nadded support for truncated distributions, censoring, and general constraints in BUGS code\nnearly all calls to DSL functions or other nimbleFunctions handle R-style named or ordered arguments\nimproved handling of distribution functions in nimbleFunction run code and as deterministic functions in BUGS code\nadded an optional check when building model that alerts user to presence of nodes without values and log probability calculations that return NA\nadded calculateDiff as a fourth fundamental method (in addition to calculate, simulate, and getLogProb)\nadded flexibility and better organized return values of functions that specify MCMC samplers\nincorporated automated blocking into MCMC engine\nimproved a number of error messages to provide more useful diagnostic information\na variety of bug fixes\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nMarch 7, 2015: We’ve released Version 0.3-1\nChanges as of Version 0.3-1 include:\n\naddition of Dirichlet-multinomial conjugate sampling to the NIMBLE MCMC implementation\nhandling of arrays up to and including four dimensions\nnimbleModel() now allows data and constants to be provided together as the ‘constants’ argument\na variety of bug fixes\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nDecember 31, 2014: We’ve released Version 0.3\nChanges as of Version 0.3 include:\nImportant user-level syntax changes for creating and compiling models and algorithms that in some cases are not backwards compatible:\n\nnimbleFunctions are run via myNimbleFunction$run() instead of myNimbleFunction(). This means code written in v0.2 and earlier will not run without adding $run.\nWriting code for nimble models is now done with nimbleCode() (previously modelCode() ).\nTo customize MCMC use “myMCMCspec &lt;- configureMCMC(myModel)” (previously MCMCspec() ).\nTo build an MCMC algorithm, use either “myGenericMCMC &lt;- buildMCMC(myModel)” for a generic build or “myCustomizedMCMC &lt;- buildMCMC(myMCMCspec)” for a customized MCMC algorithm (previously one always had to build an MCMCspec, even for a generic build).\nVariables and methods of a nimbleFunction can be accessed by “myNimbleFunction\\(myVariable\" or \"myNimbleFunction\\)myMethod()” rather than “nfVar(myNimbleFunction, ‘myVariable’)” or “nfMethod(myNimbleFunction, ‘myMethod’)()”. Similarly, names of objects/methods of a nimble function can be queried by “ls(myNimbleFunction)”.\n\nIn addition:\n\nInternal changes to decrease compilation time, notably when compiling MCMC algorithms.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information.\n\n\nOctober, 2014: We’ve released Version 0.2\nChanges as of Version 0.2 include:\n\nInternal changes to decrease time to build models and nimbleFunctions.\nMultivariate conjugate updaters for the multivariate normal and Wishart are now included in our MCMC implementation, as well as block updating on multivariate nodes.\nAn extensive suite of tests of NIMBLE’s math functions, model building, and default MCMC.\nA number of bug fixes.\n\nPlease see the NEWS file in the source package or the nimble-dev GitHub site for more detailed information."
  },
  {
    "objectID": "blog/version-0-6-8-of-nimble-released-2.html",
    "href": "blog/version-0-6-8-of-nimble-released-2.html",
    "title": "Version 0.6-8 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website a week ago. Version 0.6-8 has a few new features, and more are on the way in the next few months.\nNew features include:\n\nthe proper Gaussian CAR (conditional autoregressive) model can now be used in BUGS code as dcar_proper, which behaves similarly to BUGS’ car.proper distribution;\na new nimbleMCMC function that provides one-line invocation of NIMBLE’s MCMC engine, akin to usage of JAGS and WinBUGS through R;\na new runCrossValidate function that will conduct k-fold cross-validation of NIMBLE models fit by MCMC;\ndynamic indexing in BUGS code is now allowed by default;\nand a variety of bug fixes and efficiency improvements.\n\nPlease see the NEWS file in the installed package for more details."
  },
  {
    "objectID": "blog/version-0-6-4-of-nimble-released.html",
    "href": "blog/version-0-6-4-of-nimble-released.html",
    "title": "Version 0.6-4 of NIMBLE released!",
    "section": "",
    "text": "We’ve just released the newest version of NIMBLE on CRAN and on our website. Version 0.6-4 has a bunch of new functionality for writing your own algorithms (using a natural R-like syntax) that can operate on user-provided models, specified using BUGS syntax. It also enhances the functionality of our built-in MCMC and other algorithms.\n\naddition of the functions c(), seq(), rep(), :, diag(), dim(), and which() for use in the NIMBLE language (i.e., run code) — usage generally mimics usage in R;\na complete reorganization of the User Manual, with the goal of clarifying how one can write nimbleFunctions to program with models;\naddition of the adaptive factor slice sampler, which can improve MCMC sampling for correlated blocks of parameters;\naddition of a new sampler that can handle non-conjugate Dirichlet settings;\naddition of a nimbleList data structure that behaves like R lists for use in nimbleFunctions;\naddition of eigendecomposition and SVD functions for use in the NIMBLE language;\nadditional flexibility in providing initial values for numeric(), logical(), integer(), matrix(), and array();\nlogical vectors and operators can now be used in the NIMBLE language;\nindexing of vectors and matrices can now use arbitrary numeric and logical vectors;\none can now index a vector of node names provided to values(), and more general indexing of node names in calculate(), simulate(), calculateDiff() and getLogProb();\naddition of the inverse-gamma distribution;\nuse of recycling for distribution functions used in the NIMBLE language;\nenhanced MCMC configuration functionality;\nusers can specify a user-defined BUGS distribution by simply providing a user-defined “d” function without an “r” function for use when an algorithm doesn’t need the “r” function;\nand a variety of bug fixes, speedups, and better error trapping and checking.\n\nPlease see the NEWS file in the installed package for more details."
  },
  {
    "objectID": "blog/version-0-4-released.html",
    "href": "blog/version-0-4-released.html",
    "title": "Version 0.4 released!",
    "section": "",
    "text": "In late July we released a major new version of NIMBLE, 0.4. Ok, that’s still a low version number, indicating we have a lot we still want to build and improve, but this version can do a lot and is a huge step forward from 0.3. Almost everything runs faster, from model building to model and nimbleFunction compiling to compiled execution. New features include the ability to write your own functions and distributions for BUGS (as nimbleFunctions, of course) and an algorithm that automatically adapts blocks of correlated parameters for efficient joint sampling in MCMC. Read NEWS for more details. The same information is also on GitHub."
  },
  {
    "objectID": "blog/version-0-12-2-of-nimble-released-including-an-important-bug-fix-for-some-models-using-bayesian-nonparametrics-with-the-dcrp-distribution.html",
    "href": "blog/version-0-12-2-of-nimble-released-including-an-important-bug-fix-for-some-models-using-bayesian-nonparametrics-with-the-dcrp-distribution.html",
    "title": "Version 0.12.2 of NIMBLE released, including an important bug fix for some models using Bayesian nonparametrics with the dCRP distribution",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nVersion 0.12.2 is a bug fix release. In particular, this release fixes a bug in our Bayesian nonparametric distribution (BNP) functionality that gives incorrect MCMC results for some models, specifically when using the dCRP distribution when the parameters of the mixture components (i.e., the clusters) have hyperparameters (i.e., the base measure parameters) that are unknown and sampled during the MCMC. Here is an example basic model structure that is affected by the bug:\nk[1:n] ~ dCRP(alpha, n) for(i in 1:n) {     y[i] ~ dnorm(mu[k[i]], 1)     mu[i] ~ dnorm(mu0, 1) ## mixture component parameters with hyperparameter } mu0 ~ dnorm(0, 1) ## unknown cluster hyperparameter\n(There is no problem without the hyperparameter layer - i.e., if mu0 is a fixed value – which is the situation in many models.)\nWe strongly encourage users using models with this type of structure to rerun their analyses, and we apologize for this issue.\nOther changes in this release include:\n\nFixing an issue with reversible jump variable selection under a similar situation to the BNP issue discussed above (in particular where there are unknown hyperparameters of the regression coefficients being considered, which would likely be an unusual use case).\nFixing a bug preventing setup of conjugate samplers for dwishart or dinvwishart nodes when using dynamic indexing.\nFixing a bug preventing use of truncation bounds specified via data or constants.\nFixing a bug preventing MCMC sampling with the LKJ prior for 2×2 matrices.\nFixing a bug in runCrossValidate affecting extraction of multivariate nodes.\nFixing a bug producing incorrect subset assignment into logical vectors in nimbleFunction code.\nFixing a bug preventing use of nimbleExternalCall with a constant expression.\nFixing a bug preventing use of recursion in nimbleFunctions without setup code.\n\nPlease see the release notes on our website for more details."
  },
  {
    "objectID": "blog/bayesian-nonparametric-models-in-nimble-part-2-nonparametric-random-effects.html",
    "href": "blog/bayesian-nonparametric-models-in-nimble-part-2-nonparametric-random-effects.html",
    "title": "Bayesian Nonparametric Models in NIMBLE, Part 2: Nonparametric Random Effects",
    "section": "",
    "text": "NIMBLE is a hierarchical modeling package that uses nearly the same language for model specification as the popular MCMC packages WinBUGS, OpenBUGS and JAGS, while making the modeling language extensible — you can add distributions and functions — and also allowing customization of the algorithms used to estimate the parameters of the model.\nRecently, we added support for Markov chain Monte Carlo (MCMC) inference for Bayesian nonparametric (BNP) mixture models to NIMBLE. In particular, starting with version 0.6-11, NIMBLE provides functionality for fitting models involving Dirichlet process priors using either the Chinese Restaurant Process (CRP) or a truncated stick-breaking (SB) representation of the Dirichlet process prior.\nWe will illustrate NIMBLE’s BNP capabilities using two examples. In a previous post, we showed how to use nonparametric mixture models with different kernels for density estimation. In this post, we will take a parametric generalized linear mixed model and show how to switch to a nonparametric representation of the random effects that avoids the assumption of normally-distributed random effects.\nFor more detailed information on NIMBLE and Bayesian nonparametrics in NIMBLE, see the NIMBLE User Manual.\n\n\n\nWe will illustrate the use of nonparametric mixture models for modeling random effects distributions in the context of a meta-analysis of the side effects of a formerly very popular drug for diabetes called Avandia. The data we analyze played a role in raising serious questions about the safety of this drug. The question is whether Avandia use increases the risk of myocardial infarction (heart attack). There are 48 studies (the 49th study in the data file is different in some ways and excluded here), each with treatment and control arms.\n\ndat &lt;- read.csv('../nimbleExamples/avandia.csv')\nhead(dat)\n\n  trial nAvandia avandiaMI nControl controlMI\n1     1      357         2      176         0\n2     2      391         2      207         1\n3     3      774         1      185         1\n4     4      213         0      109         1\n5     5      232         1      116         0\n6     6       43         0       47         1\n\ndat &lt;- dat[-49, ]\n\n\n\nWe begin with a standard generalized linear mixed model (GLMM)-based meta analysis. The vectors \\(n\\) and \\(x\\) contain the total number of patients in the control and the number of patients suffering from myocardial infarctions in the control group of each study, respectively. Similarly, the vectors \\(m\\) and \\(y\\) contain similar information for patients receiving the drug Avandia. The model takes the form\n\\[x_{i} \\mid \\theta, \\gamma_i \\sim \\text{Bin} \\left(n_i, \\frac{\\exp\\left\\{ \\gamma_i \\right\\}}{1 + \\exp\\left\\{ \\gamma_i \\right\\}} \\right) , \\quad\\quad y_{i} \\mid \\theta, \\gamma_i \\sim \\text{Bin} \\left(m_i, \\frac{\\exp\\left\\{ \\theta + \\gamma_i \\right\\}}{1 + \\exp\\left\\{ \\theta + \\gamma_i \\right\\}} \\right)\\]\nwhere the random effects, \\(\\gamma_i\\), follow a common normal distribution, \\(\\gamma_i \\sim \\text{N}(0, \\tau^2)\\), and the \\(\\theta\\) and \\(\\tau^2\\) are given reasonably non-informative priors. The parameter \\(\\theta\\) quantifies the difference in risk between the control and treatment arms, while the \\(\\gamma_i\\) quantify study-specific variation.\nThis model can be specified in NIMBLE using the following code:\n\nlibrary(nimble)\n\n\nx &lt;- dat$controlMI\nn &lt;- dat$nControl\ny &lt;- dat$avandiaMI\nm &lt;- dat$nAvandia\n\nnStudies &lt;- nrow(dat)\ndata &lt;- list(x = x, y = y)\nconstants = list(n = n, m = m, nStudies = nStudies)\n\ncodeParam &lt;- nimbleCode({\n    for(i in 1:nStudies) {\n        y[i] ~ dbin(size = m[i], prob = q[i]) # avandia MIs\n        x[i] ~ dbin(size = n[i], prob = p[i]) # control MIs\n        q[i] &lt;- expit(theta + gamma[i])       # Avandia log-odds\n        p[i] &lt;- expit(gamma[i])               # control log-odds\n        gamma[i] ~ dnorm(mu, var = tau2)      # study effects\n    }\n    theta ~ dflat()        # effect of Avandia\n    # random effects hyperparameters\n    mu ~ dnorm(0, 10)\n    tau2 ~ dinvgamma(2, 1)\n})\n\n\n\n\nLet’s run a basic MCMC.\n\nset.seed(9)\ninits = list(theta = 0, mu = 0, tau2 = 1, gamma = rnorm(nStudies))\n\nsamples &lt;- nimbleMCMC(code = codeParam, data = data, inits = inits,\n                      constants = constants, monitors = c(\"mu\", \"tau2\", \"theta\", \"gamma\"),\n                      thin = 10, niter = 22000, nburnin = 2000, nchains = 1, setSeed = TRUE)\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\nChecking model calculations\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\nrunning chain 1...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\npar(mfrow = c(1, 4), cex = 0.9, mgp = c(1.8,.7,0), mai = c(.5,.5,.4,.1))\nts.plot(samples[ , 'theta'], xlab = 'iteration', ylab = expression(theta))\nhist(samples[ , 'theta'], xlab = expression(theta), main = 'effect of Avandia')\n\ngammaCols &lt;- grep('gamma', colnames(samples))\ngammaMn &lt;- colMeans(samples[ , gammaCols])\nhist(gammaMn, xlab = 'posterior means', main = 'random effects distribution')\nhist(samples[1000, gammaCols], xlab = 'single draw',\n                   main = 'random effects distribution')\n\n\n\n\n\n\n\n\nThe results suggests there is an overall difference in risk between the control and treatment arms. But what about the normality assumption? Are our conclusions robust to that assumption? Perhaps the random effects distribution are skewed. (And recall that the estimates above of the random effects are generated under the normality assumption, which pushes the estimated effects to look more normal…)\n\n\n\n\n\n\nNow, we use a nonparametric distribution for the \\(\\gamma_i\\)s. More specifically, we assume that each \\(\\gamma_i\\) is generated from a location-scale mixture of normal distributions:\n\\[\\gamma_i \\mid \\mu_i, \\tau_i^2 \\sim \\text{N}(\\mu_i, \\tau_i^2), \\quad\\quad (\\mu_i, \\tau_i^2) \\mid G \\sim G, \\quad\\quad G \\sim \\text{DP}(\\alpha, H),\\]\nwhere \\(H\\) is a normal-inverse-gamma distribution.\nThis specification induces clustering among the random effects. As in the case of density estimation problems, the DP prior allows the data to determine the number of components, from as few as one component (i.e., simplifying to the parametric model), to as many as \\(n\\) components, i.e., one component for each observation. This allows the distribution of the random effects to be multimodal if the data supports such behavior, greatly increasing its flexibility. This model can be specified in NIMBLE using the following code:\n\ncodeBNP &lt;- nimbleCode({\n    for(i in 1:nStudies) {\n        y[i] ~ dbin(size = m[i], prob = q[i])   # avandia MIs\n        x[i] ~ dbin(size = n[i], prob = p[i])   # control MIs\n        q[i] &lt;- expit(theta + gamma[i])         # Avandia log-odds\n        p[i] &lt;- expit(gamma[i])                 # control log-odds\n        gamma[i] ~ dnorm(mu[i], var = tau2[i])  # random effects from mixture dist.\n        mu[i] &lt;- muTilde[xi[i]]                 # mean for random effect from cluster xi[i]\n        tau2[i] &lt;- tau2Tilde[xi[i]]             # var for random effect from cluster xi[i]\n    }\n    # mixture component parameters drawn from base measures\n    for(i in 1:nStudies) {\n        muTilde[i] ~ dnorm(mu0, var = var0)\n        tau2Tilde[i] ~ dinvgamma(a0, b0)\n    }\n    # CRP for clustering studies to mixture components\n    xi[1:nStudies] ~ dCRP(alpha, size = nStudies)\n    # hyperparameters\n    alpha ~ dgamma(1, 1)\n    mu0 ~ dnorm(0, 10)\n    var0 ~ dinvgamma(2, 1)\n    a0 ~ dinvgamma(2, 1)\n    b0 ~ dinvgamma(2, 1)\n    theta ~ dflat()          # effect of Avandia\n})\n\n\n\n\nThe following code compiles the model and runs a collapsed Gibbs sampler for the model\n\ninits &lt;- list(gamma = rnorm(nStudies), xi = sample(1:2, nStudies, replace = TRUE),\n              alpha = 1, mu0 = 0, var0 = 1, a0 = 1, b0 = 1, theta = 0,\n              muTilde = rnorm(nStudies), tau2Tilde = rep(1, nStudies))\n\nsamplesBNP &lt;- nimbleMCMC(code = codeBNP, data = data, inits = inits,\n               constants = constants,\n               monitors = c(\"theta\", \"gamma\", \"alpha\", \"xi\", \"mu0\", \"var0\", \"a0\", \"b0\"),\n               thin = 10, niter = 22000, nburnin = 2000, nchains = 1, setSeed = TRUE)\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\nChecking model calculations\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\nrunning chain 1...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\ngammaCols &lt;- grep('gamma', colnames(samplesBNP))\ngammaMn &lt;- colMeans(samplesBNP[ , gammaCols])\nxiCols &lt;- grep('xi', colnames(samplesBNP))\n\npar(mfrow = c(1,5), cex = 0.9, mgp = c(1.8,.7,0), mai = c(.5,.5,.4,.1))\nts.plot(samplesBNP[ , 'theta'], xlab = 'iteration', ylab = expression(theta),\n   main = expression(paste('traceplot for ', theta)))\nhist(samplesBNP[ , 'theta'], xlab = expression(theta), main = 'effect of Avandia')\nhist(gammaMn, xlab = 'posterior means',\n              main = \"random effects distrib'n\")\nhist(samplesBNP[1000, gammaCols], xlab = 'single draw',\n                   main = \"random effects distrib'n\")\n\n# How many mixture components are inferred?\nxiRes &lt;- samplesBNP[ , xiCols]\nnGrps &lt;- apply(xiRes, 1, function(x) length(unique(x)))\nts.plot(nGrps, xlab = 'iteration', ylab = 'number of components',\n   main = 'number of components')\n\n\n\n\n\n\n\n\nThe primary inference seems robust to the original parametric assumption. This is probably driven by the fact that there is not much evidence of lack of normality in the random effects distribution (as evidenced by the fact that the posterior distribution of the number of mixture components places a large amount of probability on exactly one component).\n\n\n\n\nPlease see our User Manual for more details.\nWe’re in the midst of improvements to the existing BNP functionality as well as adding additional Bayesian nonparametric models, such as hierarchical Dirichlet processes and Pitman-Yor processes, so please add yourself to our announcement or user support/discussion Google groups."
  },
  {
    "objectID": "blog/bayesian-nonparametric-models-in-nimble-part-2-nonparametric-random-effects.html#overview",
    "href": "blog/bayesian-nonparametric-models-in-nimble-part-2-nonparametric-random-effects.html#overview",
    "title": "Bayesian Nonparametric Models in NIMBLE, Part 2: Nonparametric Random Effects",
    "section": "",
    "text": "NIMBLE is a hierarchical modeling package that uses nearly the same language for model specification as the popular MCMC packages WinBUGS, OpenBUGS and JAGS, while making the modeling language extensible — you can add distributions and functions — and also allowing customization of the algorithms used to estimate the parameters of the model.\nRecently, we added support for Markov chain Monte Carlo (MCMC) inference for Bayesian nonparametric (BNP) mixture models to NIMBLE. In particular, starting with version 0.6-11, NIMBLE provides functionality for fitting models involving Dirichlet process priors using either the Chinese Restaurant Process (CRP) or a truncated stick-breaking (SB) representation of the Dirichlet process prior.\nWe will illustrate NIMBLE’s BNP capabilities using two examples. In a previous post, we showed how to use nonparametric mixture models with different kernels for density estimation. In this post, we will take a parametric generalized linear mixed model and show how to switch to a nonparametric representation of the random effects that avoids the assumption of normally-distributed random effects.\nFor more detailed information on NIMBLE and Bayesian nonparametrics in NIMBLE, see the NIMBLE User Manual."
  },
  {
    "objectID": "blog/bayesian-nonparametric-models-in-nimble-part-2-nonparametric-random-effects.html#parametric-meta-analysis-of-avandia-myocardial-infarctions-mis",
    "href": "blog/bayesian-nonparametric-models-in-nimble-part-2-nonparametric-random-effects.html#parametric-meta-analysis-of-avandia-myocardial-infarctions-mis",
    "title": "Bayesian Nonparametric Models in NIMBLE, Part 2: Nonparametric Random Effects",
    "section": "",
    "text": "We will illustrate the use of nonparametric mixture models for modeling random effects distributions in the context of a meta-analysis of the side effects of a formerly very popular drug for diabetes called Avandia. The data we analyze played a role in raising serious questions about the safety of this drug. The question is whether Avandia use increases the risk of myocardial infarction (heart attack). There are 48 studies (the 49th study in the data file is different in some ways and excluded here), each with treatment and control arms.\n\ndat &lt;- read.csv('../nimbleExamples/avandia.csv')\nhead(dat)\n\n  trial nAvandia avandiaMI nControl controlMI\n1     1      357         2      176         0\n2     2      391         2      207         1\n3     3      774         1      185         1\n4     4      213         0      109         1\n5     5      232         1      116         0\n6     6       43         0       47         1\n\ndat &lt;- dat[-49, ]\n\n\n\nWe begin with a standard generalized linear mixed model (GLMM)-based meta analysis. The vectors \\(n\\) and \\(x\\) contain the total number of patients in the control and the number of patients suffering from myocardial infarctions in the control group of each study, respectively. Similarly, the vectors \\(m\\) and \\(y\\) contain similar information for patients receiving the drug Avandia. The model takes the form\n\\[x_{i} \\mid \\theta, \\gamma_i \\sim \\text{Bin} \\left(n_i, \\frac{\\exp\\left\\{ \\gamma_i \\right\\}}{1 + \\exp\\left\\{ \\gamma_i \\right\\}} \\right) , \\quad\\quad y_{i} \\mid \\theta, \\gamma_i \\sim \\text{Bin} \\left(m_i, \\frac{\\exp\\left\\{ \\theta + \\gamma_i \\right\\}}{1 + \\exp\\left\\{ \\theta + \\gamma_i \\right\\}} \\right)\\]\nwhere the random effects, \\(\\gamma_i\\), follow a common normal distribution, \\(\\gamma_i \\sim \\text{N}(0, \\tau^2)\\), and the \\(\\theta\\) and \\(\\tau^2\\) are given reasonably non-informative priors. The parameter \\(\\theta\\) quantifies the difference in risk between the control and treatment arms, while the \\(\\gamma_i\\) quantify study-specific variation.\nThis model can be specified in NIMBLE using the following code:\n\nlibrary(nimble)\n\n\nx &lt;- dat$controlMI\nn &lt;- dat$nControl\ny &lt;- dat$avandiaMI\nm &lt;- dat$nAvandia\n\nnStudies &lt;- nrow(dat)\ndata &lt;- list(x = x, y = y)\nconstants = list(n = n, m = m, nStudies = nStudies)\n\ncodeParam &lt;- nimbleCode({\n    for(i in 1:nStudies) {\n        y[i] ~ dbin(size = m[i], prob = q[i]) # avandia MIs\n        x[i] ~ dbin(size = n[i], prob = p[i]) # control MIs\n        q[i] &lt;- expit(theta + gamma[i])       # Avandia log-odds\n        p[i] &lt;- expit(gamma[i])               # control log-odds\n        gamma[i] ~ dnorm(mu, var = tau2)      # study effects\n    }\n    theta ~ dflat()        # effect of Avandia\n    # random effects hyperparameters\n    mu ~ dnorm(0, 10)\n    tau2 ~ dinvgamma(2, 1)\n})\n\n\n\n\nLet’s run a basic MCMC.\n\nset.seed(9)\ninits = list(theta = 0, mu = 0, tau2 = 1, gamma = rnorm(nStudies))\n\nsamples &lt;- nimbleMCMC(code = codeParam, data = data, inits = inits,\n                      constants = constants, monitors = c(\"mu\", \"tau2\", \"theta\", \"gamma\"),\n                      thin = 10, niter = 22000, nburnin = 2000, nchains = 1, setSeed = TRUE)\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\nChecking model calculations\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\nrunning chain 1...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\npar(mfrow = c(1, 4), cex = 0.9, mgp = c(1.8,.7,0), mai = c(.5,.5,.4,.1))\nts.plot(samples[ , 'theta'], xlab = 'iteration', ylab = expression(theta))\nhist(samples[ , 'theta'], xlab = expression(theta), main = 'effect of Avandia')\n\ngammaCols &lt;- grep('gamma', colnames(samples))\ngammaMn &lt;- colMeans(samples[ , gammaCols])\nhist(gammaMn, xlab = 'posterior means', main = 'random effects distribution')\nhist(samples[1000, gammaCols], xlab = 'single draw',\n                   main = 'random effects distribution')\n\n\n\n\n\n\n\n\nThe results suggests there is an overall difference in risk between the control and treatment arms. But what about the normality assumption? Are our conclusions robust to that assumption? Perhaps the random effects distribution are skewed. (And recall that the estimates above of the random effects are generated under the normality assumption, which pushes the estimated effects to look more normal…)"
  },
  {
    "objectID": "blog/bayesian-nonparametric-models-in-nimble-part-2-nonparametric-random-effects.html#dp-based-random-effects-modeling-for-meta-analysis",
    "href": "blog/bayesian-nonparametric-models-in-nimble-part-2-nonparametric-random-effects.html#dp-based-random-effects-modeling-for-meta-analysis",
    "title": "Bayesian Nonparametric Models in NIMBLE, Part 2: Nonparametric Random Effects",
    "section": "",
    "text": "Now, we use a nonparametric distribution for the \\(\\gamma_i\\)s. More specifically, we assume that each \\(\\gamma_i\\) is generated from a location-scale mixture of normal distributions:\n\\[\\gamma_i \\mid \\mu_i, \\tau_i^2 \\sim \\text{N}(\\mu_i, \\tau_i^2), \\quad\\quad (\\mu_i, \\tau_i^2) \\mid G \\sim G, \\quad\\quad G \\sim \\text{DP}(\\alpha, H),\\]\nwhere \\(H\\) is a normal-inverse-gamma distribution.\nThis specification induces clustering among the random effects. As in the case of density estimation problems, the DP prior allows the data to determine the number of components, from as few as one component (i.e., simplifying to the parametric model), to as many as \\(n\\) components, i.e., one component for each observation. This allows the distribution of the random effects to be multimodal if the data supports such behavior, greatly increasing its flexibility. This model can be specified in NIMBLE using the following code:\n\ncodeBNP &lt;- nimbleCode({\n    for(i in 1:nStudies) {\n        y[i] ~ dbin(size = m[i], prob = q[i])   # avandia MIs\n        x[i] ~ dbin(size = n[i], prob = p[i])   # control MIs\n        q[i] &lt;- expit(theta + gamma[i])         # Avandia log-odds\n        p[i] &lt;- expit(gamma[i])                 # control log-odds\n        gamma[i] ~ dnorm(mu[i], var = tau2[i])  # random effects from mixture dist.\n        mu[i] &lt;- muTilde[xi[i]]                 # mean for random effect from cluster xi[i]\n        tau2[i] &lt;- tau2Tilde[xi[i]]             # var for random effect from cluster xi[i]\n    }\n    # mixture component parameters drawn from base measures\n    for(i in 1:nStudies) {\n        muTilde[i] ~ dnorm(mu0, var = var0)\n        tau2Tilde[i] ~ dinvgamma(a0, b0)\n    }\n    # CRP for clustering studies to mixture components\n    xi[1:nStudies] ~ dCRP(alpha, size = nStudies)\n    # hyperparameters\n    alpha ~ dgamma(1, 1)\n    mu0 ~ dnorm(0, 10)\n    var0 ~ dinvgamma(2, 1)\n    a0 ~ dinvgamma(2, 1)\n    b0 ~ dinvgamma(2, 1)\n    theta ~ dflat()          # effect of Avandia\n})\n\n\n\n\nThe following code compiles the model and runs a collapsed Gibbs sampler for the model\n\ninits &lt;- list(gamma = rnorm(nStudies), xi = sample(1:2, nStudies, replace = TRUE),\n              alpha = 1, mu0 = 0, var0 = 1, a0 = 1, b0 = 1, theta = 0,\n              muTilde = rnorm(nStudies), tau2Tilde = rep(1, nStudies))\n\nsamplesBNP &lt;- nimbleMCMC(code = codeBNP, data = data, inits = inits,\n               constants = constants,\n               monitors = c(\"theta\", \"gamma\", \"alpha\", \"xi\", \"mu0\", \"var0\", \"a0\", \"b0\"),\n               thin = 10, niter = 22000, nburnin = 2000, nchains = 1, setSeed = TRUE)\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\nChecking model calculations\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\nrunning chain 1...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\ngammaCols &lt;- grep('gamma', colnames(samplesBNP))\ngammaMn &lt;- colMeans(samplesBNP[ , gammaCols])\nxiCols &lt;- grep('xi', colnames(samplesBNP))\n\npar(mfrow = c(1,5), cex = 0.9, mgp = c(1.8,.7,0), mai = c(.5,.5,.4,.1))\nts.plot(samplesBNP[ , 'theta'], xlab = 'iteration', ylab = expression(theta),\n   main = expression(paste('traceplot for ', theta)))\nhist(samplesBNP[ , 'theta'], xlab = expression(theta), main = 'effect of Avandia')\nhist(gammaMn, xlab = 'posterior means',\n              main = \"random effects distrib'n\")\nhist(samplesBNP[1000, gammaCols], xlab = 'single draw',\n                   main = \"random effects distrib'n\")\n\n# How many mixture components are inferred?\nxiRes &lt;- samplesBNP[ , xiCols]\nnGrps &lt;- apply(xiRes, 1, function(x) length(unique(x)))\nts.plot(nGrps, xlab = 'iteration', ylab = 'number of components',\n   main = 'number of components')\n\n\n\n\n\n\n\n\nThe primary inference seems robust to the original parametric assumption. This is probably driven by the fact that there is not much evidence of lack of normality in the random effects distribution (as evidenced by the fact that the posterior distribution of the number of mixture components places a large amount of probability on exactly one component)."
  },
  {
    "objectID": "blog/bayesian-nonparametric-models-in-nimble-part-2-nonparametric-random-effects.html#more-information-and-future-development",
    "href": "blog/bayesian-nonparametric-models-in-nimble-part-2-nonparametric-random-effects.html#more-information-and-future-development",
    "title": "Bayesian Nonparametric Models in NIMBLE, Part 2: Nonparametric Random Effects",
    "section": "",
    "text": "Please see our User Manual for more details.\nWe’re in the midst of improvements to the existing BNP functionality as well as adding additional Bayesian nonparametric models, such as hierarchical Dirichlet processes and Pitman-Yor processes, so please add yourself to our announcement or user support/discussion Google groups."
  },
  {
    "objectID": "blog/a-close-look-at-some-posted-trials-of-nimble-for-accelerated-failure-time-models.html",
    "href": "blog/a-close-look-at-some-posted-trials-of-nimble-for-accelerated-failure-time-models.html",
    "title": "A close look at some posted trials of nimble for accelerated failure time models",
    "section": "",
    "text": "A bunch of folks have brought to our attention a manuscript by Beraha, Falco and Guglielmi (BFG) posted on arXiv giving some comparisons between JAGS, NIMBLE, and Stan. Naturally, we wanted to take a look. Each package performs best in some of their comparisons. There’s a lot going on, so here we’re just going to work through the last of their four examples, an accelerated failure time (AFT) model, because that’s the one where NIMBLE looks the worst in their results. The code from BFG is given on GitHub here.\nThere may be some issues with their other three examples as well, and we might work through those in future blog post(s). NIMBLE provides a lot of flexibility for configuring MCMCs in different ways (with different samplers), which means a comparison using our default configuration is just a start. Performance differences can also arise from writing the same model in different ways. We see both kinds of issues coming up for the other examples. But the AFT example gives a lot to talk about, so we’re sticking to that one here.\nIt turns out that NIMBLE and JAGS were put at a huge disadvantage compared to Stan, and that BFG’s results from NIMBLE don’t look valid, and that there isn’t any exploration of NIMBLE’s configurability. If we make the model for NIMBLE and JAGS comparable to the model for Stan, NIMBLE does roughly 2-45 times better in various cases than what BFG reported. If we explore a simple block sampling option, NIMBLE gets a small additional boost in some cases. It’s hard to compare results exactly with what BFG report, and we are not out to re-run the full comparison including JAGS and Stan. A “back of the envelope” comparison suggests that NIMBLE is still less efficient than Stan for this example, but not nearly to the degree reported. We’re also not out to explore many sampling configurations to try for better performance in this particular example problem, but part of NIMBLE’s design is to make it easy to do so.\nBefore starting into the AFT models, it’s worth recognizing that software benchmarks and other kinds of performance comparisons are really hard to do well. It’s almost inevitable that, when done by developers of one package, that package gets a boost in results even if objectivity is the honest goal. That’s because package developers almost can’t help using their package effectively and likely don’t know how to use other packages as well as their own. In this case, it’s fair to point out that NIMBLE needs more care in providing valid initial values (which BFG’s code doesn’t do) and that NIMBLE’s default samplers don’t work well here, which is because this problem features heavy right tails of Weibull distributions with shape parameter &lt; 1. For many users, that is not a typical problem. By choosing slice samplers (which JAGS often uses too) instead of NIMBLE’s default Metropolis-Hastings samplers, the mixing is much better. This issue is only relevant to the problem as BFG formulated it for JAGS and NIMBLE and goes away when we put it on par with the formulation BFG gave to Stan. In principle, comparisons by third parties, like BFG, might be more objective than those by package developers, but in this case the comparisons by BFG don’t use JAGS or NIMBLE effectively and include incorrect results from NIMBLE.\nBelow we try to reproduce their (invalid) results for NIMBLE and to run some within-NIMBLE comparisons of other methods. We’ll stick to their model scenarios and performance metrics. Those metrics are not the way we’ve done some published MCMC comparisons here, here and here, but using them will allow readers to interpret our results alongside theirs.\nFirst we’ll give a brief summary of their model scenarios. Here goes."
  },
  {
    "objectID": "blog/a-close-look-at-some-posted-trials-of-nimble-for-accelerated-failure-time-models.html#right-censored-failure-time-data",
    "href": "blog/a-close-look-at-some-posted-trials-of-nimble-for-accelerated-failure-time-models.html#right-censored-failure-time-data",
    "title": "A close look at some posted trials of nimble for accelerated failure time models",
    "section": "Right-censored failure time data",
    "text": "Right-censored failure time data\nWhen a failure time is directly observed, its likelihood contribution is \\(f_W(t | a, s e^{x' \\eta})\\). When a unit hasn’t failed by its last observation, all that is known is that it lasted at least until \\(t\\). Then its likelihood contribution is \\(1-F_W(t | a, s e^{x' \\eta})\\). This is called a right-censored observation. Thus the data consist of some \\(t\\)s that are actual failure times and some \\(t\\)s that are right-censoring times.\nThere are two ways to handle a right-censored observation in MCMC:\n\nInclude the likelihood factor \\(1-F_W(t | a, s e^{x' \\eta})\\). This is how BFG set up the model for Stan.\nInclude a latent state, \\(t'\\), for the failure time. Include the likelihood factor \\(f_W(t' | a, s e^{x' \\eta})\\) and let MCMC sample \\(t'\\), with the numerical effect of integrating over it. This is how BFG set up the model for JAGS and NIMBLE.\n\nThe first version is marginalized relative to the second version because \\(1-F_W(t | a, s e^{x' \\eta})\\) integrates over \\(t'\\) without needing to sample it. Often, but not always, marginalization is computationally faster and gives better mixing, so it makes the MCMC problem easier. That’s why the comparison as set up by BFG seems like an apples-to-oranges comparison. They’ve made the problem substantially easier for Stan.\nIt’s easy to set up the marginalized version for JAGS or NIMBLE. This can be done using the “zeroes” trick in the BUGS language, which both packages use for writing models. In NIMBLE this can also be done by writing a user-defined distribution as a nimbleFunction, which can be compiled along with a model."
  },
  {
    "objectID": "blog/a-close-look-at-some-posted-trials-of-nimble-for-accelerated-failure-time-models.html#bfgs-scenarios",
    "href": "blog/a-close-look-at-some-posted-trials-of-nimble-for-accelerated-failure-time-models.html#bfgs-scenarios",
    "title": "A close look at some posted trials of nimble for accelerated failure time models",
    "section": "BFG’s scenarios",
    "text": "BFG’s scenarios\nBFG included the following scenarios:\n\nSample size, \\(N\\), is 100 or 1000.\nNumber of explanatory variables, \\(p\\), is 4 or 16. These always include an intercept. Other covariates, and the true coefficient values, are simulated.\nCensoring times are drawn from another Weibull distribution. This is set up following previous works such that the expected proportion of censored values is 20%, 50% or 80%.\nMost of their comparisons use informative priors. Those are the ones we look at here. Again, we weren’t out to look at everything they did.\nThey used \\(N_{it} = 10,000\\) total iterations. Of these, \\(5,000\\) were discarded as burn-in (warmup). They used a thinning interval of 2, resulting in \\(N_s = 2,500\\) saved samples."
  },
  {
    "objectID": "blog/nimble-virtual-short-course-may-26-28.html",
    "href": "blog/nimble-virtual-short-course-may-26-28.html",
    "title": "NIMBLE virtual short course, May 26-28",
    "section": "",
    "text": "We’ll be holding a virtual training workshop on NIMBLE, May 26-28, from 8 am to 1 pm US Pacific (California) time each day. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nThe workshop will roughly follow the material covered in our June 2020 virtual training, in particular:\n\nthe basic concepts and workflows for using NIMBLE and converting BUGS or JAGS models to work in NIMBLE.\noverview of different MCMC sampling strategies and how to use them in NIMBLE.\nwriting new distributions and functions for more flexible modeling and more efficient computation.\ntips and tricks for improving computational efficiency.\nusing advanced model components, including Bayesian non-parametric distributions (based on Dirichlet process priors), conditional auto-regressive (CAR) models for spatially correlated random fields, and reversible jump samplers for variable selection.\nan introduction to programming new algorithms in NIMBLE.\ncalling R and compiled C++ code from compiled NIMBLE models or functions.\n\nIf participant interests vary sufficiently, the third session will be split into two tracks. One of these will likely focus on ecological models. The other will be chosen based on attendee interest from topics such as (a) advanced NIMBLE programming including writing new MCMC samplers, (b) advanced spatial or Bayesian non-parametric modeling, or (c) non-MCMC algorithms in NIMBLE, such as sequential Monte Carlo.\nIf you are interested in attending, please pre-register at https://forms.gle/6AtNgfdUdvhni32Q6. This will hold a spot for you and allow us to learn about your specific interests. No payment is necessary to pre-register. Fees to finalize registration will be $100 (regular) or $50 (student). We will offer a process for students to request a fee waiver.\nThe workshop will assume attendees have a basic understanding of hierarchical/Bayesian models and MCMC, the BUGS (or JAGS) model language, and some familiarity with R."
  },
  {
    "objectID": "blog/version-1-2-1-of-nimble-released.html",
    "href": "blog/version-1-2-1-of-nimble-released.html",
    "title": "Version 1.2.1 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC, Laplace approximation, and SMC).\nThis is a micro release that primarily addresses some packaging changes requested by CRAN. In addition, this release includes:\n\nA multinomial MCMC sampler, sampler_RW_multinomial, for random variables following a multinomial distribution.\nSome enhancements to error trapping and warning messages.\nA variety of minor bug fixes.",
    "crumbs": [
      "Home",
      "Blog Posts",
      "Version 1.2.1 of NIMBLE released"
    ]
  },
  {
    "objectID": "blog/nimblehmc-version-0-2-0-released-providing-improved-hmc-performance.html",
    "href": "blog/nimblehmc-version-0-2-0-released-providing-improved-hmc-performance.html",
    "title": "nimbleHMC version 0.2.0 released, providing improved HMC performance",
    "section": "",
    "text": "nimbleHMC provides Hamiltonian Monte Carlo samplers for use with NIMBLE, in particular NUTS samplers. NIMBLE’s HMC samplers can be flexibly assigned to a subset of model parameters, allowing users to consider various sampling configurations.\nWe’ve released version 0.2.0 of nimbleHMC, which includes a new default NUTS sampler inspired by Stan’s implementation of NUTS. It also provides an updated version of our previous NUTS sampler (which is based on the original Hoffman and Gelman paper, and is now called the ‘NUTS_classic’ sampler in NIMBLE) that fixes performance issues in version 0.1.1."
  },
  {
    "objectID": "blog/version-0-13-1-of-nimble-released.html",
    "href": "blog/version-0-13-1-of-nimble-released.html",
    "title": "Version 0.13.1 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. This version is purely a bug fix release that fixes a bug introduced in our new handling of predictive nodes in version 0.13.0 (released in November). If you installed version 0.13.0, please upgrade to 0.13.1."
  },
  {
    "objectID": "blog/version-0-6-3-released.html",
    "href": "blog/version-0-6-3-released.html",
    "title": "Version 0.6-3 released.",
    "section": "",
    "text": "Version 0.6-3 is a very minor release primarily intended to address some CRAN packaging issues that do not affect users. We also fixed a bug involving MCEM functionality and a bug that prevented use of the sd() and var() functions in BUGS code.\nFor most users, there is probably no need to upgrade from version 0.6-2."
  },
  {
    "objectID": "blog/version-0-13-0-of-nimble-released.html",
    "href": "blog/version-0-13-0-of-nimble-released.html",
    "title": "Version 0.13.0 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nVersion 0.13.0 provides new functionality (in particular improved handling of predictive nodes in MCMC) and minor bug fixes, including:\n\nThoroughly revamping handling of posterior predictive nodes in the MCMC system, in particular that MCMC samplers, by default, will now exclude predictive dependencies from internal sampler calculations. This should improve MCMC mixing for models with predictive nodes. Posterior predictive nodes are now sampled conditional on all other model nodes at the end of each MCMC iteration.\nAdding functionality to the MCMC configuration system, including a new replaceSamplers method and updates to the arguments for the addSamplers method.\nAdding an option to the WAIC system to allow additional burnin (in addition to standard MCMC burnin) before calculating online WAIC, thereby allowing inspection of initial samples without forcing them to be used for WAIC.\nWarning users of unused constants during model building.\nFixing bugs that prevented use of variables starting with “logProb” or named “i” in model code.\nFixing a bug to prevent infinite recursion in particular cases in conjugacy checking.\nFixing a bug in simulating from dcar_normal nodes when multiple nodes passed to simulate.\n\nPlease see the release notes on our website for more details."
  },
  {
    "objectID": "blog/building-particle-filters-and-particle-mcmc-in-nimble-2.html",
    "href": "blog/building-particle-filters-and-particle-mcmc-in-nimble-2.html",
    "title": "Building Particle Filters and Particle MCMC in NIMBLE",
    "section": "",
    "text": "An Example of Using nimble’s Particle Filtering Algorithms\nThis example shows how to construct and conduct inference on a state space model using particle filtering algorithms. nimble currently has versions of the bootstrap filter, the auxiliary particle filter, the ensemble Kalman filter, and the Liu and West filter implemented. Additionally, particle MCMC samplers are available and can be specified for both univariate and multivariate parameters.\n\nModel Creation\nAssume \\(x_{i}\\) is the latent state and \\(y_{i}\\) is the observation at time \\(i\\) for \\(i=1,\\ldots,t\\). We define our state space model as\n\\[x_{i} \\sim N(a \\cdot x_{i-1} + b, \\sigma_{PN})\\] \\[y_{i} \\sim t_{5}(x_{i}, \\sigma_{OE})\\]\nwith initial states\n\\[x_{1} \\sim N\\left(\\frac{b}{1-a}, \\frac{\\sigma_{PN}}{\\sqrt{1-a^2}}\\right)\\] \\[y_{1} \\sim t_{5}(x_{1}, \\sigma_{OE})\\]\nand prior distributions\n\\[a \\sim \\text{Unif}(-0.999, 0.999)\\] \\[b \\sim N(0, 1000)\\]\nwhere \\(N(\\mu, \\sigma)\\) denotes a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), and \\(t_{\\nu}(\\mu, \\sigma)\\) is a shifted, scaled \\(t\\)-distribution with center parameter \\(\\mu\\), scale parameter \\(\\sigma\\), and \\(\\nu\\) degrees of freedom.\nWe specify and build our state space model below, using \\(t=10\\) time points:\n\nlibrary('nimbleSMC')\n\n\nset.seed(1)\n\n## define the model\nstateSpaceCode &lt;- nimbleCode({\n    a ~ dunif(-0.9999, 0.9999)\n    b ~ dnorm(0, sd = 1000)\n    sigPN ~ dunif(1e-04, 1)\n    sigOE ~ dunif(1e-04, 1)\n    x[1] ~ dnorm(b/(1 - a), sd = sigPN/sqrt((1-a*a)))\n    y[1] ~ dt(mu = x[1], sigma = sigOE, df = 5)\n    for (i in 2:t) {\n        x[i] ~ dnorm(a * x[i - 1] + b, sd = sigPN)\n        y[i] ~ dt(mu = x[i], sigma = sigOE, df = 5)\n    }\n})\n\n## define data, constants, and initial values  \ndata &lt;- list(\n    y = c(0.213, 1.025, 0.314, 0.521, 0.895, 1.74, 0.078, 0.474, 0.656, 0.802)\n)\nconstants &lt;- list(\n    t = 10\n)\ninits &lt;- list(\n    a = 0,\n    b = .5,\n    sigPN = .1,\n    sigOE = .05\n)\n\n## build the model\nstateSpaceModel &lt;- nimbleModel(stateSpaceCode,\n                              data = data,\n                              constants = constants,\n                              inits = inits,\n                              check = FALSE)\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\n  [Note] This model is not fully initialized. This is not an error.\n         To see which variables are not initialized, use model$initializeInfo().\n         For more information on model initialization, see help(modelInitialization).\n\n\n\n\nConstruct and run a bootstrap filter\nWe next construct a bootstrap filter to conduct inference on the latent states of our state space model. Note that the bootstrap filter, along with the auxiliary particle filter and the ensemble Kalman filter, treat the top-level parameters a, b, sigPN, and sigOE as fixed. Therefore, the bootstrap filter below will proceed as though a = 0, b = .5, sigPN = .1, and sigOE = .05, which are the initial values that were assigned to the top-level parameters.\nThe bootstrap filter takes as arguments the name of the model and the name of the latent state variable within the model. The filter can also take a control list that can be used to fine-tune the algorithm’s configuration.\n\n## build bootstrap filter and compile model and filter\nbootstrapFilter &lt;- buildBootstrapFilter(stateSpaceModel, nodes = 'x')\ncompiledList &lt;- compileNimble(stateSpaceModel, bootstrapFilter)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n## run compiled filter with 10,000 particles.  \n## note that the bootstrap filter returns an estimate of the log-likelihood of the model.\ncompiledList$bootstrapFilter$run(10000)\n\n[1] -28.13009\n\n\nParticle filtering algorithms in nimble store weighted samples of the filtering distribution of the latent states in the mvSamples modelValues object. Equally weighted samples are stored in the mvEWSamples object. By default, nimble only stores samples from the final time point.\n\n## extract equally weighted posterior samples of x[10]  and create a histogram\nposteriorSamples &lt;- as.matrix(compiledList$bootstrapFilter$mvEWSamples)\nhist(posteriorSamples)\n\n\n\n\n\n\n\n\nThe auxiliary particle filter and ensemble Kalman filter can be constructed and run in the same manner as the bootstrap filter.\n\n\nConduct inference on top-level parameters using particle MCMC\nParticle MCMC can be used to conduct inference on the posterior distribution of both the latent states and any top-level parameters of interest in a state space model. The particle marginal Metropolis-Hastings sampler can be specified to jointly sample the a, b, sigPN, and sigOE top level parameters within nimble’s MCMC framework as follows:\n\n## create MCMC specification for the state space model\nstateSpaceMCMCconf &lt;- configureMCMC(stateSpaceModel, nodes = NULL)\n\n===== Monitors =====\nthin = 1: a, b, sigOE, sigPN\n===== Samplers =====\n(no samplers assigned)\n===== Comments =====\n\n\n  [Warning] No samplers assigned for 14 nodes, use conf$getUnsampledNodes() for node names.\n\n## add a block pMCMC sampler for a, b, sigPN, and sigOE \nstateSpaceMCMCconf$addSampler(target = c('a', 'b', 'sigPN', 'sigOE'),\n                              type = 'RW_PF_block', control = list(latents = 'x'))\n\n## build and compile pMCMC sampler\nstateSpaceMCMC &lt;- buildMCMC(stateSpaceMCMCconf)\ncompiledList &lt;- compileNimble(stateSpaceModel, stateSpaceMCMC, resetFunctions = TRUE)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n## run compiled sampler for 5000 iterations\ncompiledList$stateSpaceMCMC$run(5000)\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\n## create trace plots for each parameter\npar(mfrow = c(2,2))\nposteriorSamps &lt;- coda::as.mcmc(as.matrix(compiledList$stateSpaceMCMC$mvSamples))\ncoda::traceplot(posteriorSamps[,'a'], ylab = 'a')\ncoda::traceplot(posteriorSamps[,'b'], ylab = 'b')\ncoda::traceplot(posteriorSamps[,'sigPN'], ylab = 'sigPN')\ncoda::traceplot(posteriorSamps[,'sigOE'], ylab = 'sigOE')\n\n\n\n\n\n\n\n\nThe above RW_PF_block sampler uses a multivariate normal proposal distribution to sample vectors of top-level parameters. To sample a scalar top-level parameter, use the RW_PF sampler instead."
  },
  {
    "objectID": "blog/version-0-11-1-of-nimble-released.html",
    "href": "blog/version-0-11-1-of-nimble-released.html",
    "title": "Version 0.11.1 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nVersion 0.11.1 is a bug fix release, fixing a bug that was introduced in Version 0.11.0 (which was released on April 17, 2021) that affected MCMC sampling in MCMCs using the posterior_predictive_branch sampler introduced in version 0.11.0. This sampler would be listed by name when the MCMC configuration object is created and would be assigned to any set of multiple nodes that (as a group of nodes) have no data dependencies and are therefore sampled as a group from their predictive distributions.\nFor those currently using version 0.11.0, please update your version of NIMBLE. For users currently using other versions, this release won’t directly affect you, but we generally encourage you to update as we release new versions."
  },
  {
    "objectID": "blog/version-1-0-1-of-nimble-released-fixing-a-bug-in-version-1-0-0-affecting-certain-models.html",
    "href": "blog/version-1-0-1-of-nimble-released-fixing-a-bug-in-version-1-0-0-affecting-certain-models.html",
    "title": "Version 1.0.1 of NIMBLE released, fixing a bug in version 1.0.0 affecting certain models",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nVersion 1.0.1 follows shortly after 1.0.0 and fixes an issue and a bug introduced in version 1.0.0 causing data to be set incorrectly in certain models.\nBoth cases occur only when a variable (e.g., x) contains both stochastic nodes (e.g. x[2] ~ &lt;some distribution&gt;) and either deterministic nodes (e.g. x[3] &lt;- &lt;some calculation&gt;) or right-hand-side-only nodes (e.g. x[4] appears only on the right-hand-side, like an explanatory value).\nThe issue involves a change of behavior (relative to previous nimble versions) when both setting data values for some nodes and initial values for other nodes within the same variable (that satisfies the previous condition). Data values for right-hand-side-only nodes were replaced by initial values (inits) if both were provided. Version 1.0.1 reverts to previous behavior that data values are not replaced by initial values in that situation.\nThe bug involves models where (for a variable satisfying the previous condition) not every scalar element within the variable is used as a node and some of the nodes in the variable are data. In that situation, data values may be set incorrectly. This could typically occur in models with autoregressive structure directly on some data nodes (such as may be the case for capture-recapture models involving many individual capture histories within the same variable, indexed by individual and time, with some individuals not present for the entire time series, resulting in unused scalar elements of the variable).\nPlease see the release notes on our website for more details."
  },
  {
    "objectID": "blog/version-0-10-1-of-nimble-released.html",
    "href": "blog/version-0-10-1-of-nimble-released.html",
    "title": "Version 0.10.1 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nWe’ve released version 0.10.1. Version 0.10.1 is primarily a bug fix release:\n\nIn particular, it fixes a bug in retrieving parameter values from distributions that was introduced in version 0.10.0. The bug can cause incorrect behavior of conjugate MCMC samplers under certain model structures (such as particular state-space models), so we strongly encourage users to upgrade to 0.10.1.\n\nIn addition, version 0.10.1 restricts use of WAIC to the conditional version of WAIC (conditioning on all parameters directly involved in the likelihood). Previous versions of NIMBLE gave incorrect results when not conditioning on all parameters directly involved in the likelihood (i.e., when not monitoring all such parameters). In a future version of NIMBLE we plan to make a number of improvements to WAIC, including allowing use of marginal versions of WAIC, where the WAIC calculation integrates over random effects.\n\nPlease see the release notes on our website for more details."
  },
  {
    "objectID": "blog/version-0-5-released.html",
    "href": "blog/version-0-5-released.html",
    "title": "Version 0.5 released!",
    "section": "",
    "text": "We’ve just released the next major version of NIMBLE.\nChanges include\n\nmore efficient computations for conjugate sampling,\nadditional automated checking of BUGS syntax to improve NIMBLE’s warning/error messages,\nnew API functionality to allow the use of syntax such as model$calculate(), etc. (syntax such as calculate(model) still works),\nnew API functionality for MCMC sampler specification,\nimprovements in speed and memory use in building models,\naddition of forwardsolve, backsolve, and solve to the NIMBLE DSL, and\na variety of other items.\n\nMore details in the NEWS file that accompanies the package.\nWe anticipate being on CRAN in coming weeks and a next release soon that will include a full suite of sequential Monte Carlo (i.e., particle filtering) algorithms."
  },
  {
    "objectID": "blog/nimble-virtual-short-course-january-4-6-2023.html",
    "href": "blog/nimble-virtual-short-course-january-4-6-2023.html",
    "title": "NIMBLE virtual short course, January 4-6, 2023",
    "section": "",
    "text": "We’ll be holding a virtual training workshop on NIMBLE, January 4-6, 2023 from 8 am to 1 pm US Pacific (California) time each day. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nRecently we added support for automatic differentiation (AD) to NIMBLE in a beta release, and the workshop will cover NIMBLE’s AD capabilities in detail.\nThe workshop will cover the following material:\n\nthe basic concepts and workflows for using NIMBLE and converting BUGS or JAGS models to work in NIMBLE.\noverview of different MCMC sampling strategies and how to use them in NIMBLE, including Hamiltonian Monte Carlo (HMC).\nwriting new distributions and functions for more flexible modeling and more efficient computation.\ntips and tricks for improving computational efficiency.\nusing advanced model components, including Bayesian non-parametric distributions (based on Dirichlet process priors), conditional auto-regressive (CAR) models for spatially correlated random fields, Laplace approximation, and reversible jump samplers for variable selection.\nan introduction to programming new algorithms in NIMBLE.\nuse of automatic differentiation (AD) in algorithms.\ncalling R and compiled C++ code from compiled NIMBLE models or functions.\n\nIf you are interested in attending, please pre-register. Registration fees will be $125 (regular) or $50 (student). We are also offering a process (see the pre-registration form) for students to request a fee waiver.\nThe workshop will assume attendees have a basic understanding of hierarchical/Bayesian models and MCMC, the BUGS (or JAGS) model language, and some familiarity with R."
  },
  {
    "objectID": "blog/nimble-paper-in-journal-of-computational-and-graphical-statistics.html",
    "href": "blog/nimble-paper-in-journal-of-computational-and-graphical-statistics.html",
    "title": "NIMBLE paper in Journal of Computational and Graphical Statistics",
    "section": "",
    "text": "Our paper giving an overview on the rationale and design of NIMBLE has appeared online in accepted manuscript form at the Journal of Computational and Graphical Statistics. You can get it here."
  },
  {
    "objectID": "blog/nimble-is-hiring-a-programmer.html",
    "href": "blog/nimble-is-hiring-a-programmer.html",
    "title": "NIMBLE is hiring a programmer",
    "section": "",
    "text": "This position includes work to harness parallel processing and automatic differentiation, to generate interfaces with other languages such as Python, to improve NIMBLE’s scope and efficiency for large statistical models, and to build other new features into NIMBLE.\nThe work will involve programming in R and C++, primarily designing and implementing software involving automated generation of C++ code for class and function definitions, parallel computing, use of external libraries for automatic differentiation and linear algebra, statistical algorithms and related problems. The position will also involve writing documentation and following good open-source software practices.\nSee here to apply."
  },
  {
    "objectID": "blog/beta-version-of-nimble-with-automatic-differentiation-including-hmc-sampling-and-laplace-approximation.html",
    "href": "blog/beta-version-of-nimble-with-automatic-differentiation-including-hmc-sampling-and-laplace-approximation.html",
    "title": "Beta version of NIMBLE with automatic differentiation, including HMC sampling and Laplace approximation",
    "section": "",
    "text": "We’re excited to announce that NIMBLE now supports automatic differentiation (AD), also known as algorithmic differentiation, in a beta version available on our website. In this beta version, NIMBLE now provides:\n\nHamiltonian Monte Carlo (HMC) sampling for an entire parameter vector or arbitrary subsets of the parameter vector (i.e., combined with other samplers for the remaining parameters).\nLaplace approximation for approximate integration over latent states in a model, allowing maximum likelihood estimation and MCMC based on the marginal likelihood (via the RW_llFunction samplers).\nThe ability for users and algorithm developers to write nimbleFunctions that calculate derivatives of functions, including many but not all mathematical operations that are supported in the NIMBLE language.\n\nWe’re making this beta release available to allow our users to test and evaluate the AD functionality and the new algorithms, but it is not recommended for production use at this stage. So please give it a try, and let us know of any problems or suggestions you have, either via the nimble-users list, bug reports to our GitHub repository, or email to nimble.stats@gmail.com.\nYou can download the beta version and view an [extensive draft manual]https://r-nimble.org/ADuserManual_draft/chapter_AD.html) for the AD functionality.\nWe plan to release this functionality in the next NIMBLE release on CRAN in the coming months."
  },
  {
    "objectID": "blog/nimble-short-course-at-march-enar-meeting-in-nashville.html",
    "href": "blog/nimble-short-course-at-march-enar-meeting-in-nashville.html",
    "title": "NIMBLE short course at March ENAR meeting in Nashville",
    "section": "",
    "text": "We’ll be holding a half-day short course on NIMBLE on March 22 at the ENAR meeting in Nashville, Tennessee.\nThe annual ENAR meeting is a major biostatistics conference, sponsored by the eastern North American region of the International Biometric Society.\nThe short course will focus on usage of NIMBLE in applied statistics and is titled:\n“Programming with hierarchical statistical models:\nUsing the BUGS-compatible NIMBLE system for MCMC and more”\nMore details on the conference and the short course are available at the conference website."
  },
  {
    "objectID": "blog/version-0-9-0-of-nimble-released.html",
    "href": "blog/version-0-9-0-of-nimble-released.html",
    "title": "Version 0.9.0 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC). Version 0.9.0 provides some new features as well as providing a variety of speed improvements, better output handling, and bug fixes.\nNew features and bug fixes include:\n\nadded an iterated filtering 2 (IF2) algorithm (a sequential Monte Carlo (SMC) method) for parameter estimation via maximum likelihood;\nfixed several bugs in our SMC algorithms;\nimproved the speed of MCMC configuration;\nimproved the user interface for interacting with the MCMC configuration; and\nimproved our conjugacy checking system to detect some additional cases of conjugacy.\n\nPlease see the release notes on our website for more details."
  },
  {
    "objectID": "blog/two-day-workshop-flexible-programming-of-mcmc-and-other-methods-for-hierarchical-and-bayesian-models.html",
    "href": "blog/two-day-workshop-flexible-programming-of-mcmc-and-other-methods-for-hierarchical-and-bayesian-models.html",
    "title": "Two day workshop: Flexible programming of MCMC and other methods for hierarchical and Bayesian models",
    "section": "",
    "text": "We’ll be giving a two day workshop at the 43rd Annual Summer Institute of Applied Statistics at Brigham Young University (BYU) in Utah, June 19-20, 2018.\nAbstract is below, and registration and logistics information can be found here.\nThis workshop provides a hands-on introduction to using, programming, and sharing Bayesian and hierarchical modeling algorithms using NIMBLE (r-nimble.org). In addition to learning the NIMBLE system, users will develop hands-on experience with various computational methods. NIMBLE is an R-based system that allows one to fit models specified using BUGS/JAGS syntax but with much more flexibility in defining the statistical model and the algorithm to be used on the model. Users operate from within R, but NIMBLE generates C++ code for models and algorithms for fast computation. I will open with an overview of creating a hierarchical model and fitting the model using a basic MCMC, similarly to how one can use WinBUGS, JAGS, and Stan. I will then discuss how NIMBLE allows the user to modify the MCMC – changing samplers and specifying blocking of parameters. Next I will show how to extend the BUGS syntax with user-defined distributions and functions that provide flexibility in specifying a statistical model of interest. With this background we can then explore the NIMBLE programming system, which allows one to write new algorithms not already provided by NIMBLE, including new MCMC samplers, using a subset of the R language. I will then provide examples of non-MCMC algorithms that have been programmed in NIMBLE and how algorithms can be combined together, using the example of a particle filter embedded within an MCMC. We will see new functionality in NIMBLE that allows one to fit Bayesian nonparametric models and spatial models. I will close with a discussion of how NIMBLE enables sharing of new methods and reproducibility of research. The workshop will include a number of breakout periods for participants to use and program MCMC and other methods, either on example problems or problems provided by participants. In addition, participants will see NIMBLE’s flexibility in action in several real problems."
  },
  {
    "objectID": "blog/version-0-4-1-released.html",
    "href": "blog/version-0-4-1-released.html",
    "title": "Version 0.4-1 released!",
    "section": "",
    "text": "We’ve just released version 0.4-1, a minor release that fixes some logistical issues and adds a bit of functionality to our MCMC engine.\nChanges as of Version 0.4-1 include:\n\nadded an elliptical slice sampler to the MCMC engine,\nfixed bug preventing use of nimbleFunctions in packages depending on NIMBLE, and\nreduced C++ compiler warnings on Windows during use of compileNimble."
  },
  {
    "objectID": "blog/nimble-in-person-short-course-june-1-3-lisbon-portugal.html",
    "href": "blog/nimble-in-person-short-course-june-1-3-lisbon-portugal.html",
    "title": "NIMBLE in-person short course, June 1-3, Lisbon, Portugal",
    "section": "",
    "text": "We’ll be holding a in-person training workshop on NIMBLE, June 1-3, 2022, in Lisbon, Portugal, sponsored by the Centro de Estatística e Aplicações at the Universidade Lisboa (CEAUL).\nNIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nMore details and registration are available at the workshop website. No previous NIMBLE experience is required, but the workshop will assume some familiarity with hierarchical models, Markov chain Monte Carlo (MCMC), and R."
  },
  {
    "objectID": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html",
    "href": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html",
    "title": "Quick guide for converting from JAGS or BUGS to NIMBLE",
    "section": "",
    "text": "Converting to NIMBLE from JAGS, OpenBUGS or WinBUGS"
  },
  {
    "objectID": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html#main-steps-for-converting-existing-code",
    "href": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html#main-steps-for-converting-existing-code",
    "title": "Quick guide for converting from JAGS or BUGS to NIMBLE",
    "section": "Main steps for converting existing code",
    "text": "Main steps for converting existing code\nThese steps assume you are familiar with running WinBUGS, OpenBUGS or JAGS through an R package such as R2WinBUGS, R2jags, rjags, or jagsUI.\n\nWrap your model code in nimbleCode({}), directly in R.\n\nThis replaces the step of writing or generating a separate file containing the model code.\nAlternatively, you can read standard JAGS- and BUGS-formatted code and data files using\nreadBUGSmodel.\n\nProvide information about missing or empty indices\n\nExample: If x is a matrix, you must write at least x[,] to show it has two dimensions.\nIf other declarations make the size of x clear, x[,] will work in some circumstances.\nIf not, either provide index ranges (e.g. x[1:n, 1:m]) or use the dimensions argument to nimbleModel to provide the sizes in each dimension.\n\nChoose how you want to run MCMC.\n\nUse nimbleMCMC() as the just-do-it way to run an MCMC. This will take all steps to\nset up and run an MCMC using NIMBLE’s default configuration.\nTo use NIMBLE’s full flexibility: build the model, configure and build the MCMC, and compile both the model and MCMC. Then run the MCMC via runMCMC or by calling the run function of the compiled MCMC. See the NIMBLE User Manual to learn more about what you can do.\n\n\nSee below for a list of some more nitty-gritty additional steps you may need to consider for some models."
  },
  {
    "objectID": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html#example-an-animal-abundance-model",
    "href": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html#example-an-animal-abundance-model",
    "title": "Quick guide for converting from JAGS or BUGS to NIMBLE",
    "section": "Example: An animal abundance model",
    "text": "Example: An animal abundance model\nThis example is adapted from Chapter 6, Section 6.4 of Applied Hierarchical Modeling in Ecology: Analysis of distribution, abundance and species richness in R and BUGS. Volume I: Prelude and Static Models by Marc Kéry and J. Andrew Royle (2015, Academic Press). The book’s web site provides code for its examples.\n\nOriginal code\nThe original model code looks like this:\n\ncat(file = \"model2.txt\",\"\nmodel {\n# Priors\nfor(k in 1:3){                # Loop over 3 levels of hab or time factors\n   alpha0[k] ~ dunif(-10, 10) # Detection intercepts\n   alpha1[k] ~ dunif(-10, 10) # Detection slopes\n   beta0[k] ~ dunif(-10, 10)  # Abundance intercepts\n   beta1[k] ~ dunif(-10, 10)  # Abundance slopes\n}\n\n# Likelihood\n# Ecological model for true abundance\nfor (i in 1:M){\n   N[i] ~ dpois(lambda[i])\n   log(lambda[i]) &lt;- beta0[hab[i]] + beta1[hab[i]] * vegHt[i]\n   # Some intermediate derived quantities\n   critical[i] &lt;- step(2-N[i])# yields 1 whenever N is 2 or less\n   z[i] &lt;- step(N[i]-0.5)     # Indicator for occupied site\n   # Observation model for replicated counts\n   for (j in 1:J){\n      C[i,j] ~ dbin(p[i,j], N[i])\n      logit(p[i,j]) &lt;- alpha0[j] + alpha1[j] * wind[i,j]\n   }\n}\n\n# Derived quantities\nNocc &lt;- sum(z[])         # Number of occupied sites among sample of M\nNtotal &lt;- sum(N[])       # Total population size at M sites combined\nNhab[1] &lt;- sum(N[1:33])  # Total abundance for sites in hab A\nNhab[2] &lt;- sum(N[34:66]) # Total abundance for sites in hab B\nNhab[3] &lt;- sum(N[67:100])# Total abundance for sites in hab C\nfor(k in 1:100){         # Predictions of lambda and p ...\n   for(level in 1:3){    #    ... for each level of hab and time factors\n      lam.pred[k, level] &lt;- exp(beta0[level] + beta1[level] * XvegHt[k])\n      logit(p.pred[k, level]) &lt;- alpha0[level] + alpha1[level] * Xwind[k]\n   }\n}\nN.critical &lt;- sum(critical[]) # Number of populations with critical size\n}\")\n\n\n\nBrief summary of the model\nThis is known as an “N-mixture” model in ecology. The details aren’t really important for illustrating the mechanics of converting this model to NIMBLE, but here is a brief summary anyway. The latent abundances N[i] at sites i = 1...M are assumed to follow a Poisson. The j-th count at the i-th site, C[i, j], is assumed to follow a binomial with detection probability p[i, j]. The abundance at each site depends on a habitat-specific intercept and coefficient for vegetation height, with a log link. The detection probability for each sampling occasion depends on a date-specific intercept and coefficient for wind speed. Kéry and Royle concocted this as a simulated example to illustrate the hierarchical modeling approaches for estimating abundance from count data on repeated visits to multiple sites."
  },
  {
    "objectID": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html#nimble-version-of-the-model-code",
    "href": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html#nimble-version-of-the-model-code",
    "title": "Quick guide for converting from JAGS or BUGS to NIMBLE",
    "section": "NIMBLE version of the model code",
    "text": "NIMBLE version of the model code\nHere is the model converted for use in NIMBLE. In this case, the only changes to the code are to insert some missing index ranges (see comments).\n\nlibrary(nimble)\n\n\nSection6p4_code &lt;- nimbleCode({\n  # Priors\n  for(k in 1:3) {                # Loop over 3 levels of hab or time factors\n    alpha0[k] ~ dunif(-10, 10) # Detection intercepts\n    alpha1[k] ~ dunif(-10, 10) # Detection slopes\n    beta0[k] ~ dunif(-10, 10)  # Abundance intercepts\n    beta1[k] ~ dunif(-10, 10)  # Abundance slopes\n  }\n\n  # Likelihood\n  # Ecological model for true abundance\n  for (i in 1:M){\n    N[i] ~ dpois(lambda[i])\n    log(lambda[i]) &lt;- beta0[hab[i]] + beta1[hab[i]] * vegHt[i]\n    # Some intermediate derived quantities\n    critical[i] &lt;- step(2-N[i])# yields 1 whenever N is 2 or less\n    z[i] &lt;- step(N[i]-0.5)     # Indicator for occupied site\n    # Observation model for replicated counts\n    for (j in 1:J){\n      C[i,j] ~ dbin(p[i,j], N[i])\n      logit(p[i,j]) &lt;- alpha0[j] + alpha1[j] * wind[i,j]\n      }\n  }\n\n  # Derived quantities; unnececssary when running for inference purpose\n  # NIMBLE: We have filled in indices in the next two lines.\n  Nocc &lt;- sum(z[1:100])         # Number of occupied sites among sample of M\n  Ntotal &lt;- sum(N[1:100])       # Total population size at M sites combined\n  Nhab[1] &lt;- sum(N[1:33])  # Total abundance for sites in hab A\n  Nhab[2] &lt;- sum(N[34:66]) # Total abundance for sites in hab B\n  Nhab[3] &lt;- sum(N[67:100])# Total abundance for sites in hab C\n  for(k in 1:100){         # Predictions of lambda and p ...\n    for(level in 1:3){    #    ... for each level of hab and time factors\n      lam.pred[k, level] &lt;- exp(beta0[level] + beta1[level] * XvegHt[k])\n      logit(p.pred[k, level]) &lt;- alpha0[level] + alpha1[level] * Xwind[k]\n      }\n    }\n  # NIMBLE: We have filled in indices in the next line. \n  N.critical &lt;- sum(critical[1:100]) # Number of populations with critical size\n})"
  },
  {
    "objectID": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html#simulated-data",
    "href": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html#simulated-data",
    "title": "Quick guide for converting from JAGS or BUGS to NIMBLE",
    "section": "Simulated data",
    "text": "Simulated data\nTo carry this example further, we need some simulated data. Kéry and Royle provide separate code to do this. With NIMBLE we could use the model itself to simulate data rather than writing separate simulation code. But for our goals here, we simply copy Kéry and Royle’s simulation code, and we compact it somewhat:\n\n# Code from Kery and Royle (2015)\n# Choose sample sizes and prepare obs. data array y\nset.seed(1)                   # So we all get same data set\nM &lt;- 100                      # Number of sites\nJ &lt;- 3                        # Number of repeated abundance measurements\nC &lt;- matrix(NA, nrow = M, ncol = J) # to contain the observed data\n\n# Create a covariate called vegHt\nvegHt &lt;- sort(runif(M, -1, 1)) # sort for graphical convenience\n\n# Choose parameter values for abundance model and compute lambda\nbeta0 &lt;- 0                    # Log-scale intercept\nbeta1 &lt;- 2                    # Log-scale slope for vegHt\nlambda &lt;- exp(beta0 + beta1 * vegHt) # Expected abundance\n\n# Draw local abundance\nN &lt;- rpois(M, lambda)\n\n# Create a covariate called wind\nwind &lt;- array(runif(M * J, -1, 1), dim = c(M, J))\n\n# Choose parameter values for measurement error model and compute detectability\nalpha0 &lt;- -2                        # Logit-scale intercept\nalpha1 &lt;- -3                        # Logit-scale slope for wind\np &lt;- plogis(alpha0 + alpha1 * wind) # Detection probability\n\n# Take J = 3 abundance measurements at each site\nfor(j in 1:J) {\n  C[,j] &lt;- rbinom(M, N, p[,j])\n}\n\n# Create factors\ntime &lt;- matrix(rep(as.character(1:J), M), ncol = J, byrow = TRUE)\nhab &lt;- c(rep(\"A\", 33), rep(\"B\", 33), rep(\"C\", 34))  # assumes M = 100\n\n# Bundle data\n# NIMBLE: For full flexibility, we could separate this list\n#         into constants and data lists.  For simplicity we will keep\n#         it as one list to be provided as the \"constants\" argument.\n#         See comments about how we would split it if desired.\nwin.data &lt;- list(\n    ## NIMBLE: C is the actual data\n    C = C,\n    ## NIMBLE: Covariates can be data or constants\n    ##         If they are data, you could modify them after the model is built\n    wind = wind,\n    vegHt = vegHt,\n    XvegHt = seq(-1, 1,, 100), # Used only for derived quantities\n    Xwind = seq(-1, 1,,100),   # Used only for derived quantities\n    ## NIMBLE: The rest of these are constants, needed for model definition\n    ## We can provide them in the same list and NIMBLE will figure it out.\n    M = nrow(C),\n    J = ncol(C),\n    hab = as.numeric(factor(hab))\n)"
  },
  {
    "objectID": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html#initial-values",
    "href": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html#initial-values",
    "title": "Quick guide for converting from JAGS or BUGS to NIMBLE",
    "section": "Initial values",
    "text": "Initial values\nNext we need to set up initial values and choose parameters to monitor in the MCMC output. To do so we will again directly use Kéry and Royle’s code.\n\nNst &lt;- apply(C, 1, max)+1   # Important to give good inits for latent N\ninits &lt;- function() list(N = Nst,\n                         alpha0 = rnorm(3),\n                         alpha1 = rnorm(3),\n                         beta0 = rnorm(3),\n                         beta1 = rnorm(3))\n\n# Parameters monitored\n# could also estimate N, bayesian counterpart to BUPs before: simply add \"N\" to the list\nparams &lt;- c(\"alpha0\", \"alpha1\", \"beta0\", \"beta1\", \"Nocc\", \"Ntotal\", \"Nhab\", \"N.critical\", \"lam.pred\", \"p.pred\")"
  },
  {
    "objectID": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html#run-mcmc-with-nimblemcmc",
    "href": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html#run-mcmc-with-nimblemcmc",
    "title": "Quick guide for converting from JAGS or BUGS to NIMBLE",
    "section": "Run MCMC with nimbleMCMC",
    "text": "Run MCMC with nimbleMCMC\nNow we are ready to run an MCMC in nimble. We will run only one chain, using the same settings as Kéry and Royle.\n\nsamples &lt;- nimbleMCMC(\n    code = Section6p4_code,\n    constants = win.data, ## provide the combined data & constants as constants\n    inits = inits,\n    monitors = params,\n    niter = 22000,\n    nburnin = 2000,\n    thin = 10)\n\nDefining model\n\n\n  [Note] Using 'C' (given within 'constants') as data.\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\nChecking model calculations\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\nrunning chain 1...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|"
  },
  {
    "objectID": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html#work-with-the-samples",
    "href": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html#work-with-the-samples",
    "title": "Quick guide for converting from JAGS or BUGS to NIMBLE",
    "section": "Work with the samples",
    "text": "Work with the samples\nFinally we want to look at our samples. NIMBLE returns samples as a simple matrix with named columns. There are numerous packages for processing MCMC output. If you want to use the coda package, you can convert a matrix to a coda mcmc object like this:\n\nlibrary(coda)\n\n\nAttaching package: 'coda'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    densplot\n\ncoda.samples &lt;- as.mcmc(samples)\n\nAlternatively, if you call nimbleMCMC with the argument samplesAsCodaMCMC = TRUE, the samples will be returned as a coda object.\nTo show that MCMC really happened, here is a plot of N.critical:\n\nplot(jitter(samples[, \"N.critical\"]), xlab = \"iteration\", ylab = \"N.critical\",\n     main = \"Number of populations with critical size\",\n     type = \"l\")"
  },
  {
    "objectID": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html#smaller-steps-you-may-need-for-converting-existing-code",
    "href": "blog/quick-guide-for-converting-from-jags-or-bugs-to-nimble.html#smaller-steps-you-may-need-for-converting-existing-code",
    "title": "Quick guide for converting from JAGS or BUGS to NIMBLE",
    "section": "Smaller steps you may need for converting existing code",
    "text": "Smaller steps you may need for converting existing code\nIf the main steps above aren’t sufficient, consider these additional steps when converting from JAGS, WinBUGS or OpenBUGS to NIMBLE.\n\nConvert any use of truncation syntax\n\ne.g. x ~ dnorm(0, tau) T(a, b) should be re-written as x ~ T(dnorm(0, tau), a, b).\nIf reading model code from a file using readBUGSmodel, the x ~ dnorm(0, tau) T(a, b) syntax will work.\n\nPossibly split the data into data and constants for NIMBLE.\n\nNIMBLE has a more general concept of data, so NIMBLE makes a distinction between data and constants.\nConstants are necessary to define the model, such as nsite in for(i in 1:nsite) {...} and constant vectors of factor indices (e.g. block in mu[block[i]]).\nData are observed values of some variables.\nAlternatively, one can provide a list of both constants and data for the constants argument to nimbleModel, and NIMBLE will try to determine which is which. Usually this will work, but when in doubt, try separating them.\n\nPossibly update initial values (inits).\n\nIn some cases, NIMBLE likes to have more complete inits than the other packages.\nIn a model with stochastic indices, those indices should have inits values.\nWhen using nimbleMCMC or runMCMC, inits can be a function, as in R packages for calling WinBUGS, OpenBUGS or JAGS. Alternatively, it can be a list.\nWhen you build a model with nimbleModel for more control than nimbleMCMC, you can provide inits as a list. This sets defaults that can be over-ridden with the inits argument to runMCMC."
  },
  {
    "objectID": "blog/new-nimble-cheatsheat-available.html",
    "href": "blog/new-nimble-cheatsheat-available.html",
    "title": "New NIMBLE cheatsheat available",
    "section": "",
    "text": "We have prepared a cheatsheet that summarizes NIMBLE’s functionality and syntax in a two-page format that follows the RStudio collection of cheatsheets. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nPlease see our documentation page for a link.\nSpecial thanks to Sally Paganin and Ben Goldstein for leading the development of the cheatsheet."
  },
  {
    "objectID": "blog/new-nimble-code-examples.html",
    "href": "blog/new-nimble-code-examples.html",
    "title": "New NIMBLE code examples",
    "section": "",
    "text": "We’ve posted a variety of new code examples for NIMBLE at https://r-nimble.org/examples.\nThese include some introductory code for getting started with NIMBLE, as well as examples of setting up specific kinds of models such as GLMMs, spatial models, item-response theory models, and a variety of models widely used in ecology. We also have examples of doing variable selection via reversible jump MCMC, using MCEM, and writing new distributions for use with NIMBLE.\nAnd if you have examples that you think might be useful to make available to others, please get in touch with us."
  },
  {
    "objectID": "blog/version-1-0-0-of-nimble-released-providing-automatic-differentiation-laplace-approximation-and-hmc-sampling.html",
    "href": "blog/version-1-0-0-of-nimble-released-providing-automatic-differentiation-laplace-approximation-and-hmc-sampling.html",
    "title": "Version 1.0.0 of NIMBLE released, providing automatic differentiation, Laplace approximation, and HMC sampling",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nVersion 1.0.0 provides substantial new functionality. This includes:\n\nA Laplace approximation algorithm that allows one to find the MLE for model parameters based on approximating the marginal likelihood in models with continuous random effects/latent process values.\nA Hamiltonian Monte Carlo (HMC) MCMC sampler implementing the NUTS algorithm (available in the newly-released nimbleHMC package).\nSupport in NIMBLE’s algorithm programming system to obtain derivatives of functions and arbitrary calculations within models.\nA parameter transformation system allowing algorithms to work in unconstrained parameter spaces when model parameters have constrained domains.\n\nThese are documented via the R help system and a new section at the end of our User Manual. We’re excited for users to try out the new features and let us know of their experiences. In particular, given these major additions to the NIMBLE system, we anticipate the possibility of minor glitches. The best place to reach out for support is still the nimble-users list.\nIn addition to the new functionality above, other enhancements and bug fixes include:\n\nFixing a bug (previously reported in a nimble-users message) giving incorrect results in NIMBLE’s cross-validation function (runCrossValidate) for all but the ‘predictive’ loss function for NIMBLE versions 0.10.0 – 0.13.2.\nFixing a bug in conjugacy checking causing incorrect identification of conjugate relationships in models with unusual uses of subsets, supersets, and slices of multivariate normal nodes.\nImproving control of the addSampler method for MCMC.\nImproving the WAIC system in a few small ways.\nEnhancing error trapping and warning messages.\n\nPlease see the NEWS file in the package source for more details.",
    "crumbs": [
      "Home",
      "Blog Posts",
      "Version 1.0.0 of NIMBLE released, providing automatic differentiation, Laplace approximation, and HMC sampling"
    ]
  },
  {
    "objectID": "blog/a-close-look-at-some-linear-model-mcmc-comparisons.html",
    "href": "blog/a-close-look-at-some-linear-model-mcmc-comparisons.html",
    "title": "A close look at some linear model MCMC comparisons",
    "section": "",
    "text": "This is our second blog post taking a careful look at some of the results posted in an arXiv manuscript by Beraha, Falco, and Guglielmi (BFG). They compare JAGS, Stan, and NIMBLE using four examples. In their results, each package performs best in at least one example.\nIn our previous post, we explained that they compared apples to oranges in the accelerated failure time (AFT) example. They gave Stan a different and easier problem than they gave JAGS and NIMBLE. When we gave NIMBLE the same problem, we saw that its MCMC performance was up to 45 times better than what they reported. We looked first at the AFT example because that’s where NIMBLE seemed to perform comparatively worst.\nIn this post we’re looking at the simple linear model example. It turns out that the models were written more efficiently for Stan than for JAGS and NIMBLE , because matrix multiplication was used for Stan but all scalar steps of matrix multiplication were written in JAGS and NIMBLE. JAGS and NIMBLE do support matrix multiplication and inner products. When we modify the models to also use matrix multiplication, NIMBLE’s MCMC performance with default samplers increases often by 1.2 to 3-fold but sometimes by 5 to &gt;10-fold over what was reported by BFG, as far as we can tell. This had to do with both raw computational efficiency and also MCMC samplers invoked by different ways to write the model code. Other issues are described below.\nBFG’s linear model examples explore different data sizes (n = 30, 100, 1000, or in one case 2000), different numbers of explanatory variables (4, 16, 30, 50 or 100), and different priors for the variance and/or coefficients (beta[i]s), all in a simple linear model. The priors included:\n\n“LM-C”: an inverse gamma prior for variance, which is used for both residual variance and variance of normal priors for beta[i]s (regression coefficients). This setup should offer conjugate sampling for both the variance parameter and the beta[i]s.\n“LM-C Bin”: the same prior as “LM-C”. This case has Bernoulli- instead of normally-distributed explanatory variables in the data simulations. It’s very similar to “LM-C”.\n“LM-WI”: A weakly informative (“WI”) prior for residual standard deviation using a truncated, scaled t-distribution. beta[i]s have a non-informative (sd = 100) normal prior.\n“LM-NI”: A non-informative (“NI”) flat prior for residual standard deviation. beta[i]s have a non-informative (sd = 100) normal prior.\n“LM-L”: A lasso (“L”) prior for beta[i]s. This uses a double-exponential prior for beta[i]s, with a parameter that itself follows an exponential prior. This prior is a Bayesian analog of the lasso for variable selection, so the scenarios used for this have large numbers of explanatory variables, with different numbers of them (z) set to 0 in the simulations. Residual variance has an inverse gamma prior.\n\nAgain, we are going to stick to NIMBLE here and not try to reproduce or explore results for JAGS or Stan.\nIn more detail, the big issues that jumped out from BFG’s code are:\n\nStan was given matrix multiplication for X %*% beta, while NIMBLE and JAGS were given code to do all of the element-by-element steps of matrix multiplication. Both NIMBLE and JAGS support matrix multiplication and inner products, so we think it is better and more directly comparable to use these features.\nFor the “LM-C” and “LM-C Bin” cases, the prior for the beta[i]s was given as a multivariate normal with a diagonal covariance matrix. It is better (and equivalent) to give each element a univariate normal prior.\n\nThere are two reasons that writing out matrix multiplication as they did is not a great way to code a model. The first is that it is just inefficient. For X that is N-by-p and beta that is p-by-1, there are N*p scalar multiplications and N summations of length p in the model code. Although somewhere in the computer those elemental steps need to be taken, they will be substantially faster if not broken up by hand-coding them. When NIMBLE generates (and then compiles) C++, it generates C++ for the Eigen linear algebra library, which gives efficient implementations of matrix operations.\nThe second reason, however, may be more important in this case. Using either matrix multiplication or inner products makes it easier for NIMBLE to determine that the coefficients (“beta[i]”s) in many of these cases have conjugate relationships that can be used for Gibbs sampling. The way BFG wrote the model revealed to us that we’re not detecting the conjugacy in this case. That’s something we plan to fix, but it’s not a situation that’s come before us yet. Detecting conjugacy in a graphical model — as written in the BUGS/JAGS/NIMBLE dialects of the BUGS language — involves symbolic algebra, so it’s difficult to catch all cases.\nThe reasons it’s better to give a set of univariate normal priors than a single multivariate normal are similar. It’s more computationally efficient, and it makes it easier to detect conjugacy.\nIn summary, they wrote the model inefficiently for NIMBLE and differently between packages, and we didn’t detect conjugacy for the way they wrote it. In the results below, the “better” results use matrix multiplication directly (in all cases) and use univariate normal priors instead of a multivariate normal (in the “LM-C” and “LM-C Bin” cases).\nIt also turns out that neither JAGS nor NIMBLE detects conjugacy for the precision parameter of the “LM-C” and “LM-C Bin” cases. (This is shown by list.samplers in rjags and configureMCMC in NIMBLE.) In NIMBLE, a summary of how conjugacy is determined is in Table 7.1 of our User Manual. It can be obtained by changing sd = sigma to var = sigmasq in one line of BFG’s code. In these examples, we found that this issue doesn’t make much different to MCMC efficiency, so we leave it as they coded it.\nBefore giving our results, we’ll make a few observations on BFG’s results, shown in their Table 2. One is that JAGS gives very efficient sampling for many of these cases, and that’s something we’ve seen before. Especially when conjugate sampling is available, JAGS does well. Next is that Stan and NIMBLE each do better than the other in some cases. As we wrote about in the previous post, BFG chose not to calculate what we see as the most relevant metric for comparison. That is the rate of generating effectively independent samples, the ESS/time, which we call MCMC efficiency. An MCMC system can be efficient by slowly generating well-mixed samples or by rapidly generating poorly-mixed samples. One has to make choices such as whether burn-in (or warmup) time is counted in the denominator, depending on exactly what is of interest. BFG reported only ESS/recorded iterations and total iterations/time. The product of these is a measure of ESS/time, scaled by a ratio of total iterations / recorded iterations.\nFor example, in the “LM-C” case with “N = 1000, p = 4”, Stan has (ESS/recorded iterations) * (total iterations/time) = 0.99 * 157=155, while NIMBLE has 0.14 * 1571=220. Thus in this case NIMBLE is generating effectively independent samples faster than Stan, because the faster computation out-weighs the poorer mixing. In other cases, Stan has higher ESS/time than NIMBLE. When BFG round ESS/recorded iterations to “1%” in some cases, the ESS/time is unknown up to a factor of 3 because 1% could be rounded from 0.50 or from 1.49. For most cases, Stan and NIMBLE are within a factor of 2 of each other, which is close. One case where Stan really stands out is the non-informative prior (LM-NI) with p&gt;n, but it’s worth noting that this is a statistically unhealthy case. With p&gt;n, parameters are not identifiable without the help of a prior. In the LM-NI case, the prior is uninformative, and the posteriors for beta[i]s are not much different than their priors.\nOne other result jumps out as strange from their Table 2. The run-time results for “LM-WI” (total iterations / time) are much, much slower than in other cases. For example, with N = 100 and p = 4, this case was only 2.6% (294 vs 11,000 ) as fast as the corresponding “LM-C” case. We’re not sure how that could make sense, so it was something we wanted to check.\nWe took all of BFG’s source code and organized it to be more fully reproducible. After our previous blog post, set.seed calls were added to their source code, so we use those. We also organize the code into functions and sets of runs to save and process together. We think we interpreted their code correctly, but we can’t be sure. For ESS estimation, we used coda::effectiveSize, but Stan and mcmcse are examples of packages with other methods, and we aren’t sure what BFG used. They thin by 2 and give average results for beta[i]s. We want to compare to their results, so we take those steps too.\nHere are the results:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBFG\n\n\nBetter code\n\nImprovement\n\n\n\n\n\n\nESS/Ns\nNit/t\nESS/t\nESS/Ns\nNit/t\nESS/t\nBetter by\n\n\n\nLM-C\n\n\n\n\n\n\n\n\n\n\nN=100, p=4\n0.15\n56122.45\n3738.90\n1.03\n23060.80\n10842.00\n2.90\n\n\n\nN=1000, p=4\n0.14\n9401.71\n609.97\n1.00\n2866.82\n1303.10\n2.14\n\n\n\nN=100, p=16\n0.04\n25345.62\n428.45\n0.95\n5555.56\n2396.00\n5.59\n\n\n\nN=1000, p=16\n0.03\n3471.13\n54.06\n1.00\n613.98\n278.53\n5.15\n\n\n\nN=2000, p=30\n0.01\n863.83\n5.52\n1.00\n137.60\n62.67\n11.35\n\n\n\nN=30, p=50\n0.00\n11470.28\n24.49\n0.07\n3869.15\n114.62\n4.68\n\n\n\nLM-C Bin\n\n\n\n\n\n\n\n\n\n\nN=100, p=4\n0.12\n61452.51\n3303.31\n0.52\n22916.67\n5384.40\n1.63\n\n\n\nN=1000, p=4\n0.10\n9945.75\n441.07\n0.47\n2857.14\n606.16\n1.37\n\n\n\nN=100, p=16\n0.04\n26699.03\n430.92\n0.49\n5530.42\n1223.25\n2.84\n\n\n\nN=1000, p=16\n0.03\n3505.42\n41.68\n0.55\n655.46\n163.59\n3.92\n\n\n\nN=30, p=50\n0.01\n11815.25\n44.01\n0.12\n3941.24\n211.66\n4.81\n\n\n\nLM-WI\n\n\n\n\n\n\n\n\n\n\nN=100, p=4\n0.38\n44117.65\n5595.82\n0.99\n22865.85\n7545.97\n1.35\n\n\n\nN=1000, p=4\n0.44\n4874.88\n709.03\n0.98\n2834.47\n929.87\n1.31\n\n\n\nN=100, p=16\n0.32\n11441.65\n1233.59\n0.94\n5845.67\n1837.45\n1.49\n\n\n\nN=1000, p=16\n0.42\n1269.14\n179.09\n1.00\n653.62\n217.22\n1.21\n\n\n\nLM-NI\n\n\n\n\n\n\n\n\n\n\nN=100, p=4\n0.37\n43604.65\n5415.31\n1.01\n22935.78\n7749.15\n1.43\n\n\n\nN=1000, p=4\n0.43\n5613.77\n804.61\n1.06\n2751.28\n974.50\n1.21\n\n\n\nN=100, p=16\n0.31\n12386.46\n1298.40\n0.94\n6134.97\n1932.29\n1.49\n\n\n\nN=1000, p=16\n0.43\n1271.83\n182.56\n1.02\n625.94\n212.29\n1.16\n\n\n\nN=30, p=50\n0.01\n8581.24\n14.45\n0.01\n3755.63\n13.80\n0.96\n\n\n\nLM-Lasso\n\n\n\n\n\n\n\n\n\n\nN=100, p=16, z=0\n0.33\n10881.39\n905.68\n0.33\n17730.50\n1475.74\n1.63\n\n\n\nN=1000, p=16, z=0\n0.44\n1219.59\n132.65\n0.44\n2129.02\n231.57\n1.75\n\n\n\nN=1000, p=30, z=2\n0.41\n552.30\n56.81\n0.41\n942.42\n96.94\n1.71\n\n\n\nN=1000, p=30, z=15\n0.42\n540.51\n56.91\n0.42\n941.97\n99.17\n1.74\n\n\n\nN=1000, p=30, z=28\n0.42\n541.01\n56.27\n0.42\n970.73\n100.97\n1.79\n\n\n\nN=1000, p=100, z=2\n0.36\n77.75\n7.06\n0.36\n141.22\n12.83\n1.82\n\n\n\nN=1000, p=100, z=50\n0.37\n74.89\n6.89\n0.37\n141.32\n13.01\n1.89\n\n\n\nN=1000, p=100, z=98\n0.39\n74.78\n7.37\n0.39\n142.60\n14.05\n1.91\n\n\n\n\nThe “BFG” columns gives results from the same way BFG ran the cases, we think. The “ESS/Ns” is the same as their \\(\\varepsilon_{\\eta}\\). ESS is averaged for the beta parameters. Ns is the number of saved samples, after burn-in and thinning. Their code gives different choices of burn-in and saved iterations for the different cases, and we used their settings. The “Nit/t” is the total number of iterations (including burn-in) divided by total computation time. The final column, which BFG don’t give, is “ESS/t”, what we call MCMC efficiency. Choice of time in the denominator includes burn-in time (the same as for “Nit/t”).\nThe “Better code” columns give results when we write the code with matrix multiplication and, for “LM-C” and “LM-C Bin”, univariate priors. It is almost as efficient to write the code using an inner product for each mu[i] instead of matrix multiplication for all mu[i] together. Matrix multiplication makes sense when all of the inputs that might changes (in this case, beta[i]s updated by MCMC) require all of the same likelihood contributions to be calculated from the result (in this case, all y[i]s from all mu[i]s). Either way of coding the model makes it easier for NIMBLE to sample the beta[i]s with conjugate samplers and avoids the inefficiency of putting every scalar step into the model code.\nThe “Better by” column gives the ratio of “ESS/t” for the “Better code” to “ESS/t” for the BFG code. This is the factor by which the “Better code” version improves upon the “BFG” version.\nWe can see that writing better code often give improvements of say 1.2-3.0 fold, and sometimes of 5-10+ fold in ESS/time. These improvements — which came from writing the model in NIMBLE more similarly to how it was written in Stan — often put NIMBLE closer to or faster than Stan in various cases, and sometimes faster than JAGS with BFG’s version of the model. We’re sticking to NIMBLE, so we haven’t run JAGS with the better-written code to see how much it improves. Stan still shines for p&gt;n, and JAGS is still really good at linear models. The results show that, for the first four categories (above the LM-Lasso results), NIMBLE also can achieve very good mixing (near 100% ESS/saved samples), with the exception of the p&gt;n cases. BFG’s results showed worse mixing for NIMBLE in those cases.\nWe can also see that BFG’s computation-time results for “LM-WI” (which we noted above) do appear to be really weird. In our results, that case ran somewhat slower than the LM-C cases with matching N and p, but not around 40-times slower as reported by BFG. We won’t make detailed comparisons of LM-WI cases because we’re not confident BFG’s results are solid for these.\nAs a example, take LM-C, with the simplest being “N=100, p=4” and the hardest being “N=2000, p=30”, not counting the p&gt;n case. For the simplest case, BFG report that JAGS is about 2.1 times more efficient than Stan and about 2.4 times more efficient than NIMBLE. (E.g., the 2.1 comes from (100 * 3667)/(96 * 1883), reading numbers from their Table 2.) By writing the model in the simpler, better way in NIMBLE, we see a 2.9 fold gain in efficiency. This would make NIMBLE more efficient than Stan. We did not also re-run JAGS with the better code. For the hardest case, BFG report JAGS being about 1.8 times more efficient than Stan and about 2.1 times more efficient than NIMBLE. In that case coding the model better makes NIMBLE 11.4 times more efficient, apparently more efficient than Stan and possibly than JAGS. Again, we did not run JAGS with and without the coding improvement. As a final example, in one of the middle LM-L cases, with N = 1000, p = 30, and 15 of those truly 0, Stan is reported by BFG to be about 3.6 times more efficient than NIMBLE. The better-coded model improves NIMBLE by about 1.7-fold, leaving it still behind Stan but only by about half as much.\nWe ran these comparisons on a MacBook Pro (2.4 GHz 8-Core Intel Core i9). It looks like this was roughly 5 times faster than the computer on which BFG ran.\nInspection of traceplots revealed that the traceplots for the variance in the 5th and 6th “LM-C” cases had not yet converged in the “BFG” version of the model. More burn-in iterations would be needed. This goes hand-in-hand with the recognition that NIMBLE benefits from good initial values. In a real analysis, if a long burn-in was observed, a practical step would be to provide better initial values for the next run. Applied analysis always involves multiple MCMC runs as one gets things working and checked. With the “better code” version, the chains do appear to have converged.\nAt this point we should highlight that there isn’t only one version of NIMBLE’s MCMC performance. NIMBLE’s MCMC system is highly configurable, and its default samplers are just one possible choice among many. When putting real effort into boosting performance for hard models, we’ve seen improvements by 1-3 orders of magnitude (here, here and here). In non-conjugate cases where JAGS performs well, it is worth noting that JAGS uses a lot of slice samplers, and those can also be configured in NIMBLE. (But the cases here use lots of conjugate samplers, rather than slice samplers.)\nThe takeaway is that we don’t know why BFG gave Stan the benefit of matrix multiplication but didn’t do so for JAGS or NIMBLE, and doing so makes a substantial difference for NIMBLE. Also, we see more conjugacy cases to catch in our symbolic processing of model relationships."
  },
  {
    "objectID": "blog/version-0-12-1-of-nimble-released.html",
    "href": "blog/version-0-12-1-of-nimble-released.html",
    "title": "Version 0.12.1 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC and SMC).\nVersion 0.12.1, in combination with version 0.12.0 (which was released just last week), provides a variety of new functionality (in particular enhanced WAIC functionality and adding the LKJ distribution) plus bug fixes affecting MCMC in specific narrow cases described below and that warrant upgrading for some users. The changes include:\n\nCompletely revamping WAIC in NIMBLE, creating an online version that does not require any particular variable monitors. The new WAIC can calculate conditional or marginal WAIC and can group data nodes into joint likelihood terms if desired. In addition there is a new calculateWAIC() function that will calculate the basic conditional WAIC from MCMC output without having to enable the WAIC when creating the MCMC.\nAdding the LKJ distribution, useful for prior distributions for correlation matrices, along with random walk samplers for them. These samplers operate in an unconstrained transformed parameter space and are assigned by default during MCMC configuration.\nFixing a bug introduced in conjugacy processing in version 0.11.0 that causes incorrect MCMC sampling only in specific cases. The impacted cases have terms of the form a[i] + x[i] * beta (or more simply x[i] * beta), with beta subject to conjugate sampling and either (i) x provided via NIMBLE’s constants argument and x[1] == 1 or (ii) a provided via NIMBLE’s constants argument and a[1] == 0.\nFixing an error in the sampler for the proper CAR distribution (dcar_proper) that gives incorrect MCMC results when the mean of the proper CAR is not the same value for all locations, e.g., when embedding covariate effects directly in the mu parameter of the dcar_proper distribution.\nFixing isData(\"y\") to return TRUE whenever any elements of a multivariate data node (y) are flagged as data. As a result, attempting to carry out MCMC on the non-data elements will now fail. Formerly if only some elements were flagged as data, isData would only check the first element, potentially leading to other elements that were flagged as data being overwritten.\nError trapping cases where a BNP model has a differing number of dependent stochastic nodes (e.g., observations) or dependent deterministic nodes per group of elements clustered jointly (using functionality introduced in version 0.10.0). Previously we were not error trapping this, and incorrect MCMC results would be obtained.\nImproving the formatting of standard logging messages."
  },
  {
    "objectID": "blog/nimble-is-hiring-a-programmer-2.html",
    "href": "blog/nimble-is-hiring-a-programmer-2.html",
    "title": "NIMBLE is hiring a programmer",
    "section": "",
    "text": "The NIMBLE development team is hiring for a one-year programmer position. We are looking for someone with R and C++ experience. There is also a possibility of a part-time position. The deadline for full consideration is February 12. Application information is here: https://aprecruit.berkeley.edu/JPF02822."
  },
  {
    "objectID": "blog/version-1-3-0-of-nimble-released.html",
    "href": "blog/version-1-3-0-of-nimble-released.html",
    "title": "Version 1.3.0 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC, Laplace approximation, and SMC).\nVersion 1.3.0 provides some new and improved functionality, plus some bug fixes and improved error trapping.\nThe new and improved functionality includes:\n\nA new multivariate sampler, the Barker proposal sampler (sampler_barker). We encourage users to try this sampler in place of the block Metropolis RW_block sampler and let us know how well it works. The Barker sampler uses gradient information and may improve adaptation behavior, including\nbetter mixing when parameters are on different scales or the initialproposal scale is too large.\nAn improved Laplace/AGHQ implementation that includes use of the nlminb optimizer for both inner and outer optimization (for better optimization performance), improved messaging and output naming, returning the log-likelihood and degrees of freedom for model selection calculations, and unified control of optimization method and other controls at either the build stage or through the updateSettings method.\nThe addition of the BOBYQA optimization method through nimOptim, registered via nimOptimMethod.\n\nIn addition to the new and improved functionality above, other bug fixes, improved error trapping, and enhancements include:\n\nPreventing the use of nimbleFunction method names and nimbleFunction names that conflict with names in the nimble language (DSL).\nMore carefully checking for and warning of cases of NaN and non-finite log probability values in various samplers that in some cases may indicate invalid MCMC sampling.\nMore carefully handling of NaN and non-finite log probability values in the CRP sampler.\nError trapping cases of dynamic indices producing a non-scalar result in AD-enabled models and provide a suggested work-around.\nError trapping use of a non-existent nimbleList.\nPreventing use of a single seed when running multiple chains via runMCMC.\nImproving messaging related to lack of derivative support for functions.\nAdding information about model macros to the manual.\nFixing bug in caching values in the CRP sampler when maximum number of clusters is exceeded, which would have caused incorrect sampling (albeit with the user having been warned that they should increase the maximum number of clusters).\nFixing an issue preventing use of nimbleList elements in nimCat.\nPreventing an adaptation interval of one for various block samplers for which an interval of one leads to an error.\nAllowing runLaplace to use an uncompiled Laplace object.\n\nPlease see the release notes on our website for more details.",
    "crumbs": [
      "Home",
      "Blog Posts",
      "Version 1.3.0 of NIMBLE released"
    ]
  },
  {
    "objectID": "blog/nimble-a-new-way-to-do-mcmc-and-more-from-bugs-code-in-r-3.html",
    "href": "blog/nimble-a-new-way-to-do-mcmc-and-more-from-bugs-code-in-r-3.html",
    "title": "NIMBLE: A new way to do MCMC (and more) from BUGS code in R",
    "section": "",
    "text": "Yesterday we released version 0.5 of NIMBLE on our web site, r-nimble.org. (We’ll get it onto CRAN soon, but it has some special needs to work out.) NIMBLE tries to fill a gap in what R programmers and analysts can do with general hierarchical models. Packages like WinBUGS, OpenBUGS, JAGS and Stan provide a language for writing a model flexibly, and then they provide one flavor of MCMC. These have been workhorses of the Bayesian revolution, but they don’t provide much control over how the MCMC works (what samplers are used) or let one do anything else with the model (though Stan provides some additional fitting methods).\nThe idea of NIMBLE has been to provide a layer of programmability for algorithms that use models written in BUGS. We adopted BUGS as a model declaration language because these is so much BUGS code out there and so many books that use BUGS for teaching Bayesian statistics. Our implementation processes BUGS code in R and creates a model object that you can program with. For MCMC, we provide a default set of samplers, but these choices can be modified. It is easy to write your own sampler and add it to the MCMC. And it is easy to add new distributions and functions for use in BUGS code, something that hasn’t been possible (in any easy way) before. These features can allow big gains in MCMC efficiency.\nMCMCs are heavily computational, so NIMBLE includes a compiler that generates C++ specific to a model and algorithm (MCMC samplers or otherwise), compiles it, loads it into R and gives you an interface to it. To be able to compile an algorithm, you need to write it as a nimbleFunction rather than a regular R function. nimbleFunctions can interact with model objects, and they can use a subset of R for math and flow-control. Among other things, the NIMBLE compiler automatically generates code for the Eigen C++ linear algebra library and manages all the necessary interfaces.\nActually, NIMBLE is not specific to MCMC or to Bayesian methods. You can write other algorithms to use whatever model you write in BUGS code. Here’s one simple example: in the past if you wanted to do a simulation study for a model written in BUGS code, you had to re-write the model in R just to simulate from it. With NIMBLE you can simulate from the model as written in BUGS and have complete control over what parts of the model you use. You can also query the model about how nodes are related so that you can make an algorithm adapt to what it finds in a model. We have a set of sequential Monte Carlo (particle filter) methods in development that we’ll release soon. But the idea is that NIMBLE provides a platform for others to develop and disseminate model-generic algorithms.\nNIMBLE also extends BUGS in a bunch of ways that I won’t go into here. And it has one major limitation right now: it doesn’t handle models with stochastic indices, like latent class membership models.\nHere is a toy example of what it looks like to set up and run an MCMC using NIMBLE.\n\nlibrary(nimble)\n\nmyBUGScode &lt;- nimbleCode({\n    mu ~ dnorm(0, sd = 100) ## uninformative prior\n    sigma ~ dunif(0, 100)\n    for(i in 1:10) y[i] ~ dnorm(mu, sd = sigma)\n})\n\nmyModel &lt;- nimbleModel(myBUGScode)\n\n\n\nmyData &lt;- rnorm(10, mean = 2, sd = 5)\nmyModel$setData(list(y = myData))\nmyModel$setInits(list(mu = 0, sigma = 1))\n\nmyMCMC &lt;- buildMCMC(myModel)\ncompiled &lt;- compileNimble(myModel, myMCMC)\n\ncompiled$myMCMC$run(10000)\n\nsamples &lt;- as.matrix(compiled$myMCMC$mvSamples)\n\n\nplot(density(samples[,'mu']))\n\n\n\n\n\n\n\nplot(density(samples[,'sigma']))"
  },
  {
    "objectID": "blog/version-1-1-0-of-nimble-released.html",
    "href": "blog/version-1-1-0-of-nimble-released.html",
    "title": "Version 1.1.0 of NIMBLE released",
    "section": "",
    "text": "We’ve released the newest version of NIMBLE on CRAN and on our website. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC,Laplace approximation, and SMC).\nThis release provides new functionality as well as various bug fixes and improved error trapping, including:\n\nImproving our automatic differentiation (AD) system so it can be used in a wider range of models, including models with stochastic indexing, discrete latent states, and CAR distributions. Support for AD for these models means that HMC sampling and Laplace approximation can be used.\nAllowing distributions and functions (whether user-defined or built-in) that lack AD support (such as dinterval, dconstraint, and truncated distributions) to be used and compiled in AD-enabled models. The added flexibility increases the range of models in which one can use AD methods (HMC or Laplace) on some parts of a model and other samplers or methods on other parts.\nAdding nimIntegrate to the NIMBLE language, providing one-dimensional numerical integration via adaptive quadrature, equivalent to R’s integrate. This can, for example, be used in a user-defined function or distribution for use in model code, such as to implement certain point process or survival models that involve a one-dimensional integral.\nAdding a “prior samples” MCMC sampler, which uses an existing set of numerical samples to define the prior distribution of model node(s).\nBetter support of the dCRP distribution in non-standard model structures.\nAdding error trapping to prevent accidental use of C++ keywords as model variable names.\nRemoving the RW_multinomial MCMC sampler, which was found to generate incorrect posterior results (in cases when a latent state followed a multinomial distribution)\nFixing a bug in conjugacy checking in a case of subsets of multivariate nodes.\nFixing is.na and is.nan to operate in the expected vectorized fashion.\nImproving documentation of AD, nimbleHMC, and nimbleSMC in the manual.\nUpdating Eigen (the C++ linear algebra library used by nimble) to version 3.4.0.\n\nPlease see the release notes on our website for more details.",
    "crumbs": [
      "Home",
      "Blog Posts",
      "Version 1.1.0 of NIMBLE released"
    ]
  },
  {
    "objectID": "blog/announcing-the-nimblemacros-package-and-the-use-of-macros-in-nimble-models.html",
    "href": "blog/announcing-the-nimblemacros-package-and-the-use-of-macros-in-nimble-models.html",
    "title": "announcing the nimbleMacros package and the use of macros in NIMBLE models",
    "section": "",
    "text": "Recent versions of NIMBLE now include the ability to use macros in models. NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods (such as MCMC, Laplace approximation, and SMC).\nA NIMBLE macro is a succinct syntax that expands to create the NIMBLE model code for part or all of a model.\nWe recently released the first version of the nimbleMacros package on CRAN, which provides an initial set of macros available to users and developers. As an example, one could set up the code for a linear mixed effects model by using the LM (“linear model”) macro like this:\n\nlibrary(nimbleMacros)\ncode &lt;- nimbleCode({\n  LM(weight[1:N] ~ Time + (1|Chick))\n})\n\nwith the formula syntax mimicking that of the lme4 package. After building the model based on the code object, you can see the model code produced after the macro is expanded with model$getCode(). The nimbleMacros package also includes macros for creating linear predictors and for loops, and we plan to add additional macros in the future.\nDevelopers can use the tools in nimble itself to create their own macros. See Section 12.4 of the NIMBLE user manual, the nimbleMacros vignette, or help(buildMacro) for more information.",
    "crumbs": [
      "Home",
      "Blog Posts",
      "announcing the nimbleMacros package and the use of macros in NIMBLE models"
    ]
  },
  {
    "objectID": "recent-posts.html",
    "href": "recent-posts.html",
    "title": "Recent Posts",
    "section": "",
    "text": "Stay up to date with the latest NIMBLE developments, releases, and community news.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Posts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nannouncing the nimbleMacros package and the use of macros in NIMBLE models\n\n\n\nannouncement\n\nrelease\n\n\n\n\n\n\n\n\n\nMar 19, 2025\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 1.3.0 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nDec 21, 2024\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 1.2.1 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 1.2.0 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nJun 14, 2024\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 1.1.0 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nnimbleHMC version 0.2.0 released, providing improved HMC performance\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 1.0.1 of NIMBLE released, fixing a bug in version 1.0.0 affecting certain models\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 1.0.0 of NIMBLE released, providing automatic differentiation, Laplace approximation, and HMC sampling\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.13.1 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nBug in newly-released version 0.13.0 affecting MCMC for models with predictive nodes\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.13.0 of NIMBLE released\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nWe’re looking for a programmer\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE virtual short course, January 4-6, 2023\n\n\n\neducation\n\nannouncement\n\n\n\n\n\n\n\n\n\nSep 8, 2022\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nBeta version of NIMBLE with automatic differentiation, including HMC sampling and Laplace approximation\n\n\n\nannouncement\n\nrelease\n\n\n\n\n\n\n\n\n\nJul 15, 2022\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 0.12.2 of NIMBLE released, including an important bug fix for some models using Bayesian nonparametrics with the dCRP distribution\n\n\n\nrelease\n\nannouncement\n\n\n\n\n\n\n\n\n\nMar 4, 2022\n\n\nNIMBLE Development Team\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE in-person short course, June 1-3, Lisbon, Portugal\n\n\n\neducation\n\nannouncement\n\n\n\n\n\n\n\n\n\nMar 2, 2022\n\n\nChris Paciorek\n\n\n\n\n\n\n\n\n\n\n\n\nA close look at some linear model MCMC comparisons\n\n\n\n\n\n\n\n\nNov 11, 2021\n\n\nPerry de Valpine\n\n\n\n\n\n\n\n\n\n\n\n\nA close look at some posted trials of nimble for accelerated failure time models\n\n\n\n\n\n\n\n\nOct 29, 2021\n\n\nPerry de Valpine\n\n\n\n\n\n\n\n\n\n\n\n\nNIMBLE online tutorial, November 18, 2021\n\n\n\neducation\n\nannouncement\n\n\n\n\n\n\n\n\n\nOct 20, 2021\n\n\nChris Paciorek\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "Examples",
    "section": "",
    "text": "NIMBLE provides a rich collection of examples to help you get started with statistical modeling and algorithm development. Examples cover a wide range of topics from basic BUGS model usage to advanced algorithm implementation.\n\n\n\n\n\n\nBuilding a model from BUGS code\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nConverting to NIMBLE\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nCreating a default MCMC\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nCustomizing an MCMC\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nParallelizing NIMBLE\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nSimulating from a model (quickly!)\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nUsing linear predictors in a regression model\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nBuilding a generalized linear mixed model and an MCMC for it\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nVariable selection using reversible jump MCMC (RJMCMC)\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nRestarting an MCMC\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nPosterior predictive sampling and other post-MCMC use of samples\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nEcology examples from the book Applied Hierarchical Modeling in Ecology\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nWriting a new distribution for use in BUGS code: zero-inflated Poisson\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nGaussian process models\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nConditional autoregressive (CAR) models\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nItem response theory (IRT) models\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nMaximum likelihood via Monte Carlo Expectation-Maximization (MCEM)\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nMaximum likelihood (basic example)\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nStochastic volatility modeling using a custom distribution\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nBayesian nonparametric density estimation\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nBayesian nonparametric random effects\n\n\n\n\n\nRead this example on a new page.\n\n\n\n\n\n\n\n\n\n\n\nBayesian nonparametrics with jointly clustered observations\n\n\n\n\n\nRead this example on a new page."
  },
  {
    "objectID": "license-and-citation.html",
    "href": "license-and-citation.html",
    "title": "License and Citation",
    "section": "",
    "text": "License\nNIMBLE is largely licensed under the BSD 3-Clause license, but our C++ code is licensed under the GPL (&gt;=2) and the package includes external contributions with their own licensing. Please see the COPYRIGHTS file in the package.\n\n\nCiting NIMBLE\nPlease cite the Journal of Computational and Graphical Statistics article in published work that uses or mentions NIMBLE. In work that also uses NIMBLE, please also cite the R package for the specific version. To help us track usage to justify funding support for NIMBLE, please include the DOI in the citation. Please cite the manual for specific material from the manual.\nHow to cite NIMBLE in general:\n@article{nimble-article:2017, \n  author = {{de Valpine}, P. and Turek, D. and Paciorek, C.J. and Anderson-Bergman, C. and {Temple Lang}, D. and Bodik, R.} \n  title = {Programming with models: writing statistical algorithms for general model structures with {NIMBLE}},\n  year = {2017}, \n  journal = {Journal of Computational and Graphical Statistics},\n  volume = 26,\n  pages = {403-417},\n  doi = {10.1080/10618600.2016.1172487}\n}\nHow to cite the package:\nNIMBLE Development Team. 2024. NIMBLE: MCMC, Particle Filtering, and Programmable Hierarchical Modeling. doi: 10.5281/zenodo.1211190. R package version 1.3.0, https://cran.r-project.org/package=nimble.\n@misc{nimble-software:2024,\n  author = {{de Valpine}, P. and Paciorek, C. and Turek, D. and Michaud, N. and Anderson-Bergman, C. and Obermeyer, F. and Wehrhahn Cortes, C. and Rodr{\\'i}guez, A. and {Temple Lang}, D. and Zhang, W. and Paganin, S. and Hug, J. and van Dam-Bates, P.}, \n  year = {2024},\n  title = {NIMBLE: MCMC, Particle Filtering, and Programmable Hierarchical Modeling}\n  version = 1.3.0,\n  doi = {10.5281/zenodo.1211190},\n  url = {https://cran.r-project.org/package=nimble}\n}\nHow to cite the manual:\nNIMBLE Development Team. 2024. NIMBLE User Manual. doi: 10.5281/zenodo.1211190. R package manual version 1.3.0. https://r-nimble.org.\n@manual{nimble-manual:2024,\n  author = {{de Valpine}, P. and Paciorek, C. and Turek, D. and Michaud, N. and Anderson-Bergman, C. and Obermeyer, F. and Wehrhahn Cortes, C. and Rodr{\\'i}guez, A. and {Temple Lang}, D. and Zhang, W. and Paganin, S. and Hug, J. and van Dam-Bates, P.}, \n  year = {2024}, \n  title = {NIMBLE User Manual}, \n  version = 1.3.0, \n  doi = {10.5281/zenodo.1211190}, \n  url = {https://r-nimble.org} \n}"
  },
  {
    "objectID": "CONVERSION_NOTES.html",
    "href": "CONVERSION_NOTES.html",
    "title": "WordPress to Quarto Conversion - NIMBLE Website",
    "section": "",
    "text": "Successfully converted the NIMBLE WordPress website to a Quarto website format. The conversion includes:\n\n\n\nMain Site Structure\n\nHomepage (index.qmd) with all key features\nNavigation structure in _quarto.yml\nCustom styling (styles.css, custom.scss)\n\nCore Pages Converted\n\nAbout Us (with team member photos)\nWhat is NIMBLE?\nDownload/Installation instructions\nDocumentation links\nExamples page\nContributing guidelines\nLicense and Citation\nGroups and Issues\nRelease Notes\nRecent Posts listing\n\nBlog System\n\nBlog index with listing functionality\nSample blog posts converted\nCategory system implemented\nRSS-ready structure\n\nAssets\n\nLogo and team member images\nEssential graphics (BUGS diagrams, flow charts)\nNIMBLE cheat sheet PDF\n\nConversion Tools\n\nPython script for HTML to Markdown conversion\nBash script for batch processing\nDocumentation for future conversions\n\n\n\n\n\n\nResponsive Design: Mobile-friendly navigation and layout\nSearch: Built-in site search functionality\nRSS: Automatic RSS feed generation\nCategories: Blog post categorization system\nSocial Links: GitHub and Twitter integration\nModern UI: Clean, professional appearance\n\n\n\n\nwebsite/\n├── _quarto.yml           # Main configuration\n├── index.qmd            # Homepage\n├── about-us.qmd         # Team information\n├── what-is-nimble.qmd   # Introduction\n├── download.qmd         # Installation\n├── documentation.qmd    # Docs links\n├── examples.qmd         # Code examples\n├── contributing.qmd     # How to contribute\n├── license-and-citation.qmd\n├── groups-and-issues.qmd\n├── release-notes.qmd\n├── recent-posts.qmd\n├── archived-versions-of-nimble-and-the-user-manual.qmd\n├── styles.css           # Custom CSS\n├── custom.scss          # SCSS variables\n├── blog/               # Blog posts\n│   ├── index.qmd       # Blog listing\n│   ├── version-1-3-0-of-nimble-released.qmd\n│   ├── version-1-2-1-of-nimble-released.qmd\n│   ├── version-1-2-0-of-nimble-released.qmd\n│   └── announcing-the-nimblemacros-package.qmd\n├── images/             # Static assets\n│   ├── nimble-logo-oval-small.png\n│   ├── team member photos...\n│   ├── BUGSfig.png\n│   ├── mixingExample.png\n│   ├── compileFlowChart.png\n│   └── NimbleCheatSheet.pdf\n├── convert_post.py     # HTML to Markdown converter\n├── convert_posts.sh    # Batch conversion script\n└── README.md           # Documentation\n\n\n\n\nConvert Remaining Blog Posts\n# Use the provided script\n./convert_posts.sh\nReview and Clean Up\n\nEdit converted blog posts for formatting\nUpdate dates and categories\nFix any broken links or images\n\nAdditional Content\n\nConvert WordPress pages not yet included\nAdd any missing documentation\nUpdate external links\n\nDeployment\n\nSet up hosting (Netlify, GitHub Pages, etc.)\nConfigure domain name\nSet up continuous deployment\n\n\n\n\n\n\nAdding New Posts: Create .qmd files in blog/ directory\nUpdating Pages: Edit corresponding .qmd files\nStyling Changes: Modify styles.css or custom.scss\nNavigation: Update _quarto.yml\n\n\n\n\n\nVersion Control: Full Git integration\nMaintainability: Markdown-based content\nPerformance: Static site generation\nModern Features: Built-in search, RSS, responsive design\nExtensibility: Easy to add new features\nSecurity: No database or dynamic components\n\nThe conversion preserves all essential content and functionality while providing a modern, maintainable foundation for the NIMBLE website."
  },
  {
    "objectID": "CONVERSION_NOTES.html#conversion-summary",
    "href": "CONVERSION_NOTES.html#conversion-summary",
    "title": "WordPress to Quarto Conversion - NIMBLE Website",
    "section": "",
    "text": "Successfully converted the NIMBLE WordPress website to a Quarto website format. The conversion includes:\n\n\n\nMain Site Structure\n\nHomepage (index.qmd) with all key features\nNavigation structure in _quarto.yml\nCustom styling (styles.css, custom.scss)\n\nCore Pages Converted\n\nAbout Us (with team member photos)\nWhat is NIMBLE?\nDownload/Installation instructions\nDocumentation links\nExamples page\nContributing guidelines\nLicense and Citation\nGroups and Issues\nRelease Notes\nRecent Posts listing\n\nBlog System\n\nBlog index with listing functionality\nSample blog posts converted\nCategory system implemented\nRSS-ready structure\n\nAssets\n\nLogo and team member images\nEssential graphics (BUGS diagrams, flow charts)\nNIMBLE cheat sheet PDF\n\nConversion Tools\n\nPython script for HTML to Markdown conversion\nBash script for batch processing\nDocumentation for future conversions\n\n\n\n\n\n\nResponsive Design: Mobile-friendly navigation and layout\nSearch: Built-in site search functionality\nRSS: Automatic RSS feed generation\nCategories: Blog post categorization system\nSocial Links: GitHub and Twitter integration\nModern UI: Clean, professional appearance\n\n\n\n\nwebsite/\n├── _quarto.yml           # Main configuration\n├── index.qmd            # Homepage\n├── about-us.qmd         # Team information\n├── what-is-nimble.qmd   # Introduction\n├── download.qmd         # Installation\n├── documentation.qmd    # Docs links\n├── examples.qmd         # Code examples\n├── contributing.qmd     # How to contribute\n├── license-and-citation.qmd\n├── groups-and-issues.qmd\n├── release-notes.qmd\n├── recent-posts.qmd\n├── archived-versions-of-nimble-and-the-user-manual.qmd\n├── styles.css           # Custom CSS\n├── custom.scss          # SCSS variables\n├── blog/               # Blog posts\n│   ├── index.qmd       # Blog listing\n│   ├── version-1-3-0-of-nimble-released.qmd\n│   ├── version-1-2-1-of-nimble-released.qmd\n│   ├── version-1-2-0-of-nimble-released.qmd\n│   └── announcing-the-nimblemacros-package.qmd\n├── images/             # Static assets\n│   ├── nimble-logo-oval-small.png\n│   ├── team member photos...\n│   ├── BUGSfig.png\n│   ├── mixingExample.png\n│   ├── compileFlowChart.png\n│   └── NimbleCheatSheet.pdf\n├── convert_post.py     # HTML to Markdown converter\n├── convert_posts.sh    # Batch conversion script\n└── README.md           # Documentation\n\n\n\n\nConvert Remaining Blog Posts\n# Use the provided script\n./convert_posts.sh\nReview and Clean Up\n\nEdit converted blog posts for formatting\nUpdate dates and categories\nFix any broken links or images\n\nAdditional Content\n\nConvert WordPress pages not yet included\nAdd any missing documentation\nUpdate external links\n\nDeployment\n\nSet up hosting (Netlify, GitHub Pages, etc.)\nConfigure domain name\nSet up continuous deployment\n\n\n\n\n\n\nAdding New Posts: Create .qmd files in blog/ directory\nUpdating Pages: Edit corresponding .qmd files\nStyling Changes: Modify styles.css or custom.scss\nNavigation: Update _quarto.yml\n\n\n\n\n\nVersion Control: Full Git integration\nMaintainability: Markdown-based content\nPerformance: Static site generation\nModern Features: Built-in search, RSS, responsive design\nExtensibility: Easy to add new features\nSecurity: No database or dynamic components\n\nThe conversion preserves all essential content and functionality while providing a modern, maintainable foundation for the NIMBLE website."
  },
  {
    "objectID": "about-us.html",
    "href": "about-us.html",
    "title": "About Us",
    "section": "",
    "text": "Perry de Valpine is in the Department of Environmental Science, Policy and Management at the University of California, Berkeley.\n\n\n\nChris Paciorek is an adjunct professor in the Department of Statistics at UC Berkeley, as well as the department’s statistical computing consultant.\n\n\n\nDaniel Turek is in the Department of Mathematics at Lafayette College.\n\n\n\nDuncan Temple Lang is in the Department of Statistics at the University of California, Davis."
  },
  {
    "objectID": "about-us.html#core-team",
    "href": "about-us.html#core-team",
    "title": "About Us",
    "section": "",
    "text": "Perry de Valpine is in the Department of Environmental Science, Policy and Management at the University of California, Berkeley.\n\n\n\nChris Paciorek is an adjunct professor in the Department of Statistics at UC Berkeley, as well as the department’s statistical computing consultant.\n\n\n\nDaniel Turek is in the Department of Mathematics at Lafayette College.\n\n\n\nDuncan Temple Lang is in the Department of Statistics at the University of California, Davis."
  },
  {
    "objectID": "what-is-nimble.html",
    "href": "what-is-nimble.html",
    "title": "What is NIMBLE?",
    "section": "",
    "text": "NIMBLE is a system for building and sharing analysis methods for statistical models, especially for hierarchical models and computationally-intensive methods. NIMBLE is built in R but compiles your models and algorithms using C++ for speed. It includes three components:\nNIMBLE can also be used without models as a way to compile simple R-like code into C++, which is then compiled and loaded into R with an interface function or object."
  },
  {
    "objectID": "what-is-nimble.html#funding",
    "href": "what-is-nimble.html#funding",
    "title": "What is NIMBLE?",
    "section": "Funding",
    "text": "Funding\nThe development of NIMBLE has been funded by:\n\nan NSF Advances in Biological Informatics grant (DBI-1147230) to P. de Valpine, C. Paciorek, and D. Temple Lang;\nan NSF SI2-SSI grant (ACI-1550488) to P. de Valpine, C. Paciorek, and D. Temple Lang; and\nan NSF Collaborative Research grant (DMS-1622444) to P. de Valpine, A. Rodriguez, and C. Paciorek.\nan NSF Collaborative Research grant (DMS-2152860) to P. de Valpine, C. Paciorek, and D. Turek.\n\nwith additional support provided by postdoctoral funding for D. Turek from the Berkeley Institute for Data Science and Google Summer of Code fellowships for N. Michaud (2015) and C. Lewis-Beck (2017)."
  }
]